lapply(as.list(1:4), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)
unlist(lapply(as.list(1:4), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees))
unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees))
plot(unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)))
plot(unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)), ylab = "y")
plot(unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)), log = "y")
plot(unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)), log = "y", type = "l")
plot(1:145, unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)), log = "y", type = "l")
plot(1:145, unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)), log = "y", type = "l", lwd = 2)
plot(1:145, unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)), log = "y", type = "l", lwd = 2); polygon(x = c(0, 145, 145, 0), y = c(10^78, 10^78, 10^80, 10^80))
plot(1:145, unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)), log = "y", type = "l", lwd = 2); polygon(x = c(0, 145, 145, 0), y = c(10^78, 10^78, 10^80, 10^80), border = 0)
plot(1:145, unlist(lapply(as.list(1:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TotalTrees)), log = "y", type = "l", lwd = 2); polygon(x = c(0, 145, 145, 0), y = c(10^78, 10^78, 10^80, 10^80), border = 0, col = rgb(1, 0, 0, 0.5))
plot(3:145, unlist(lapply(as.list(3:145), function(x) GetNLabelledMultifurcatingRootedTrees(x)$TreesByNodeCount[2])), log = "y", type = "l", lwd = 2); polygon(x = c(0, 145, 145, 0), y = c(10^78, 10^78, 10^80, 10^80), border = 0, col = rgb(1, 0, 0, 0.5))
?hypRspace
library(hypRspace)
?hypRspace
GetNLabelledMultifurcatingRootedTrees(10)[[1]]
GetNLabelledMultifurcatingRootedTrees(40)[[1]]
GetNLabelledMultifurcatingRootedTrees(45)[[1]]
GetNLabelledMultifurcatingRootedTrees(50)[[1]]
GetNLabelledMultifurcatingRootedTrees(49)[[1]]
library(metatree)
?metatree
PseudosuchiaSpecies <- PaleobiologyDBChildFinder(taxon_names = "Pseudosuchia", returnrank = "3")
PseudosuchiaSpecies[1:2, ]
lapply(apply(PseudosuchiaSpecies[, c("OriginalTaxonNo", "ResolvedTaxonNo")], 1, as.list), function(x) gsub("txn:|var:", "", unlist(x)[!is.na(unlist(x))][1]))
unname(unlist(lapply(apply(PseudosuchiaSpecies[, c("OriginalTaxonNo", "ResolvedTaxonNo")], 1, as.list), function(x) gsub("txn:|var:", "", unlist(x)[!is.na(unlist(x))][1]))))
library(metatree)#
#
PseudosuchiaSpecies <- PaleobiologyDBChildFinder(taxon_names = "Pseudosuchia", returnrank = "3")#
#
PseudosuchiaSpeciesNumbers <- unname(unlist(lapply(apply(PseudosuchiaSpecies[, c("OriginalTaxonNo", "ResolvedTaxonNo")], 1, as.list), function(x) gsub("txn:|var:", "", unlist(x)[!is.na(unlist(x))][1]))))#
#
PseudosuchiaOccurrences <- PaleobiologyDBOccurrenceQuerier(taxon_nos = PseudosuchiaSpeciesNumbers)
PseudosuchiaOccurrences[1:2, ]
nrow(PseudosuchiaOccurrences)
PseudosuchiaOccurrences
library(devtools)
devtools::install_github("graemetlloyd/metatree")
?write.csv
PseudosuchiaOccurrences[1:2, ]
nchar("?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????10001001???????????????????????????????????????????????????????????")
# Load Claddis library:#
library(Claddis)#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get file list:#
file.list <- list.files()#
#
# Get just the group matrix pages:#
file.list <- file.list[grep("matr[a-z]{4}.html", file.list)]#
#
# Vector for storing output:#
results <- vector(mode = "character")#
#
# Main loop:#
for(i in 1:length(file.list)) {#
  # Read in ith file:#
  X <- scan(file.list[i], what = "", sep = "\n", quiet = TRUE)#
  # Find first p tag opening:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # FInd last p tag closing:#
  ends <- grep("</p>", X)#
  # Reduce X to just the portion with references:#
  X <- X[begins[1]:ends[length(ends)]]#
  # Find where p tags open:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # Find where p tags close:#
  ends <- grep("</p>", X)#
  # Check p tags are closed and warn if not:#
  if(length(begins) != length(ends)) print(paste("Error in", file.list[i]))#
  # For each set of p tags:#
  for(j in 1:length(ends)) {#
    # Get full reference block:#
    Y <- X[begins[j]:ends[j]]#
    # Only proceed if this has not already been dealt with:#
    if(length(grep("<a href", Y)) == 0) {#
      # Remove bookmarks:#
      Y <- gsub("</p>", "", gsub("<p class=\"hangingindent\">", "", Y))#
      # Strip out leading whitespace:#
      while(length(grep("\t", Y)) > 0) Y <- gsub("\t", " ", Y)#
      # Strip out leading whitespace:#
      while(length(grep("  ", Y)) > 0) Y <- gsub("  ", " ", Y)#
      # Strip out last leading whitespace"#
      for(k in 1:length(Y)) Y[k] <- paste(strsplit(Y[k], "")[[1]][2:length(strsplit(Y[k], "")[[1]])], collapse = "")#
      # Isolate author and year:#
      authorandyear <- strsplit(gsub(" and ", "%%", gsub("\\., ", ".%%", Y[1])), "%%")[[1]]#
      # Isolate title:#
      title <- Y[2]#
      ##
      locale <- gsub("</b>", "", gsub("<b>", "", gsub("</em>", "", gsub("<em>", "", strsplit(gsub("\\.", "", gsub(", ", "%%", Y[3])), "%%")[[1]]))))#
      ##
      authorline <- paste("\t\t<Author>\n", paste("\t\t\t<List>", authorandyear[1:(length(authorandyear) - 1)], "</List>", sep = "", collapse = "\n"), "\n\t\t</Author>\n", sep = "")#
      ##
      yearline <- paste("\t\t<Year>", gsub("\\.", "", authorandyear[length(authorandyear)]), "</Year>\n", sep = "")#
      ##
      year <- gsub("</Year>\n", "", gsub("\t\t<Year>", "", yearline))#
      ##
      titleline <- strsplit(title, "")[[1]]#
      ##
      if(titleline[length(titleline)] == ".") titleline <- titleline[-length(titleline)]#
      ##
      titleline <- paste(titleline, collapse = "")#
      ##
      titleline <- paste("\t\t<Title>", titleline, "</Title>\n", sep = "")#
      # Case if a book chapter:#
      if(length(grep("In ", locale[1])) == 1) {#
        # Restore locale to original line:#
        locale <- Y[3]#
        ##
        locale <- gsub("<em>In</em> ", "", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\) ", "%%", locale)#
        # Isolate editors#
        editors <- strsplit(locale, "%%")[[1]][1]#
        # Add "and" separator:#
        editors <- gsub(" and ", "%%", editors)#
        ##
        if(length(grep(",", editors)) > 0) {#
          # Case if single editor in correct "Surname, Initials" format:#
          if(length(grep("%%", editors)) == 0) editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
          # Case if authors are in incorrect "Intitals Surname" format:#
          if(strsplit(editors, "")[[1]][2] == ".") {#
            # Add separator between names:#
            editors <- gsub(", ", "%%", editors)#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
            ##
          } else {#
            # Add separator between names:#
            editors <- gsub("\\., ", ".%%", editors)#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", strsplit(editors, "%%")[[1]], "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
          ##
        } else {#
          # Case if single editor in incorrect "Intitals Surname" format:#
          if(length(grep("%%",editors)) == 0) {#
            ##
            editors <- strsplit(editors, "\\. ")[[1]]#
            ##
            editors <- paste(paste(editors[length(editors)], ",", sep = ""), paste(editors[1:(length(editors) - 1)], ".", sep = "", collapse = " "), collapse = " ")#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
            # Case of two authors in incorrect "Intitals Surname" format:#
          } else {#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
        }#
        # Remove editors from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Find end of book title separator:#
        locale <- gsub("\\. ", "%%", locale)#
        # Remove trailing period:#
        locale <- gsub("\\.", "", locale)#
        # Isolate booktitle:#
        booktitleline <- paste("\t\t<Booktitle>", strsplit(locale, "%%")[[1]][1], "</Booktitle>\n", sep = "")#
        # Remove booktitle from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Remove false gaps:#
        while(length(locale) > 1) locale <- paste(locale, collapse = ". ")#
        # Separate remaining portions:#
        locale <- strsplit(locale, ", ")[[1]]#
        ##
        publisherline <- paste("\t\t<Publisher>", locale[1], "</Publisher>\n", sep = "")#
        ##
        cityline <- paste("\t\t<City>", locale[2], "</City>\n", sep = "")#
        ##
        pagesline <- paste("\t\t<Pages>", gsub("<br>", "", gsub("p", "", locale[3])), "</Pages>\n", sep = "")#
        ##
        fulllines <- paste(authorline, yearline, titleline, "\t\t<Journal/>\n", "\t\t<Volume/>\n", pagesline, booktitleline, publisherline, cityline, editorsline, sep = "")#
        # Case if a journal:#
      } else {#
        ##
        if(year == "in press") {#
          # Case if journal title with commas:#
          if(length(locale) > 2) {#
            # Collapse journal title:#
            locale[1] <- paste(locale[1], locale[2], sep = ", ")#
            # Remove redudnant second part#
            locale <- locale[-2]#
          }#
          # Delete empty volume value#
          if(locale[2] == "") locale <- locale[-2]#
        }#
        # Find journal titles with commas:#
        while(length(locale) > 3) {#
          # Collapse journal title:#
          locale[1] <- paste(locale[1], locale[2], sep = ", ")#
          # Remove redudnant second part:#
          locale <- locale[-2]#
        }#
        ##
        journalline <- paste("\t\t<Journal>", locale[1], "</Journal>\n", sep = "")#
        ##
        if(length(locale) > 1) {#
          ##
          volumeline <- paste("\t\t<Volume>", locale[2], "</Volume>\n", sep = "")#
          ##
        } else {#
          ##
          volumeline <- "\t\t<Volume/>\n"#
        }#
        ##
        if(length(locale) > 2) {#
          ##
          pagesline <- paste("\t\t<Pages>", locale[3], "</Pages>\n", sep = "")#
          ##
        } else {#
          ##
          pagesline <- "\t\t<Pages/>\n"#
        }#
        ##
        fulllines <- paste(authorline, yearline, titleline, journalline, volumeline, pagesline, "\t\t<Booktitle/>\n", "\t\t<Publisher/>\n", "\t\t<City/>\n","\t\t<Editor/>\n", sep = "")#
      }#
    }#
    ##
    results <- c(results, fulllines)#
  }#
}#
#
# Collapse to just unique references (not sure how duplicates ended up in here...):#
results <- sort(unique(results))#
#
# Create empty vector to store hypothetical file names:#
filenames <- vector(mode = "character")#
#
# For each reference:#
for(i in 1:length(results)) {#
  # Isolate authors:#
  authors <- strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]][which(nchar(strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]]) > 0)]#
  # Isolate surnames:#
  surnames <- unlist(lapply(strsplit(authors, split = ","), '[', 1))#
  # Get publication year:#
  year <- gsub(" ", "", strsplit(gsub("\n|\t", "", results[i]), split = "<Year>|</Year>")[[1]][2])#
  # If a single author:#
  if(length(surnames) == 1) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames, year, sep = "_"))))#
  # If two authors:#
  if(length(surnames) == 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(paste(surnames, collapse = "_et_"), year, sep = "_"))))#
  # If more than two authors:#
  if(length(surnames) > 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames[1], "etal", year, sep = "_"))))#
}#
#
# Isolate references that have multiple file names (i.e., two or more refrences could be contracted to the same name):#
duplicates <- unique(filenames[duplicated(filenames)])#
#
# Set working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Get list of folders:#
folder.list <- list.files()[-grep("\\.", list.files())]#
#
# Get full paths for each folder:#
for(i in 1:length(folder.list)) folder.list[i] <- paste(getwd(), "/", folder.list[i], sep = "")#
#
# Vector for storing nexus file list:#
file.list <- vector(mode = "character")#
#
# Find all file paths for nexus files:#
for(i in 1:length(folder.list)) {#
  # Set working directory for current folder:#
  setwd(folder.list[i])#
  # Look for NEXUS files:#
  if(length(grep(".nex", list.files())) > 0) {#
    # Add any found to file list:#
    file.list <- c(file.list, paste(folder.list[i], "/", list.files()[grep(".nex", list.files())], sep = ""))#
  }#
}#
#
# Get just the NEXUS file names:#
nexus.files <- unlist(lapply(strsplit(file.list, "/"), '[', 9))#
#
# Reset working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Create vector to store multiple hits:#
multi_hitters <- vector(mode = "character")#
#
# Set scratch counter:#
scratch_counter <- 1#
#
# Create nexus, tnt and xml files:#
for(i in 1:length(file.list)) {#
  # Start feedback:#
  cat("Attempting to read: ", file.list[i], "...")#
  # Get stripped verion of name (i.e., missing a, b, aa etc. ending):#
  stripped_name <- gsub(strsplit(nexus.files[i], "[:0-9:]{4}|inpress")[[1]][2], "", nexus.files[i])#
  # Get hits for stripped name in filenames:#
  hits <- grep(stripped_name, filenames)#
  # Check there is a match:#
  if(length(hits) == 0) stop("No reference with matching name.")#
  # Create reference info:#
  reference_info <- paste(results[hits], collapse = "\n\nOR\n\n")#
  # If multiple hits add to list so these can be manually checked later:#
  if(length(hits) > 1) multi_hitters <- c(multi_hitters, nexus.files[i])#
  # Read in matrix:#
  mymatrix <- ReadMorphNexus(file.list[i])#
  # Update header text:#
  mymatrix$Topper$Header <- "File downloaded from graemetlloyd.com"#
  # Make file name:#
  file.name <- gsub(".nex", "", strsplit(file.list[i], "/")[[1]][length(strsplit(file.list[i], "/")[[1]])])#
  # Write out NEXUS data:#
  #WriteMorphNexus(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus", "/", file.name, ".nex", sep = ""))#
  # Write out TNT data:#
  #WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/tnt", "/", file.name, ".tnt", sep = ""))#
  # Write out TNT for analysis:#
  WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""), add.analysis.block = TRUE)#
}
TNTFA <- readLines(paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""))
grep("scractch,tre", TNTFA, fixed = TRUE)
grep("scratch,tre", TNTFA, fixed = TRUE)
grep("scratch.tre", TNTFA, fixed = TRUE)
length(grep("scratch.tre", TNTFA, fixed = TRUE))
paste(scratch", scratch_counter, ".tre)
paste(scratch", scratch_counter, ".tre")
paste("scratch", scratch_counter, ".tre")
paste("scratch", scratch_counter, ".tre", collapse = "")
paste("scratch", scratch_counter, ".tre", sep = "")
gsub("scratch.tre", paste("scratch", scratch_counter, ".tre", sep = ""), TNTFA, fixed = TRUE)
# Load Claddis library:#
library(Claddis)#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get file list:#
file.list <- list.files()#
#
# Get just the group matrix pages:#
file.list <- file.list[grep("matr[a-z]{4}.html", file.list)]#
#
# Vector for storing output:#
results <- vector(mode = "character")#
#
# Main loop:#
for(i in 1:length(file.list)) {#
  # Read in ith file:#
  X <- scan(file.list[i], what = "", sep = "\n", quiet = TRUE)#
  # Find first p tag opening:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # FInd last p tag closing:#
  ends <- grep("</p>", X)#
  # Reduce X to just the portion with references:#
  X <- X[begins[1]:ends[length(ends)]]#
  # Find where p tags open:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # Find where p tags close:#
  ends <- grep("</p>", X)#
  # Check p tags are closed and warn if not:#
  if(length(begins) != length(ends)) print(paste("Error in", file.list[i]))#
  # For each set of p tags:#
  for(j in 1:length(ends)) {#
    # Get full reference block:#
    Y <- X[begins[j]:ends[j]]#
    # Only proceed if this has not already been dealt with:#
    if(length(grep("<a href", Y)) == 0) {#
      # Remove bookmarks:#
      Y <- gsub("</p>", "", gsub("<p class=\"hangingindent\">", "", Y))#
      # Strip out leading whitespace:#
      while(length(grep("\t", Y)) > 0) Y <- gsub("\t", " ", Y)#
      # Strip out leading whitespace:#
      while(length(grep("  ", Y)) > 0) Y <- gsub("  ", " ", Y)#
      # Strip out last leading whitespace"#
      for(k in 1:length(Y)) Y[k] <- paste(strsplit(Y[k], "")[[1]][2:length(strsplit(Y[k], "")[[1]])], collapse = "")#
      # Isolate author and year:#
      authorandyear <- strsplit(gsub(" and ", "%%", gsub("\\., ", ".%%", Y[1])), "%%")[[1]]#
      # Isolate title:#
      title <- Y[2]#
      ##
      locale <- gsub("</b>", "", gsub("<b>", "", gsub("</em>", "", gsub("<em>", "", strsplit(gsub("\\.", "", gsub(", ", "%%", Y[3])), "%%")[[1]]))))#
      ##
      authorline <- paste("\t\t<Author>\n", paste("\t\t\t<List>", authorandyear[1:(length(authorandyear) - 1)], "</List>", sep = "", collapse = "\n"), "\n\t\t</Author>\n", sep = "")#
      ##
      yearline <- paste("\t\t<Year>", gsub("\\.", "", authorandyear[length(authorandyear)]), "</Year>\n", sep = "")#
      ##
      year <- gsub("</Year>\n", "", gsub("\t\t<Year>", "", yearline))#
      ##
      titleline <- strsplit(title, "")[[1]]#
      ##
      if(titleline[length(titleline)] == ".") titleline <- titleline[-length(titleline)]#
      ##
      titleline <- paste(titleline, collapse = "")#
      ##
      titleline <- paste("\t\t<Title>", titleline, "</Title>\n", sep = "")#
      # Case if a book chapter:#
      if(length(grep("In ", locale[1])) == 1) {#
        # Restore locale to original line:#
        locale <- Y[3]#
        ##
        locale <- gsub("<em>In</em> ", "", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\) ", "%%", locale)#
        # Isolate editors#
        editors <- strsplit(locale, "%%")[[1]][1]#
        # Add "and" separator:#
        editors <- gsub(" and ", "%%", editors)#
        ##
        if(length(grep(",", editors)) > 0) {#
          # Case if single editor in correct "Surname, Initials" format:#
          if(length(grep("%%", editors)) == 0) editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
          # Case if authors are in incorrect "Intitals Surname" format:#
          if(strsplit(editors, "")[[1]][2] == ".") {#
            # Add separator between names:#
            editors <- gsub(", ", "%%", editors)#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
            ##
          } else {#
            # Add separator between names:#
            editors <- gsub("\\., ", ".%%", editors)#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", strsplit(editors, "%%")[[1]], "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
          ##
        } else {#
          # Case if single editor in incorrect "Intitals Surname" format:#
          if(length(grep("%%",editors)) == 0) {#
            ##
            editors <- strsplit(editors, "\\. ")[[1]]#
            ##
            editors <- paste(paste(editors[length(editors)], ",", sep = ""), paste(editors[1:(length(editors) - 1)], ".", sep = "", collapse = " "), collapse = " ")#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
            # Case of two authors in incorrect "Intitals Surname" format:#
          } else {#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
        }#
        # Remove editors from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Find end of book title separator:#
        locale <- gsub("\\. ", "%%", locale)#
        # Remove trailing period:#
        locale <- gsub("\\.", "", locale)#
        # Isolate booktitle:#
        booktitleline <- paste("\t\t<Booktitle>", strsplit(locale, "%%")[[1]][1], "</Booktitle>\n", sep = "")#
        # Remove booktitle from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Remove false gaps:#
        while(length(locale) > 1) locale <- paste(locale, collapse = ". ")#
        # Separate remaining portions:#
        locale <- strsplit(locale, ", ")[[1]]#
        ##
        publisherline <- paste("\t\t<Publisher>", locale[1], "</Publisher>\n", sep = "")#
        ##
        cityline <- paste("\t\t<City>", locale[2], "</City>\n", sep = "")#
        ##
        pagesline <- paste("\t\t<Pages>", gsub("<br>", "", gsub("p", "", locale[3])), "</Pages>\n", sep = "")#
        ##
        fulllines <- paste(authorline, yearline, titleline, "\t\t<Journal/>\n", "\t\t<Volume/>\n", pagesline, booktitleline, publisherline, cityline, editorsline, sep = "")#
        # Case if a journal:#
      } else {#
        ##
        if(year == "in press") {#
          # Case if journal title with commas:#
          if(length(locale) > 2) {#
            # Collapse journal title:#
            locale[1] <- paste(locale[1], locale[2], sep = ", ")#
            # Remove redudnant second part#
            locale <- locale[-2]#
          }#
          # Delete empty volume value#
          if(locale[2] == "") locale <- locale[-2]#
        }#
        # Find journal titles with commas:#
        while(length(locale) > 3) {#
          # Collapse journal title:#
          locale[1] <- paste(locale[1], locale[2], sep = ", ")#
          # Remove redudnant second part:#
          locale <- locale[-2]#
        }#
        ##
        journalline <- paste("\t\t<Journal>", locale[1], "</Journal>\n", sep = "")#
        ##
        if(length(locale) > 1) {#
          ##
          volumeline <- paste("\t\t<Volume>", locale[2], "</Volume>\n", sep = "")#
          ##
        } else {#
          ##
          volumeline <- "\t\t<Volume/>\n"#
        }#
        ##
        if(length(locale) > 2) {#
          ##
          pagesline <- paste("\t\t<Pages>", locale[3], "</Pages>\n", sep = "")#
          ##
        } else {#
          ##
          pagesline <- "\t\t<Pages/>\n"#
        }#
        ##
        fulllines <- paste(authorline, yearline, titleline, journalline, volumeline, pagesline, "\t\t<Booktitle/>\n", "\t\t<Publisher/>\n", "\t\t<City/>\n","\t\t<Editor/>\n", sep = "")#
      }#
    }#
    ##
    results <- c(results, fulllines)#
  }#
}#
#
# Collapse to just unique references (not sure how duplicates ended up in here...):#
results <- sort(unique(results))#
#
# Create empty vector to store hypothetical file names:#
filenames <- vector(mode = "character")#
#
# For each reference:#
for(i in 1:length(results)) {#
  # Isolate authors:#
  authors <- strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]][which(nchar(strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]]) > 0)]#
  # Isolate surnames:#
  surnames <- unlist(lapply(strsplit(authors, split = ","), '[', 1))#
  # Get publication year:#
  year <- gsub(" ", "", strsplit(gsub("\n|\t", "", results[i]), split = "<Year>|</Year>")[[1]][2])#
  # If a single author:#
  if(length(surnames) == 1) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames, year, sep = "_"))))#
  # If two authors:#
  if(length(surnames) == 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(paste(surnames, collapse = "_et_"), year, sep = "_"))))#
  # If more than two authors:#
  if(length(surnames) > 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames[1], "etal", year, sep = "_"))))#
}#
#
# Isolate references that have multiple file names (i.e., two or more refrences could be contracted to the same name):#
duplicates <- unique(filenames[duplicated(filenames)])#
#
# Set working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Get list of folders:#
folder.list <- list.files()[-grep("\\.", list.files())]#
#
# Get full paths for each folder:#
for(i in 1:length(folder.list)) folder.list[i] <- paste(getwd(), "/", folder.list[i], sep = "")#
#
# Vector for storing nexus file list:#
file.list <- vector(mode = "character")#
#
# Find all file paths for nexus files:#
for(i in 1:length(folder.list)) {#
  # Set working directory for current folder:#
  setwd(folder.list[i])#
  # Look for NEXUS files:#
  if(length(grep(".nex", list.files())) > 0) {#
    # Add any found to file list:#
    file.list <- c(file.list, paste(folder.list[i], "/", list.files()[grep(".nex", list.files())], sep = ""))#
  }#
}#
#
# Get just the NEXUS file names:#
nexus.files <- unlist(lapply(strsplit(file.list, "/"), '[', 9))#
#
# Reset working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Create vector to store multiple hits:#
multi_hitters <- vector(mode = "character")#
#
# Set scratch counter:#
scratch_counter <- 1#
#
# Create nexus, tnt and xml files:#
for(i in 1:length(file.list)) {#
  # Start feedback:#
  cat("Attempting to read: ", file.list[i], "...")#
  # Get stripped verion of name (i.e., missing a, b, aa etc. ending):#
  stripped_name <- gsub(strsplit(nexus.files[i], "[:0-9:]{4}|inpress")[[1]][2], "", nexus.files[i])#
  # Get hits for stripped name in filenames:#
  hits <- grep(stripped_name, filenames)#
  # Check there is a match:#
  if(length(hits) == 0) stop("No reference with matching name.")#
  # Create reference info:#
  reference_info <- paste(results[hits], collapse = "\n\nOR\n\n")#
  # If multiple hits add to list so these can be manually checked later:#
  if(length(hits) > 1) multi_hitters <- c(multi_hitters, nexus.files[i])#
  # Read in matrix:#
  mymatrix <- ReadMorphNexus(file.list[i])#
  # Update header text:#
  mymatrix$Topper$Header <- "File downloaded from graemetlloyd.com"#
  # Make file name:#
  file.name <- gsub(".nex", "", strsplit(file.list[i], "/")[[1]][length(strsplit(file.list[i], "/")[[1]])])#
  # Write out NEXUS data:#
  WriteMorphNexus(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus", "/", file.name, ".nex", sep = ""))#
  # Write out TNT data:#
  WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/tnt", "/", file.name, ".tnt", sep = ""))#
  # Write out TNT for analysis:#
  WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""), add.analysis.block = TRUE)#
  TNTFA <- readLines(paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""))#
  # If scratch.tre is found:#
  if(length(grep("scratch.tre", TNTFA, fixed = TRUE)) > 0) {#
    # Replace scratch.tre with numbered version:#
    TNTFA <- gsub("scratch.tre", paste("scratch", scratch_counter, ".tre", sep = ""), TNTFA, fixed = TRUE)#
    # Overwrite TNT for analysis with numbered scratch.tre:#
    write(TNTFA, paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""))#
    # Increment scratch counter:#
    scratch_counter <- scratch_counter + 1#
  }#
  # Make XML file:#
  myxml <- paste(paste("<?xml version=\"1.0\" standalone=\"yes\"?>\n<SourceTree>\n\t<Source>\n", reference_info, "\t</Source>"), paste("\t<Taxa number=\"", length(mymatrix$Matrix_1$Matrix[, 1]), "\">", sep = ""), paste(paste("\t\t<List recon_name=\"DELETE\" recon_no=\"-1\">", rownames(mymatrix$Matrix_1$Matrix), "</List>", sep = ""), collapse = "\n"), "\t</Taxa>\n\t<Characters>\n\t\t<Molecular/>", paste("\t\t<Morphological number=\"", sum(unlist(lapply(lapply(mymatrix[2:length(mymatrix)], '[[', "Matrix"), ncol))), "\">", sep = ""), "\t\t\t<Type>Osteology</Type>\n\t\t</Morphological>\n\t\t<Behavioural/>\n\t\t<Other/>\n\t</Characters>\n\t<Analysis>\n\t\t<Type>Maximum Parsimony</Type>\n\t</Analysis>\n\t<Notes>Based on reanalysis of the original matrix.</Notes>", paste("\t<Filename>", gsub("\\.nex", "", strsplit(file.list[i], "/")[[1]][length(strsplit(file.list[i], "/")[[1]])]), "</Filename>", sep = ""), "\t<Parent/>\n\t<Sibling/>\n</SourceTree>", sep = "\n")#
  # Write out XML file:#
  write(myxml, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/xml", "/", file.name, ".xml", sep = ""))#
  # Feedback:#
  cat("Done\n")#
}#
#
# List multiple hitters for checking:#
sort(multi_hitters)
# Open libraries:#
library(Claddis)#
library(metatree)#
#
# Set working directory:#
setwd("/Users/eargtl")#
#
# Get list of mrp files:#
mrp.list <- list.files()[grep("mrp.nex", list.files())]#
#
# Get list of mrp files:#
trees.list <- list.files()[grep("mpts_plus_strict.nex", list.files())]#
#
# Make tree files:#
for(i in 1:length(trees.list)) {#
  # Read in TNT trees and split into mpts and strict consensus:#
  mytrees <- Trees2MPTsAndStrict(trees.list[i])#
  # If the tree limit of 100000 was hit (i.e., not all MPTs are guranteed to have been sampled) or no MRP could be created due to sheer number of trees:#
  if(length(mytrees$mpts) == 100000 || sum(mrp.list == gsub("tntmpts_plus_strict.nex", "mrp.nex", trees.list[i])) == 0) {#
    # Create MRP filename:#
    mrp.filename <- gsub("tntmpts_plus_strict.nex", "mrp.nex", trees.list[i])#
    # If MRP file was generated:#
    if(length(which(mrp.list == mrp.filename)) > 0) {#
      # Remove from MRP list:#
      mrp.list <- mrp.list[-which(mrp.list == mrp.filename)]#
      # Delete raw file as likely too big anyway:#
      file.remove(mrp.filename)#
    }#
    # Create nexus file name:#
    nexus.filename <- gsub("mrp\\.nex", ".nex", mrp.filename)#
    # Read in original matrix:#
    mymatrix <- ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus", "/", nexus.filename, sep = ""))#
    # Write out regular TNT file:#
    WriteMorphTNT(mymatrix, gsub("\\.nex", ".tnt", nexus.filename))#
    # Read in TNT lines:#
    TNT.lines <- readLines(gsub("\\.nex", ".tnt", nexus.filename))#
    # Get TNT data block:#
    tnt.block <- TNT.lines[1:(grep("proc/;", TNT.lines) - 1)]#
    # Create analysis block:#
    anal.block <- paste(c("rseed*;\nhold 999;\nxmult=rss fuse 50 drift 50 ratchet 50;\nmult 50 =tbr drift;\ntsave scratch.tre;\nsave;\ntsave /;", rep("rseed*;\nhold 999;\nxmult=rss fuse 50 drift 50 ratchet 50;\nmult 50 =tbr drift;\ntsave scratch.tre +;\nsave;\ntsave /;", 4), "hold 5000;\nshortread scratch.tre;\nbbreak=tbr;"), collapse = "\n")#
    # Cretae empty vector to store final block:#
    full.block <- vector(mode = "character")#
    # Fill out all blocks for analysis:#
    for(j in 1:20) full.block <- c(full.block, paste(paste(tnt.block, collapse = "\n"), anal.block, "mrp;", paste("export ", gsub("\\.nex", "", nexus.filename), "mrp_", j, ".nex;", sep = ""), sep = "\n"))#
    # Write out TNT file:#
    write(paste(paste(full.block, collapse = "\n"), "\nproc/;\n", sep = ""), gsub("\\.nex", ".tnt", nexus.filename))#
  }#
  # Make file name:#
  file.name <- gsub("tntmpts_plus_strict.nex", "", trees.list[i])#
  # Write out MPTs:#
  write(mytrees$mpts, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mpts", "/", file.name, ".tre", sep = ""))#
  # Write out first MPT:#
  write(mytrees$mpts[1], paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/firstmpt", "/", file.name, ".tre", sep = ""))#
  # Write out strict consensus:#
  write(mytrees$strict, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/sc", "/", file.name, ".tre", sep = ""))#
  # Delete trees file now no longer needed:#
  file.remove(trees.list[i])#
  # Spit out loop position:#
  cat(i, " ")#
}#
#
# Make mrp files:#
for(i in 1:length(mrp.list)) {#
  # Add assumptions block to MRP:#
  x <- paste(c(readLines(mrp.list[i]), "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")#
  # Write out MRP file with assumptions added (can then be read in with ReadMorphNexus):#
  write(x = x, file = mrp.list[i])#
  # Read in MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Remove root taxon:#
  mymrp$Matrix_1$Matrix <- mymrp$Matrix_1$Matrix[-which(rownames(mymrp$Matrix_1$Matrix) == "ROOT"), ]#
  # Collapse to just unique characters:#
  mymrp <- CompactifyMatrix(mymrp)#
  # Overwrite weights (set all to one):#
  mymrp$Matrix_1$Weights <- rep(1, length(mymrp$Matrix_1$Weights))#
  # Make file name:#
  file.name <- gsub(".nex", "", mrp.list[i])#
  # Isolate MPR taxon names:#
  mrp.names <- rownames(mymrp$Matrix_1$Matrix)#
  # Isolate full names:#
  nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)#
  # Check to see if MRP names are contracted:#
  if(length(setdiff(mrp.names, nexus.names)) > 0) {#
    # List all contracted names:#
    contracted.names <- setdiff(mrp.names, nexus.names)#
    # For each contracted name:#
    for(j in 1:length(contracted.names)) {#
      # Get matching full name(s):#
      full.name <- nexus.names[grep(contracted.names[j], nexus.names)]#
      # Check that there are not multiple matches:#
      if(length(full.name) > 1) stop("Multiple names match contracted form. Check manually.")#
      # Overwrite contracted name with full name:#
      rownames(mymrp$Matrix_1$Matrix)[which(rownames(mymrp$Matrix_1$Matrix) == contracted.names[j])] <- full.name#
    }#
  }#
  # Write out MRP in #NEXUS format:#
  WriteMorphNexus(mymrp, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, ".nex", sep = ""))#
  # Delete file once finished:#
  file.remove(mrp.list[i])#
  # Spit out loop position:#
  cat(i, " ")#
}
i
mrp.list[i]
# Make mrp files:#
for(i in 5:length(mrp.list)) {#
  # Add assumptions block to MRP:#
  x <- paste(c(readLines(mrp.list[i]), "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")#
  # Write out MRP file with assumptions added (can then be read in with ReadMorphNexus):#
  write(x = x, file = mrp.list[i])#
  # Read in MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Remove root taxon:#
  mymrp$Matrix_1$Matrix <- mymrp$Matrix_1$Matrix[-which(rownames(mymrp$Matrix_1$Matrix) == "ROOT"), ]#
  # Collapse to just unique characters:#
  mymrp <- CompactifyMatrix(mymrp)#
  # Overwrite weights (set all to one):#
  mymrp$Matrix_1$Weights <- rep(1, length(mymrp$Matrix_1$Weights))#
  # Make file name:#
  file.name <- gsub(".nex", "", mrp.list[i])#
  # Isolate MPR taxon names:#
  mrp.names <- rownames(mymrp$Matrix_1$Matrix)#
  # Isolate full names:#
  nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)#
  # Check to see if MRP names are contracted:#
  if(length(setdiff(mrp.names, nexus.names)) > 0) {#
    # List all contracted names:#
    contracted.names <- setdiff(mrp.names, nexus.names)#
    # For each contracted name:#
    for(j in 1:length(contracted.names)) {#
      # Get matching full name(s):#
      full.name <- nexus.names[grep(contracted.names[j], nexus.names)]#
      # Check that there are not multiple matches:#
      if(length(full.name) > 1) stop("Multiple names match contracted form. Check manually.")#
      # Overwrite contracted name with full name:#
      rownames(mymrp$Matrix_1$Matrix)[which(rownames(mymrp$Matrix_1$Matrix) == contracted.names[j])] <- full.name#
    }#
  }#
  # Write out MRP in #NEXUS format:#
  WriteMorphNexus(mymrp, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, ".nex", sep = ""))#
  # Delete file once finished:#
  file.remove(mrp.list[i])#
  # Spit out loop position:#
  cat(i, " ")#
}
i=4
# Add assumptions block to MRP:#
  x <- paste(c(readLines(mrp.list[i]), "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")
x
# Read in MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])
# Remove root taxon:#
  mymrp$Matrix_1$Matrix <- mymrp$Matrix_1$Matrix[-which(rownames(mymrp$Matrix_1$Matrix) == "ROOT"), ]
# Collapse to just unique characters:#
  mymrp <- CompactifyMatrix(mymrp)
# Overwrite weights (set all to one):#
  mymrp$Matrix_1$Weights <- rep(1, length(mymrp$Matrix_1$Weights))
# Make file name:#
  file.name <- gsub(".nex", "", mrp.list[i])
# Isolate MPR taxon names:#
  mrp.names <- rownames(mymrp$Matrix_1$Matrix)
# Isolate full names:#
  nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)
ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))
gsub("mrp", "", file.name)
ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))
# Read in MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Remove root taxon:#
  mymrp$Matrix_1$Matrix <- mymrp$Matrix_1$Matrix[-which(rownames(mymrp$Matrix_1$Matrix) == "ROOT"), ]#
  # Collapse to just unique characters:#
  mymrp <- CompactifyMatrix(mymrp)#
  # Overwrite weights (set all to one):#
  mymrp$Matrix_1$Weights <- rep(1, length(mymrp$Matrix_1$Weights))
# Make file name:#
  file.name <- gsub(".nex", "", mrp.list[i])#
  # Isolate MPR taxon names:#
  mrp.names <- rownames(mymrp$Matrix_1$Matrix)
# Write out MRP in #NEXUS format:#
  WriteMorphNexus(mymrp, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, ".nex", sep = ""))#
  # Delete file once finished:#
  file.remove(mrp.list[i])
library(Claddis)
x <- ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/nexus/Hartman_etal_2019a.nex")
y <- SafeTaxonomicReduction(x)
names(y)
y$removed.matrix
# Get functions in:#
library(Claddis)#
library(ade4)#
library(foreach)#
library(doParallel)#
#
# Register parallel back end as number of cores available:#
registerDoParallel(cores = 4)#
#
# Set working directory:#
setwd("/Users/eargtl")#
#
# Get list of mrp files:#
mrp.list <- list.files()[grep("mrp_", list.files())]#
#
# For each MRP file:#
for(i in 1:length(mrp.list)) {#
  # Read in raw MRP file:#
  x <- readLines(mrp.list[i])#
  # If there is no assumptions block:#
  if(length(grep("begin assumptions", x, ignore.case = TRUE)) == 0) {#
    # Add assumptions block to MRP:#
    x <- paste(c(x, "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")#
    # Write out MRP file with assumptions added (can then be read in with ReadMorphNexus):#
    write(x = x, file = mrp.list[i])#
  }#
}#
#
# Make mrp files:#
x <- foreach(i = 1:length(mrp.list), .combine = "rbind") %dopar% {#
#
  # Read in ith MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Compactify the matrix:#
  mymrp <- CompactifyMatrix(mymrp)#
  # To avoid non-numeric weight (e.g., 1+e05) set all weights above ten to ten:#
  if(sum(mymrp$Matrix_1$Weights > 10)) mymrp$Matrix_1$Weights[which(mymrp$Matrix_1$Weights > 10)] <- 10#
  # If any rogue NAs are found prune these from the data:#
  if(length(unique(as.vector(mymrp$Matrix_1$Matrix))) > 2) mymrp <- MatrixPruner(mymrp, characters2prune = which((apply(apply(mymrp$Matrix_1$Matrix, 2, '==', "0") + apply(mymrp$Matrix_1$Matrix, 2, '==', "1"), 2, sum)) < nrow(mymrp$Matrix_1$Matrix)))#
#
  # Overwrite original data with compactified version:#
  WriteMorphNexus(mymrp, mrp.list[i])#
}#
#
# Get unique data set names:#
data.sets <- unique(matrix(unlist(strsplit(mrp.list, "mrp_")), ncol = 2, byrow = TRUE)[, 1])#
#
# For each data set:#
for(i in 1:length(data.sets)) {#
  # Get numbers for files to read in:#
  files.to.load <- grep(data.sets[i], mrp.list)#
  # For each file in data set:#
  for(j in files.to.load) {#
    # Read in current matrix:#
    current.matrix <- ReadMorphNexus(mrp.list[j])#
    # Sort by row name to ensure taxa line up later:#
    current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[sort(rownames(current.matrix$Matrix_1$Matrix)), ]#
    # If first file of data set:#
    if(files.to.load[1] == j) {#
      # Set matrix using current matrix:#
      MATRIX <- current.matrix$Matrix_1$Matrix#
      # Set weights using current matrix:#
      WEIGHTS <- current.matrix$Matrix_1$Weights#
    # If not first file of data set:#
    } else {#
      # Add current matrix to data set:#
      MATRIX <- cbind(MATRIX, current.matrix$Matrix_1$Matrix)#
      # Add current weights to data set:#
      WEIGHTS <- c(WEIGHTS, current.matrix$Matrix_1$Weights)#
    }#
  }#
  # Overwrite current matrix with full data set:#
  current.matrix$Matrix_1$Matrix <- MATRIX#
  # Overwrite current matrix weights with full data set:#
  current.matrix$Matrix_1$Weights <- WEIGHTS#
  # Set ordering for full data set:#
  current.matrix$Matrix_1$Ordering <- rep("unord", ncol(current.matrix$Matrix_1$Matrix))#
  # Set maximum values for full data set:#
  current.matrix$Matrix_1$MinVals <- rep(1, ncol(current.matrix$Matrix_1$Matrix))#
  # Set minimum values for full data set:#
  current.matrix$Matrix_1$MaxVals <- rep(0, ncol(current.matrix$Matrix_1$Matrix))#
  # Collapse data set:#
  current.matrix <- CompactifyMatrix(current.matrix)#
  # To avoid non-numeric weight (e.g., 1+e05) set all weights above ten to ten:#
  if(sum(current.matrix$Matrix_1$Weights > 10)) current.matrix$Matrix_1$Weights[which(current.matrix$Matrix_1$Weights > 10)] <- 10#
  # Make file name:#
  file.name <- data.sets[i]#
  # Case if MRP is done (minimum weight is greater than 1):#
  if(min(current.matrix$Matrix_1$Weights) > 1) {#
    # Remove "ROOT" taxon if present:#
    if(sum(rownames(current.matrix$Matrix_1$Matrix) == "ROOT") > 0) current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[-which(rownames(current.matrix$Matrix_1$Matrix) == "ROOT"), ]#
    # Collapse matrix again:#
    current.matrix <- CompactifyMatrix(current.matrix)#
    # Overwrite all weights with 1:#
    current.matrix$Matrix_1$Weights <- rep(1, length(current.matrix$Matrix_1$Weights))#
    # Update matrix in nesting order (outgroup first):#
    current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[names(sort(apply(apply(current.matrix$Matrix_1$Matrix, 1, as.numeric), 2, sum))), ]#
    # Isolate MRP taxon names:#
    mrp.names <- rownames(current.matrix$Matrix_1$Matrix)#
    # Isolate full names:#
    nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)#
    # Check to see if MRP names are contracted:#
    if(length(setdiff(mrp.names, nexus.names)) > 0) {#
      # List all contracted names:#
      contracted.names <- setdiff(mrp.names, nexus.names)#
      # For each contracted name:#
      for(j in 1:length(contracted.names)) {#
        # Get matching full name(s):#
        full.name <- nexus.names[grep(contracted.names[j], nexus.names)]#
        # Check that there are not multiple matches:#
        if(length(full.name) > 1) stop("Multiple names match contracted form. Check manually.")#
        # Overwrite contracted name with full name:#
        rownames(current.matrix$Matrix_1$Matrix)[which(rownames(current.matrix$Matrix_1$Matrix) == contracted.names[j])] <- full.name#
      }#
    }#
    # Write out MRP in #NEXUS format:#
    WriteMorphNexus(current.matrix, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, "mrp.nex", sep = ""))#
    # Remove dead files:#
    file.remove(c(mrp.list[files.to.load], paste(file.name, ".tnt", sep = "")))#
  # Case if MRP needs to continue (minimum weight is 1):#
  } else {#
    # Remove dead files:#
    file.remove(mrp.list[files.to.load])#
    # Write out MRP in #NEXUS format:#
    WriteMorphNexus(current.matrix, paste("/Users/eargtl/", file.name, "mrp_0.nex", sep = ""))#
  }#
  # Output loop position:#
  cat(i, " ")#
}
# CODE TO FILL OUT SAFE FIRST GUESSES FOR TAXON RECONCILIATION WHERE MISSING FROM XML FILES#
# I.E., USES EXACT SPECIES NAMES AND PALEOBIOLOGY DATABASE OR USES RECONCILIATION FROM PARENT DATASET FOR EXACT SAME OTU OTHERWISE OTUS ARE NOT ALTERED AT ALL#
#
# Load libraries:#
library(metatree)#
library(Claddis)#
#
# Set working directory to HTML files:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get list of matrix HTMLs:#
MatrixHTMLfiles <- list.files()[grep("matr[:a-z:]{4}.html", list.files())]#
#
# Create empty HTML list:#
MatrixHTML <- list()#
#
# Read in each raw HTML code into list:#
for(i in 1:length(MatrixHTMLfiles)) MatrixHTML[[i]] <- readLines(MatrixHTMLfiles[i])#
#
# Add file names to HTML list:#
names(MatrixHTML) <- gsub(",html", "", MatrixHTMLfiles)#
#
# Set working directory as XML folder:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# List all XML files:#
xmlfiles <- list.files()#
#
# Create empty vector to store parent datasets:#
parentdataset <- vector(mode = "character")#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    parentdataset <- c(parentdataset, ifelse(length(grep("<Parent>", currentxml)) > 0, strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3], ""))#
}#
#
# Add file names to parent data set vector:#
names(parentdataset) <- gsub(".xml", "", xmlfiles)#
#
# Get vector of "dead" parents, i.e., those not currently present in the data pool:#
deadparents <- sort(setdiff(unique(parentdataset), names(parentdataset)))[which(nchar(sort(setdiff(unique(parentdataset), names(parentdataset)))) > 0)]#
#
# Remove dead parents from parent list:#
for(i in deadparents) parentdataset[which(parentdataset == i)] <- ""#
#
# Create empty parent depth vector (number of links to original data set):#
parentdepth <- vector(mode = "numeric")#
#
# For each data set:#
for(i in names(parentdataset)) {#
    # Set starting depth at zero (no parent at all):#
    currentdepth <- 0#
    # If there is at least an initial parent:#
    if(parentdataset[i] != "") {#
        # Increase depth by one:#
        currentdepth <- currentdepth + 1#
        # While there are further parents:#
        while(parentdataset[i] != "") {#
            # Increase depth by one#
            currentdepth <- currentdepth + 1#
            # Update new parent:#
            i <- parentdataset[i]#
        }#
    }#
    # Add parent depth to vector:#
    parentdepth <- c(parentdepth, currentdepth)#
}#
#
# Reorder xml file lists by parent depth (ensures parents are filled out before children so names can always be carried forwards):#
xmlfiles <- xmlfiles[order(parentdepth, decreasing = FALSE)]#
#
# Count of total OTUs (start with zero):#
TotalOTUs <- 0#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    if(length(grep("<Parent>", currentxml)) > 0) {#
        # Get parent data set file name:#
        parentdataset <- strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3]#
        # Check parent has been processed (has an XML file on which to draw):#
        if(!is.na(match(paste(parentdataset, ".xml", sep = ""), xmlfiles))) {#
            # Isolate taxon names block:#
            taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
            # Reformat as matrix:#
            taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
            # Add number of OTUs to count:#
            TotalOTUs <- TotalOTUs + nrow(taxonnameblock)#
            # If there are unreconciled taxa:#
            if(any(taxonnameblock[, "ReconNo"] == "-1")) {#
                # Get just the unreconciled names (we don't care about ones already done):#
                unreconcilednames <- taxonnameblock[which(taxonnameblock[, "ReconNo"] == "-1"), "OTUName"]#
                # Read in ith XML file:#
                parentxml <- readLines(paste(parentdataset, ".xml", sep = ""))#
                # Isolate taxon names block:#
                parenttaxonnameblock <- parentxml[(grep("<Taxa", parentxml) + 1):(grep("</Taxa", parentxml) - 1)]#
                # Reformat as matrix:#
                parenttaxonnameblock <- matrix(unlist(lapply(strsplit(parenttaxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
                # If at least one name can be reconciled using parent data set data:#
                if(any(!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"])))) {#
                    # Update unreconciled names as just those also present in parent data:#
                    unreconcilednames <- unreconcilednames[!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"]))]#
#
                    # Update taxon names block with parent data:#
                    taxonnameblock[match(unreconcilednames, taxonnameblock[, "OTUName"]), ] <- parenttaxonnameblock[match(unreconcilednames, parenttaxonnameblock[, "OTUName"]), ]#
                }#
            }#
            # Add taxonblock back into currentxml:#
            currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)] <- paste("\t\t<List recon_name=\"", taxonnameblock[, "ReconName"], "\" recon_no=\"", taxonnameblock[, "ReconNo"], "\">", taxonnameblock[, "OTUName"], "</List>", sep = "")#
#
        }#
#
    }#
    # Write out XML:#
    write(x = paste(currentxml, collapse = "\n"), file = i)#
}#
#
# Set empty priorites matrix:#
Priorities <- matrix(nrow = 0, ncol = 4, dimnames = list(c(), c("Name", "NUnreconciled", "ParentDepth", "HTMLFile")))#
#
# For each XML file:#
for(i in xmlfiles) {#
  # Read in ith XML file:#
  currentxml <- readLines(i)#
  # Work out which html file the data set belongs to:#
  currenthtml <- names(which(unlist(lapply(lapply(MatrixHTML, grep, pattern = paste("xml/", i, sep = "")), length)) == 1))#
  # Isolate taxon names block:#
  taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
  # Reformat as matrix:#
  taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
  # Add to priorities matrix:#
  Priorities <- rbind(Priorities, c(i, sum(taxonnameblock[, "ReconNo"] == "-1"), parentdepth[order(parentdepth, decreasing = FALSE)][which(xmlfiles == i)], currenthtml))#
}#
#
# Remove any XMLs where all taxa are already reconciled:#
Priorities <- Priorities[-which(Priorities[, "NUnreconciled"] == "0"), , drop = FALSE]#
#
# Number of files that still need taxa reconciled:#
paste(nrow(Priorities), " files still contain unreconciled taxa (", round(((length(xmlfiles) - nrow(Priorities)) / length(xmlfiles) * 100), 2), "% complete)", sep = "")#
#
# Number of individual OTU names that still need reconciling:#
paste(sum(as.numeric(Priorities[, "NUnreconciled"])), " OTUs are still unreconciled (", round(((TotalOTUs - sum(as.numeric(Priorities[, "NUnreconciled"]))) / TotalOTUs * 100), 2), "% complete)", sep = "")#
#
# Order by number of unreconciled OTUs:#
Priorities <- Priorities[order(as.numeric(Priorities[, "NUnreconciled"])), ]#
#
# Display priorities for archosaurs (excluding Cenozoic birds):#
Priorities[sort(c(which(Priorities[, "HTMLFile"] == "matrarch.html"), which(Priorities[, "HTMLFile"] == "matrdino.html"))), ]#
#
# Empty vector to store taxon names:#
newtaxonnames <- vector(mode = "character")#
#
# For each XML file:#
for(i in xmlfiles) {#
#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # Isolate taxon names block:#
    taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
    # Convert into matrix:#
    taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE)#
    # Add any unique taxon names to the vector:#
    newtaxonnames <- sort(unique(c(newtaxonnames, gsub("_", " ", taxonnameblock[which(taxonnameblock[, 2] == "-1"), 3]))))#
#
}#
#
# Create empty resolved names matrix:#
resolvednames <- matrix(nrow = 0, ncol = 9, dimnames = list(c(), c("InputName", "OriginalTaxonNo", "ResolvedTaxonNo", "TaxonName", "TaxonRank", "ParentTaxonNo", "TaxonValidity", "AcceptedNumber", "AcceptedName")))#
#
# Get unique long names (i.e., those with a space in them):#
UniqueLongNames <- newtaxonnames[grep(" ", newtaxonnames)]#
#
# Get down to just binomials:#
UniqueLongNames <- UniqueLongNames[unlist(lapply(strsplit(UniqueLongNames, split = " "), length)) == 2]#
#
# Get rid of any "sp" names:#
UniqueLongNames <- UniqueLongNames[!unlist(lapply(lapply(strsplit(UniqueLongNames, split = " "), '==', "sp"), any))]#
#
# Get rid of any "sp" names:#
UniqueLongNames <- UniqueLongNames[!unlist(lapply(lapply(strsplit(UniqueLongNames, split = " "), '==', "indet"), any))]#
#
# Remove names with numbers in them (if found):#
if(length(grep("[:0-9:]", UniqueLongNames)) > 0) UniqueLongNames <- UniqueLongNames[-grep("[:0-9:]", UniqueLongNames)]#
#
# Cut out any names with only a single character part (can't be a proper name):#
UniqueLongNames <- UniqueLongNames[!unlist(lapply(lapply(lapply(strsplit(UniqueLongNames, split = " "), nchar), '==', 1), any))]#
#
# For each (potential) species name (i.e., names containing a space):#
for(i in UniqueLongNames) {#
    # Set empty variable x as NA:#
    x <- NA#
    # Attempt to query current name in database:#
    try(x <- PaleobiologyDBTaxaQuerier(taxon_no = "1", taxon_name = i), silent = TRUE)#
    # If query works then add to resolved names matrix:#
    if(length(x) > 1) resolvednames <- rbind(resolvednames, c(i, unlist(x)))#
    # Close all connections (stops limiting factor in R):#
    closeAllConnections()#
    # Spit out loop position (i.e., current name attempting to reconcile):#
    cat(i, "\n")#
#
}#
#
# Check names match up (or remove those that return a different name):#
resolvednames <- resolvednames[which(resolvednames[, "TaxonName"] == resolvednames[, "InputName"]), ]#
#
# Check names are only regular genus species (no subgenera or subspecies) as I am not prepared to deal with these yet!:#
resolvednames <- resolvednames[which(nchar(gsub("[:A-Z:a-z:]", "", resolvednames[, "TaxonName"])) == 1), ]#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # Isolate taxon names block:#
    taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
    # For each taxon in block:#
    for(j in 1:length(taxonnameblock)) {#
        # Isolate just elements of taxonomic reconciliation from taxon string:#
        splitdata <- strsplit(gsub("\"", "", taxonnameblock[j], fixed = TRUE), "recon_name=| |recon_no=|<|>")[[1]][c(4, 6, 7)]#
        # Split out data into subvariables:#
        currentreconname <- splitdata[1]#
        currentreconnumber <- splitdata[2]#
        originalOTUname <- splitdata[3]#
        # If taxon not currently reconciled:#
        if(currentreconnumber == "-1") {#
            # Check to see if in resolved names list (returns row number if found):#
            rownumber <- which(resolvednames[, 1] == gsub("_", " ", originalOTUname))#
            # If a single reconciled name found:#
            if(length(rownumber) == 1) {#
                # Update reconnumber with new information:#
                currentreconnumber <- gsub("txn:|var:", "", rev(sort(resolvednames[rownumber, c("OriginalTaxonNo", "ResolvedTaxonNo")]))[[1]])#
                # Update taxon name block:#
                taxonnameblock[j] <- paste("\t\t<List recon_name=\"", originalOTUname, "\" recon_no=\"", currentreconnumber, "\">", originalOTUname, "</List>", sep = "")#
            }#
        }#
    }#
    # Update current xml with new taxon name block:#
    currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)] <- taxonnameblock#
#
    # Write out new XML:#
    write(x = paste(currentxml, collapse = "\n"), file = i)#
#
}#
#
# Create empty to do list of remaining data sets:#
newtodolist <- vector(mode = "numeric")#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # Isolate taxon names block:#
    taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
    # Convert into matrix:#
    taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE)#
    # If data set has any unresolved names:#
    if(any(taxonnameblock[, 2] == "-1")) {#
        # Count number still to do and add to list:#
        newtodolist <- c(newtodolist, sum(taxonnameblock[, 2] == "-1"))#
        # Add data set name to list:#
        names(newtodolist)[length(newtodolist)] <- i#
        # Reorder list:#
        newtodolist <- newtodolist[order(newtodolist)]#
    }#
}
# CODE TO FILL OUT SAFE FIRST GUESSES FOR TAXON RECONCILIATION WHERE MISSING FROM XML FILES#
# I.E., USES EXACT SPECIES NAMES AND PALEOBIOLOGY DATABASE OR USES RECONCILIATION FROM PARENT DATASET FOR EXACT SAME OTU OTHERWISE OTUS ARE NOT ALTERED AT ALL#
#
# Load libraries:#
library(metatree)#
library(Claddis)#
#
# Set working directory to HTML files:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get list of matrix HTMLs:#
MatrixHTMLfiles <- list.files()[grep("matr[:a-z:]{4}.html", list.files())]#
#
# Create empty HTML list:#
MatrixHTML <- list()#
#
# Read in each raw HTML code into list:#
for(i in 1:length(MatrixHTMLfiles)) MatrixHTML[[i]] <- readLines(MatrixHTMLfiles[i])#
#
# Add file names to HTML list:#
names(MatrixHTML) <- gsub(",html", "", MatrixHTMLfiles)#
#
# Set working directory as XML folder:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# List all XML files:#
xmlfiles <- list.files()#
#
# Create empty vector to store parent datasets:#
parentdataset <- vector(mode = "character")#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    parentdataset <- c(parentdataset, ifelse(length(grep("<Parent>", currentxml)) > 0, strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3], ""))#
}#
#
# Add file names to parent data set vector:#
names(parentdataset) <- gsub(".xml", "", xmlfiles)#
#
# Get vector of "dead" parents, i.e., those not currently present in the data pool:#
deadparents <- sort(setdiff(unique(parentdataset), names(parentdataset)))[which(nchar(sort(setdiff(unique(parentdataset), names(parentdataset)))) > 0)]#
#
# Remove dead parents from parent list:#
for(i in deadparents) parentdataset[which(parentdataset == i)] <- ""#
#
# Create empty parent depth vector (number of links to original data set):#
parentdepth <- vector(mode = "numeric")#
#
# For each data set:#
for(i in names(parentdataset)) {#
    # Set starting depth at zero (no parent at all):#
    currentdepth <- 0#
    # If there is at least an initial parent:#
    if(parentdataset[i] != "") {#
        # Increase depth by one:#
        currentdepth <- currentdepth + 1#
        # While there are further parents:#
        while(parentdataset[i] != "") {#
            # Increase depth by one#
            currentdepth <- currentdepth + 1#
            # Update new parent:#
            i <- parentdataset[i]#
        }#
    }#
    # Add parent depth to vector:#
    parentdepth <- c(parentdepth, currentdepth)#
}#
#
# Reorder xml file lists by parent depth (ensures parents are filled out before children so names can always be carried forwards):#
xmlfiles <- xmlfiles[order(parentdepth, decreasing = FALSE)]#
#
# Count of total OTUs (start with zero):#
TotalOTUs <- 0#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    if(length(grep("<Parent>", currentxml)) > 0) {#
        # Get parent data set file name:#
        parentdataset <- strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3]#
        # Check parent has been processed (has an XML file on which to draw):#
        if(!is.na(match(paste(parentdataset, ".xml", sep = ""), xmlfiles))) {#
            # Isolate taxon names block:#
            taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
            # Reformat as matrix:#
            taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
            # Add number of OTUs to count:#
            TotalOTUs <- TotalOTUs + nrow(taxonnameblock)#
            # If there are unreconciled taxa:#
            if(any(taxonnameblock[, "ReconNo"] == "-1")) {#
                # Get just the unreconciled names (we don't care about ones already done):#
                unreconcilednames <- taxonnameblock[which(taxonnameblock[, "ReconNo"] == "-1"), "OTUName"]#
                # Read in ith XML file:#
                parentxml <- readLines(paste(parentdataset, ".xml", sep = ""))#
                # Isolate taxon names block:#
                parenttaxonnameblock <- parentxml[(grep("<Taxa", parentxml) + 1):(grep("</Taxa", parentxml) - 1)]#
                # Reformat as matrix:#
                parenttaxonnameblock <- matrix(unlist(lapply(strsplit(parenttaxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
                # If at least one name can be reconciled using parent data set data:#
                if(any(!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"])))) {#
                    # Update unreconciled names as just those also present in parent data:#
                    unreconcilednames <- unreconcilednames[!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"]))]#
#
                    # Update taxon names block with parent data:#
                    taxonnameblock[match(unreconcilednames, taxonnameblock[, "OTUName"]), ] <- parenttaxonnameblock[match(unreconcilednames, parenttaxonnameblock[, "OTUName"]), ]#
                }#
            }#
            # Add taxonblock back into currentxml:#
            currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)] <- paste("\t\t<List recon_name=\"", taxonnameblock[, "ReconName"], "\" recon_no=\"", taxonnameblock[, "ReconNo"], "\">", taxonnameblock[, "OTUName"], "</List>", sep = "")#
#
        }#
#
    }#
    # Write out XML:#
    write(x = paste(currentxml, collapse = "\n"), file = i)#
}#
#
# Set empty priorites matrix:#
Priorities <- matrix(nrow = 0, ncol = 4, dimnames = list(c(), c("Name", "NUnreconciled", "ParentDepth", "HTMLFile")))#
#
# For each XML file:#
for(i in xmlfiles) {#
  # Read in ith XML file:#
  currentxml <- readLines(i)#
  # Work out which html file the data set belongs to:#
  currenthtml <- names(which(unlist(lapply(lapply(MatrixHTML, grep, pattern = paste("xml/", i, sep = "")), length)) == 1))#
  # Isolate taxon names block:#
  taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
  # Reformat as matrix:#
  taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
  # Add to priorities matrix:#
  Priorities <- rbind(Priorities, c(i, sum(taxonnameblock[, "ReconNo"] == "-1"), parentdepth[order(parentdepth, decreasing = FALSE)][which(xmlfiles == i)], currenthtml))#
}#
#
# Remove any XMLs where all taxa are already reconciled:#
Priorities <- Priorities[-which(Priorities[, "NUnreconciled"] == "0"), , drop = FALSE]#
#
# Number of files that still need taxa reconciled:#
paste(nrow(Priorities), " files still contain unreconciled taxa (", round(((length(xmlfiles) - nrow(Priorities)) / length(xmlfiles) * 100), 2), "% complete)", sep = "")#
#
# Number of individual OTU names that still need reconciling:#
paste(sum(as.numeric(Priorities[, "NUnreconciled"])), " OTUs are still unreconciled (", round(((TotalOTUs - sum(as.numeric(Priorities[, "NUnreconciled"]))) / TotalOTUs * 100), 2), "% complete)", sep = "")#
#
# Order by number of unreconciled OTUs:#
Priorities <- Priorities[order(as.numeric(Priorities[, "NUnreconciled"])), ]#
#
# Display priorities for archosaurs (excluding Cenozoic birds):#
Priorities[sort(c(which(Priorities[, "HTMLFile"] == "matrarch.html"), which(Priorities[, "HTMLFile"] == "matrdino.html"))), ]
# Load gdata library:#
library(gdata)#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# Get list of xml file names):#
xml.list <- list.files()#
#
# Create empty list to store XML data:#
XML.data <- vector(mode = "character")#
#
# Import XML data and store in list:#
for(i in xml.list) XML.data[i] <- ifelse(length(grep("<Title>", trim(readLines(i)))) > 0, strsplit(trim(readLines)(i)[grep("<Title>", readLines(i))], "<Title>|</Title>")[[1]][2], strsplit(trim(readLines(i))[grep("<Booktitle>", trim(readLines(i)))], "<Booktitle>|</Booktitle>")[[1]][2])#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/mpts")#
#
# Get list of tree file names (to use later in case of .zip endings):#
trees.list <- list.files()#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get file list:#
html.file.list <- list.files()[grep("matr[a-z]{4}.html", list.files())]#
#
# Create empty list to store html:#
html.data <- list()#
#
# For each set of matrices:#
for(i in 1:length(html.file.list)) {#
  # Read in the file:#
  X <- readLines(con = html.file.list[i])#
  # Get beginning sof references:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # Get endings of references:#
  ends <- grep("</p>", X)#
  # Check p tags are closed:#
  if(length(begins) != length(ends)) stop("Opening and closing paragraph marks do not match up.")#
  # For each reference add html to list (file name, title, actual text block):#
  for(j in 1:length(begins)) html.data[[(length(html.data) + 1)]] <- c(html.file.list[i], trim(X[begins[j] + 1]), X[begins[j]:ends[j]])#
}#
#
# For each html with links:#
for(i in which(unlist(lapply(html.data, length)) > 5)) {#
  # Strip down to just reference:#
  html.data[[i]] <- html.data[[i]][1:5]#
  # Remove remaining beginning of link:#
  html.data[[i]][5] <- gsub("<br><font size=\"-1\">", "</p>", html.data[[i]][5])#
}#
#
# Empty list to store matching titles from XML in HTML:#
matchedhtmldata <- list()#
#
# For each XML file:#
for(i in 1:length(XML.data)) {#
  # Store filename for ith XML file:#
  FileName <- names(XML.data)[i]#
  # Reset i as title sentence:#
  i <- XML.data[i]#
  # Get found matches (grep) for XML title in html data:#
  foundmatches <- grep(i, unlist(lapply(html.data, '[', 2)), fixed = TRUE)#
  # If length of found matcehs is zero (no matches) stop and warn:#
  if(length(foundmatches) == 0) stop("No matches!")#
  # If multiple matches find one closest in character length (most likely outcome!) and update foundmatches accordingly:#
  if(length(foundmatches) > 1) foundmatches <- foundmatches[which(abs(nchar(i) - nchar(unlist(lapply(html.data, '[', 2))[foundmatches])) == min(abs(nchar(i) - nchar(unlist(lapply(html.data, '[', 2))[foundmatches]))))]#
  # If a single match (ideal outcome):#
  if(length(foundmatches) == 1) matchedhtmldata[[(length(matchedhtmldata) + 1)]] <- foundmatches#
  # Final check that found matches do not exceed one:#
  if(length(foundmatches) > 1) {#
    # Get first names of authors on each match:#
    FirstNames <- unlist(lapply(lapply(lapply(lapply(lapply(lapply(lapply(html.data[foundmatches], '[', 3), strsplit, split = "<p class=\"hangingindent\">"), unlist), '[', 2), strsplit, split = ", "), unlist), '[', 1))#
    # Get first name of author from ith XML file:#
    FirstName <- strsplit(strsplit(FileName, split = "[:0-9:]{4}|inpress")[[1]][1], split = "_")[[1]][1]#
    # Update found matches with matching first author surname(s):#
    foundmatches <- foundmatches[which(FirstNames == FirstName)]#
    # If no matches stop and warn user:#
    if(length(foundmatches) == 0) stop("No matches found for first author name.")#
    # If a single match (ideal outcome):#
    if(length(foundmatches) == 1) matchedhtmldata[[(length(matchedhtmldata) + 1)]] <- foundmatches#
    # If multiple matches stop and warn user:#
    if(length(foundmatches) > 1) stop("Multiple matches found for title and first author.")#
  }#
}#
#
# Now only single matches convert matchedhtml to vector:#
matchedhtmldata <- unlist(matchedhtmldata)#
#
# For each HTML file that has an XML file:#
for(i in sort(unique(unlist(matchedhtmldata)))) {#
  # Get filenames for current html matches:#
  filenames <- sort(gsub(".xml", "", names(XML.data[which(matchedhtmldata == i)])))#
  # Create empty vectors to store mpt filenames and treestrings for treevector links:#
  mptsfilenames <- treestrings <- vector(mode = "character")#
  # Set working directory to strict consensus folder:#
  setwd("~/Documents/Homepage/www.graemetlloyd.com/sc")#
  # Get tree Newick strings for treevector links:#
  for(j in paste(filenames, ".tre", sep = "")) treestrings <- c(treestrings, paste("http://supfam.cs.bris.ac.uk/TreeVector/cgi-bin/maketree.cgi?topology=", gsub("\\;", "%3B", gsub("\\)", "%29", gsub("\\(", "%28", readLines(j)))), "&treetype=-clad", sep = ""))#
  # Set working directory to MPTs folder:#
  setwd("~/Documents/Homepage/www.graemetlloyd.com/mpts")#
  # Get MPTs filenames:#
  for(j in filenames) mptsfilenames <- c(mptsfilenames, list.files()[which(lapply(strsplit(list.files(), split = j), '[[', 1) == "")])#
  # Put it all together by updating html.data with links:#
  html.data[[i]] <- gsub("</p>", paste("<br><font size=\"-1\">\n                                        ", paste(paste("<a href=\"nexus/", filenames, ".nex\" target=\"_blank\">NEXUS</a> | <a href=\"tnt/", filenames, ".tnt\" target=\"_blank\">TNT</a> | <a href=\"mpts/", mptsfilenames, "\" target=\"_blank\">MPT(s)</a> <a href=\"firstmpt/", filenames, ".tre\" target=\"_blank\">(1)</a> | <a href=\"sc/", filenames, ".tre\" target=\"_blank\">SC</a> <a href=\"", treestrings, "\" target=\"_blank\">(TV)</a> | <a href=\"mrp/", filenames, "mrp.nex\" target=\"_blank\">MRP</a> | <a href=\"xml/", filenames, ".xml\" target=\"_blank\">XML</a>", sep = ""), collapse = "<br>\n                                    "), "</font></p>", sep = ""), html.data[[i]])#
}#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# For each html file:#
for(i in html.file.list) {#
  # Read in html for ith file:#
  fullhtml <- readLines(i)#
  # Store opening lines:#
  openinglines <- paste(fullhtml[1:max(grep("<a href=\"matr.html\">", fullhtml))], collapse = "\n")#
  # Store closing lines:#
  closinglines <- paste(fullhtml[grep("<!-- InstanceEndEditable -->", fullhtml, fixed = TRUE):length(fullhtml)], collapse = "\n")#
  # Find out which references are on the ith html page:#
  currentrefs <- which(unlist(lapply(html.data, '[', 1)) == i)#
  # Build initial references block (of html code):#
  refsblock <- html.data[currentrefs]#
  # Get publication years for each reference:#
  pubyears <- trim(gsub(".", "", unlist(lapply(lapply(strsplit(unlist(lapply(refsblock, '[', 3)), ","), rev), '[', 1)), fixed = TRUE))#
  # Little check that references are in order:#
  if(any(diff(as.numeric(setdiff(pubyears, "in press"))) > 0)) stop("References out of order!")#
  # Replace lower case in with upper case In for in press stuff:#
  pubyears <- gsub("in press", "In press", pubyears)#
  # Create new refs block (for stroing them collapsed by pub year):#
  newrefsblock <- vector(mode = "character")#
  # For each unique publication year create html block:#
  for(j in unique(pubyears)) newrefsblock <- c(newrefsblock, paste(paste("                                    <h4>", j, "</h4>\n\n", sep = ""), paste(unlist(lapply(lapply(refsblock[which(pubyears == j)], '[', 3:5), paste, collapse = "\n")), collapse = "\n\n"), sep = ""))#
  # Write html to file:#
  write(paste(c(openinglines, newrefsblock, closinglines), collapse = "\n\n"), file = i)#
}
# CODE TO FILL OUT SAFE FIRST GUESSES FOR TAXON RECONCILIATION WHERE MISSING FROM XML FILES#
# I.E., USES EXACT SPECIES NAMES AND PALEOBIOLOGY DATABASE OR USES RECONCILIATION FROM PARENT DATASET FOR EXACT SAME OTU OTHERWISE OTUS ARE NOT ALTERED AT ALL#
#
# Load libraries:#
library(metatree)#
library(Claddis)#
#
# Set working directory to HTML files:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get list of matrix HTMLs:#
MatrixHTMLfiles <- list.files()[grep("matr[:a-z:]{4}.html", list.files())]#
#
# Create empty HTML list:#
MatrixHTML <- list()#
#
# Read in each raw HTML code into list:#
for(i in 1:length(MatrixHTMLfiles)) MatrixHTML[[i]] <- readLines(MatrixHTMLfiles[i])#
#
# Add file names to HTML list:#
names(MatrixHTML) <- gsub(",html", "", MatrixHTMLfiles)#
#
# Set working directory as XML folder:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# List all XML files:#
xmlfiles <- list.files()#
#
# Create empty vector to store parent datasets:#
parentdataset <- vector(mode = "character")#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    parentdataset <- c(parentdataset, ifelse(length(grep("<Parent>", currentxml)) > 0, strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3], ""))#
}#
#
# Add file names to parent data set vector:#
names(parentdataset) <- gsub(".xml", "", xmlfiles)#
#
# Get vector of "dead" parents, i.e., those not currently present in the data pool:#
deadparents <- sort(setdiff(unique(parentdataset), names(parentdataset)))[which(nchar(sort(setdiff(unique(parentdataset), names(parentdataset)))) > 0)]#
#
# Remove dead parents from parent list:#
for(i in deadparents) parentdataset[which(parentdataset == i)] <- ""#
#
# Create empty parent depth vector (number of links to original data set):#
parentdepth <- vector(mode = "numeric")#
#
# For each data set:#
for(i in names(parentdataset)) {#
    # Set starting depth at zero (no parent at all):#
    currentdepth <- 0#
    # If there is at least an initial parent:#
    if(parentdataset[i] != "") {#
        # Increase depth by one:#
        currentdepth <- currentdepth + 1#
        # While there are further parents:#
        while(parentdataset[i] != "") {#
            # Increase depth by one#
            currentdepth <- currentdepth + 1#
            # Update new parent:#
            i <- parentdataset[i]#
        }#
    }#
    # Add parent depth to vector:#
    parentdepth <- c(parentdepth, currentdepth)#
}#
#
# Reorder xml file lists by parent depth (ensures parents are filled out before children so names can always be carried forwards):#
xmlfiles <- xmlfiles[order(parentdepth, decreasing = FALSE)]#
#
# Count of total OTUs (start with zero):#
TotalOTUs <- 0#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    if(length(grep("<Parent>", currentxml)) > 0) {#
        # Get parent data set file name:#
        parentdataset <- strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3]#
        # Check parent has been processed (has an XML file on which to draw):#
        if(!is.na(match(paste(parentdataset, ".xml", sep = ""), xmlfiles))) {#
            # Isolate taxon names block:#
            taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
            # Reformat as matrix:#
            taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
            # Add number of OTUs to count:#
            TotalOTUs <- TotalOTUs + nrow(taxonnameblock)#
            # If there are unreconciled taxa:#
            if(any(taxonnameblock[, "ReconNo"] == "-1")) {#
                # Get just the unreconciled names (we don't care about ones already done):#
                unreconcilednames <- taxonnameblock[which(taxonnameblock[, "ReconNo"] == "-1"), "OTUName"]#
                # Read in ith XML file:#
                parentxml <- readLines(paste(parentdataset, ".xml", sep = ""))#
                # Isolate taxon names block:#
                parenttaxonnameblock <- parentxml[(grep("<Taxa", parentxml) + 1):(grep("</Taxa", parentxml) - 1)]#
                # Reformat as matrix:#
                parenttaxonnameblock <- matrix(unlist(lapply(strsplit(parenttaxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
                # If at least one name can be reconciled using parent data set data:#
                if(any(!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"])))) {#
                    # Update unreconciled names as just those also present in parent data:#
                    unreconcilednames <- unreconcilednames[!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"]))]#
#
                    # Update taxon names block with parent data:#
                    taxonnameblock[match(unreconcilednames, taxonnameblock[, "OTUName"]), ] <- parenttaxonnameblock[match(unreconcilednames, parenttaxonnameblock[, "OTUName"]), ]#
                }#
            }#
            # Add taxonblock back into currentxml:#
            currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)] <- paste("\t\t<List recon_name=\"", taxonnameblock[, "ReconName"], "\" recon_no=\"", taxonnameblock[, "ReconNo"], "\">", taxonnameblock[, "OTUName"], "</List>", sep = "")#
#
        }#
#
    }#
    # Write out XML:#
    write(x = paste(currentxml, collapse = "\n"), file = i)#
}#
#
# Set empty priorites matrix:#
Priorities <- matrix(nrow = 0, ncol = 4, dimnames = list(c(), c("Name", "NUnreconciled", "ParentDepth", "HTMLFile")))#
#
# For each XML file:#
for(i in xmlfiles) {#
  # Read in ith XML file:#
  currentxml <- readLines(i)#
  # Work out which html file the data set belongs to:#
  currenthtml <- names(which(unlist(lapply(lapply(MatrixHTML, grep, pattern = paste("xml/", i, sep = "")), length)) == 1))#
  # Isolate taxon names block:#
  taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
  # Reformat as matrix:#
  taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
  # Add to priorities matrix:#
  Priorities <- rbind(Priorities, c(i, sum(taxonnameblock[, "ReconNo"] == "-1"), parentdepth[order(parentdepth, decreasing = FALSE)][which(xmlfiles == i)], currenthtml))#
}#
#
# Remove any XMLs where all taxa are already reconciled:#
Priorities <- Priorities[-which(Priorities[, "NUnreconciled"] == "0"), , drop = FALSE]#
#
# Number of files that still need taxa reconciled:#
paste(nrow(Priorities), " files still contain unreconciled taxa (", round(((length(xmlfiles) - nrow(Priorities)) / length(xmlfiles) * 100), 2), "% complete)", sep = "")#
#
# Number of individual OTU names that still need reconciling:#
paste(sum(as.numeric(Priorities[, "NUnreconciled"])), " OTUs are still unreconciled (", round(((TotalOTUs - sum(as.numeric(Priorities[, "NUnreconciled"]))) / TotalOTUs * 100), 2), "% complete)", sep = "")#
#
# Order by number of unreconciled OTUs:#
Priorities <- Priorities[order(as.numeric(Priorities[, "NUnreconciled"])), ]#
#
# Display priorities for archosaurs (excluding Cenozoic birds):#
Priorities[sort(c(which(Priorities[, "HTMLFile"] == "matrarch.html"), which(Priorities[, "HTMLFile"] == "matrdino.html"))), ]
system(ls)
system("ls")
system("tnt")
system("quit;")
system("ls")
system("cd ~/")
system("ls")
system("cd")
system("ls")
system("tnt; run ~/Hartman_etal_2019a.tnt;")
system("view;")
system("cd ~)
system("cd ~")
system("ls")
system("cd ~")
system("cd /Users/eargtl")
system("ls")
system("cd /Users/eargtl; ls")
setwd("~")
list.files()
system("/Users/eargtl/runalltnt.sh")
# R script to perform strat congruence tests.#
#
# Load metatree and strap libraries:#
library(hypSpace)#
library(metatree)#
library(strap)
# Load metatree and strap libraries:#
library(hypRspace)#
library(metatree)#
library(strap)
read.table("/Users/eargtl/Documents/Publications/in prep/Strat congruence - April/Aguirre-Fernandez_etal_2009a.nex.trees")
# R script to perform strat congruence tests.#
#
# Load metatree and strap libraries:#
library(hypRspace)#
library(metatree)#
library(strap)#
BayesianTrees <- read.table("/Users/eargtl/Documents/Publications/in prep/Strat congruence - April/Aguirre-Fernandez_etal_2009a.nex.trees")
colnames(BayesianTrees)
BayesianTrees <- read.table("/Users/eargtl/Documents/Publications/in prep/Strat congruence - April/Aguirre-Fernandez_etal_2009a.nex.trees", header = TRUE)
colnames(BayesianTrees)
BayesianTrees[1:3, ]
BayesianTrees[, "tree"]
RevBayesOutput <- read.table("/Users/eargtl/Documents/Publications/in prep/Strat congruence - April/Aguirre-Fernandez_etal_2009a.nex.trees", header = TRUE)#
#
BayesianTrees <- ape::read.tree(text = RevBayesOutput[, "tree"])
RevBayesOutput <- read.table("/Users/eargtl/Documents/Publications/in prep/Strat congruence - April/Aguirre-Fernandez_etal_2009a.nex.trees", header = TRUE, stringsAsFactors=FALSE)#
#
BayesianTrees <- ape::read.tree(text = RevBayesOutput[, "tree"])#
)
RevBayesOutput <- read.table("/Users/eargtl/Documents/Publications/in prep/Strat congruence - April/Aguirre-Fernandez_etal_2009a.nex.trees", header = TRUE, stringsAsFactors=FALSE)#
#
BayesianTrees <- ape::read.tree(text = RevBayesOutput[, "tree"])
BayesianTrees
BayesianTrees[[1]]
names(BayesianTrees[[1]])
library(metatree)
MRPDirectory <- "/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp"#
  XMLDirectory <- "/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/xml"#
  TargetClade <- "Ichthyopterygia"#
  InclusiveDataList <- sort(c(GetFilesForClade("matricht.html"), "Bickelmann_etal_2009a", "Caldwell_1996a", "Chen_etal_2014ba", "Chen_etal_2014bb", "deBraga_et_Rieppel_1997a", "Gauthier_etal_1988b", "Laurin_et_Reisz_1995a", "Muller_2004a", "Reisz_etal_2011a", "Rieppel_et_Reisz_1999a", "Rieppel_et_deBraga_1996a", "Young_2003a"))#
  ExclusiveDataList <- c("Averianov_inpressa", "Bravo_et_Gaete_2015a", "Brocklehurst_etal_2013a", "Brocklehurst_etal_2015aa", "Brocklehurst_etal_2015ab", "Brocklehurst_etal_2015ac", "Brocklehurst_etal_2015ad", "Brocklehurst_etal_2015ae", "Brocklehurst_etal_2015af", "Bronzati_etal_2012a", "Bronzati_etal_2015ab", "Brusatte_etal_2009ba", "Campbell_etal_2016ab", "Carr_et_Williamson_2004a", "Carr_etal_2017ab", "Frederickson_et_Tumarkin-Deratzian_2014aa", "Frederickson_et_Tumarkin-Deratzian_2014ab", "Frederickson_et_Tumarkin-Deratzian_2014ac", "Frederickson_et_Tumarkin-Deratzian_2014ad", "Garcia_etal_2006a", "Gatesy_etal_2004ab", "Grellet-Tinner_2006a", "Grellet-Tinner_et_Chiappe_2004a", "Grellet-Tinner_et_Makovicky_2006a", "Knoll_2008a", "Kurochkin_1996a", "Lopez-Martinez_et_Vicens_2012a", "Lu_etal_2014aa", "Norden_etal_inpressa", "Pisani_etal_2002a", "Ruiz-Omenaca_etal_1997a", "Ruta_etal_2003ba", "Ruta_etal_2003bb", "Ruta_etal_2007a", "Selles_et_Galobart_2016a", "Sereno_1993a", "Sidor_2001a", "Skutschas_etal_in
pressa", "Tanaka_etal_2011a", "Toljagic_et_Butler_2013a", "Tsuihiji_etal_2011aa", "Varricchio_et_Jackson_2004a", "Vila_etal_2017a", "Wilson_2005aa", "Wilson_2005ab", "Zelenitsky_et_Therrien_2008a")#
  HigherTaxaToCollapse = c()#
  MissingSpecies = "exclude"#
  Interval = NULL#
  VeilLine = TRUE#
  SpeciesToExclude = c()#
  IncludeSpecimenLevelOTUs = TRUE#
  BackboneConstraint = NULL#
  MonophylyConstraint = NULL#
  InclusiveDataList = c()#
  ExclusiveDataList = c()#
  # New Options (requires code to actually use them)#
  ##
  # HigherTaxaToCollapse Vector can be empty.#
  # VeilLine TRUE/FALSE (will be in output)#
  # SpeciesToExclude Vector of any species to be excluded from the final metatree. E.g., Eshanosaurus, Ricardoestesia.#
  # BackboneConstraint Newick string of backbone constraint (allows taxa not in topology). NULL as default.#
  # MonophylyConstraint Newick string of monophyly constraint (excludes taxa not in topology). NULL as default.#
  # CHECK PARENT IS A DATA SET AND NOT A REFERENCE, E.G., IF ENTER A REFERENCE AS PARENT THEN PARENT TURNS OUT TO HAVE TWO DATA SETS#
  # CHECK FOR SPECIES THAT BELONG TO A GENUS DIFFERENT TO THE ONE IN THEIR NAME!#
  # NEED TO CATCH ISSUE WHERE GENUS NUMBER IS USED FOR A SPECIES (HARD TO CHECK SO FAR DUE TO INDETERMINATES CONTINGENCY)#
  # NEED SOME TEST THAT HELPS CHECK ROOT IS SENSIBLE#
  # NEED SOME TEST THAT HELPS DETERMINE IF MULTIPLE OCCURRENCES OF SAME TAXON AFTER RECONCILIATION IS CORRECT OR AN ERROR#
  # ADD MORE COMPLEX WEIGHTS BY USING ADDITIONAL CHARACTER STATES! (EACH DATASET TOTAL WEIGHT DETERMINED BY YEAR AND DEPENDENCE THEN SUBDIVIDED ACROSS CHARACTER?) - BUT THIS SEEMS TO SLOW THINGS DRAMATICALLY MAYBE DO BY DUPLICATING CHARACTERS INSTEAD#
  # MAKE STR OPTIONAL (SAVES A LITTLE TIME)#
  # CHECK THERE ARE MULTIPLE TAXA PRE-RECONCILIATION#
  # CHECK INDETS DO NOT GIVE MULTIPLE MATCHES#
  # HOW TO DELETE DATA SETS THAT STILL CONTRIBUTE TO DEPENDENCE?#
  # Subfunction that gives just MRPs where matrix is still intact (has rows and columns):#
  ActiveMRP <- function(MRPList) unname(which(unlist(lapply(MRPList, function(x) prod(dim(x$Matrix)))) > 0))#
#
  # Check MRPDirectory is formatted correctly adn stop and warn user if not:#
  if(!all(is.character(MRPDirectory)) || length(MRPDirectory) != 1) stop("MRPDirectory must be a single character string indicating the path to the folder containing the MRP files.")#
  # Check XMLDirectory is formatted correctly adn stop and warn user if not:#
  if(!all(is.character(XMLDirectory)) || length(XMLDirectory) != 1) stop("XMLDirectory must be a single character string indicating the path to the folder containing the XML files.")#
  # Check TargetClade is formatted correctly adn stop and warn user if not:#
  if(!all(is.character(TargetClade)) || length(TargetClade) != 1) stop("TargetClade must be a single character string indicating the desired clade the metatree will represent.")#
  # Check MissingSpecies respresents a valid option:#
  if(length(setdiff(MissingSpecies, c("all", "exclude", "genus"))) > 0) stop("MissingSpecies must be one of \"all\", \"exclude\", or \"genus\".")#
  # Check VeilLine is a logical and stop and warn user if not:#
  if(!is.logical(VeilLine)) stop("VeilLine must be a logical (TRUE or FALSE).")#
  # Check IncludeSpecimenLevelOTUs is a logical and stop and warn user if not:#
  if(!is.logical(IncludeSpecimenLevelOTUs)) stop("IncludeSpecimenLevelOTUs must be a logical (TRUE or FALSE).")#
  # If not a NULL read backbone constraint (checks it is a valid Newick format):#
  if(!is.null(BackboneConstraint)) BackboneConstraintTree <- ape::read.tree(text = BackboneConstraint)#
  # If not a NULL read monophyly constraint (checks it is a valid Newick format):#
  if(!is.null(MonophylyConstraint)) MonophylyConstraintTree <- ape::read.tree(text = MonophylyConstraint)#
  # List of types of resolution that require finding a senior synonym:#
  synonyms <- c("corrected to", "misspelling of", "objective synonym of", "obsolete variant of", "recombined as", "replaced by", "subjective synonym of")#
  # List of types of resolution that require changing reconciliation to DELETE:#
  deletes <- c("nomen dubium", "nomen vanum", "nomen nudum", "nomen oblitum", "invalid subgroup of")#
  # Print current processing status:#
  cat("Reading MRP data...")#
  # Set working directory as MRP directory:#
  setwd(MRPDirectory)#
  # List MRP files (or just use inclusivedatalist if set):#
  MRPFileList <- strsplit(ifelse(length(InclusiveDataList) > 0, paste(setdiff(sort(unique(InclusiveDataList)), sort(unique(ExclusiveDataList))), "mrp.nex", sep = "", collapse = "%%"), paste(setdiff(gsub("mrp\\.nex", "", list.files()), sort(unique(ExclusiveDataList))), "mrp.nex", sep = "", collapse = "%%")), "%%")[[1]]#
  # Read in all MRP files and store in a list (include duplicate headers to store parent sibling info later):#
  MRPList <- lapply(lapply(as.list(MRPFileList), Claddis::ReadMorphNexus), function(x) {y <- list(x$Matrix_1$Matrix, x$Matrix_1$Weights, "", "", ""); names(y) <- c("Matrix", "Weights", "FileName", "Parent", "Sibling"); y})#
  # Set names of MRP files:#
  names(MRPList) <- gsub("mrp.nex", "", MRPFileList)#
  # Print current processing status:#
  cat("Done\nReading XML data...")#
  # Set working directory as XML (i.e., metadata) directory:#
  setwd(XMLDirectory)#
  # List MRP files (or just use inslusivedatalist if set):#
  XMLFileList <- strsplit(ifelse(length(InclusiveDataList) > 0, paste(setdiff(sort(unique(InclusiveDataList)), sort(unique(ExclusiveDataList))), ".xml", sep = "", collapse = "%%"), paste(setdiff(gsub("\\.xml", "", list.files()), sort(unique(ExclusiveDataList))), ".xml", sep = "", collapse = "%%")), "%%")[[1]]#
  XMLFileList <- setdiff(XMLFileList, "Hartman_etal_2019a.xml")#
  # Check there are no MRPs not listed as XMLs and vice versa (should return empty vector):#
  MRPXMLunion <- c(setdiff(gsub("\\.xml", "", XMLFileList), gsub("mrp\\.nex", "", MRPFileList)), setdiff(gsub("mrp\\.nex", "", MRPFileList), gsub("\\.xml", "", XMLFileList)))#
  # Stop if MRP datasets not listed as XMLs and vice versa:#
  if(length(MRPXMLunion) > 0) stop(paste("Datasets do not match (MRP and XML)!:", MRPXMLunion, collapse = " "))#
  # Read in all XML files and store in a list:#
  XMLList <- lapply(as.list(XMLFileList), function(x) ReadMetatreeXML(x))#
  # Add names to XML list:#
  names(XMLList) <- gsub(".xml", "", XMLFileList)#
  # Collapse to just pertinent information:#
  XMLList <- lapply(XMLList, function(x) {y <- list(); y[["TaxonMatrix"]] <- x$SourceTree$Taxa$TagContents; y[["FileName"]] <- unname(unlist(x$SourceTree$Filename)); y[["Parent"]] <- unname(unlist(x$SourceTree$Parent)); y[["Sibling"]] <- unname(unlist(x$SourceTree$Sibling)); y})#
  # Find any files that contain duplicated taxon names:#
  FilesWithDuplicatedTaxonNames <- names(XMLList)[which(unlist(lapply(XMLList, function(x) any(duplicated(x$TaxonMatrix[, "ListValue"])))))]#
  # If duplicate names were found stop and warn user:#
  if(length(FilesWithDuplicatedTaxonNames) > 0) stop(paste("The following files contain duplicate taxon names: ", paste(FilesWithDuplicatedTaxonNames, collapse = ", "), ". Ensure all taxon names are unique and try again.", sep = ""))#
  # Find any taxon names that do not match between MRP and XML:#
  TaxonMismatches <- mapply(function(x, y) {MRPNames <- rownames(x$Matrix); XMLNames <- y$TaxonMatrix[, "ListValue"]; c(setdiff(MRPNames, XMLNames), setdiff(XMLNames, MRPNames))}, x = MRPList[names(MRPList)], y = XMLList[names(MRPList)])#
  # Find any files with mismatching taxon names between MRP and XML:#
  FilesWithTaxonMismatches <- names(TaxonMismatches)[which(unlist(lapply(TaxonMismatches, function(x) length(x))) > 0)]#
  # If such files are found then stop and warn user:#
  if(length(FilesWithTaxonMismatches) > 0) stop(paste("The following files contain mismatching taxon names between the MRP and XML versions: ", paste(FilesWithTaxonMismatches, collapse = ", "), ". Ensure all taxon names match and try again.", sep = ""))#
  # Compile any name issues:#
  NameIssues <- lapply(XMLList, function(x) {TaxonMatrix <- x$TaxonMatrix; SpacesFound <- c(grep(" ", TaxonMatrix[, "recon_name"]), grep(" ", TaxonMatrix[, "recon_no"]), grep(" ", TaxonMatrix[, "ListValue"])); EmptyValuesFound <- c(which(TaxonMatrix[, "recon_name"] == ""), which(TaxonMatrix[, "recon_no"] == ""), which(TaxonMatrix[, "ListValue"] == "")); RogueNumberCharacters <- setdiff(unique(unlist(strsplit(TaxonMatrix[, "recon_no"], ""))), c(0:9, ";", "-")); RogueNameCharacters <- setdiff(unique(c(unlist(strsplit(TaxonMatrix[, "recon_name"], "")), unlist(strsplit(TaxonMatrix[, "ListValue"], "")))), c(LETTERS, letters, 0:9, "_", ",")); y <- list(SpacesFound, EmptyValuesFound, RogueNumberCharacters, RogueNameCharacters); names(y) <- c("SpacesFound", "EmptyValuesFound", "RogueNumberCharacters", "RogueNameCharacters"); y})#
  # Find any files with spaces in taxon names:#
  FilesWithSpaces <- names(NameIssues)[unlist(lapply(NameIssues, function(x) length(x$SpacesFound))) > 0]#
  # Find any values with empty values for taxon names:#
  FilesWithEmptyValues <- names(NameIssues)[unlist(lapply(NameIssues, function(x) length(x$EmptyValuesFound))) > 0]#
  # Files with rogue values in the recon number field:#
  FilesWithRogueTaxonNumbers <- names(NameIssues)[unlist(lapply(NameIssues, function(x) length(x$RogueNumberCharacters))) > 0]#
  # Files with rogue values in the name fields:#
  FilesWithRogueTaxonNames <- names(NameIssues)[unlist(lapply(NameIssues, function(x) length(x$RogueNameCharacters))) > 0]#
  # If issues with spaces in names stop and warn user:#
  if(length(FilesWithSpaces) > 0) stop(paste("The following files contain spaces in the taxonomic reconciliation (names or numbers): ", paste(FilesWithSpaces, collapse = ", "), ". Remove spaces and try again.", sep = ""))#
  # If issues with empty names stop and warn user:#
  if(length(FilesWithEmptyValues) > 0) stop(paste("The following files contain empty values in the taxonomic reconciliation (names or numbers): ", paste(FilesWithEmptyValues, collapse = ", "), ". Ensure all values are filled and try again.", sep = ""))#
  # If issues with rogue characters in number field stop and warn user:#
  if(length(FilesWithRogueTaxonNumbers) > 0) stop(paste("The following files contain rogue values in the taxonomic reconciliation (numbers): ", paste(FilesWithRogueTaxonNumbers, collapse = ", "), ". Ensure all taxon numbers only include semicolon(s) (the separating character) or dashes (for negative values) and try again.", sep = ""))#
  # If issues with rogue characters in name field stop and warn user:#
  if(length(FilesWithRogueTaxonNames) > 0) stop(paste("The following files contain rogue values in the taxonomic reconciliation (names): ", paste(FilesWithRogueTaxonNames, collapse = ", "), ". Ensure all taxon names are formed from alphanumerics, commas (the separating character) or underscores and try again.", sep = ""))#
  # Reconcile OTU names with XML version:#
  MRPList <- mapply(function(x, y) {rownames(x$Matrix)[unlist(lapply(as.list(rownames(x$Matrix)), function(z) which(y$TaxonMatrix[, "ListValue"] == z)))] <- paste(y$TaxonMatrix[, "recon_no"], y$TaxonMatrix[, "recon_name"], sep = "%%%%"); x$FileName <- y$FileName; if(!is.null(y$Parent)) x$Parent <- y$Parent; if(!is.null(y$Sibling)) x$Sibling <- y$Sibling; x}, x = MRPList[names(MRPList)], y = XMLList[names(MRPList)], SIMPLIFY = FALSE)#
#
  # Print current processing status:#
  cat("Done\nChecking for unsampled parents and siblings...")#
  # Extract parent and sibling names:#
  ParentAndSiblingNames <- sort(unlist(lapply(as.list(unique(unname(unlist(lapply(MRPList, '[', c("Parent", "Sibling")))))), function(x) x[nchar(x) > 0])))#
  # Warn user about any unsampled parents and/or siblings:#
  if(length(setdiff(ParentAndSiblingNames, names(MRPList))) > 0) print(paste("The following parents and siblings are not in the sample (check they are correct or add them into the sample): ", paste(setdiff(ParentAndSiblingNames, names(MRPList)), collapse = ", "), sep = ""))#
  # Print current processing status:#
  cat("Done\nFinding initial multiple-taxon reconciliations...")#
  # Subfunction to make multi-taxon reconciliations unique OTUs:#
  SeparateMultiTaxonReconciliations <- function(ListBlock) {#
    # Find comma rows (multiple taxa in initial reconciliation):#
    commarows <- grep(",", rownames(ListBlock$Matrix))#
    # If there is at least one multiple-taxon reconciliation:#
    if(length(commarows) > 0) {#
      # For each multiple-taxon reconciliation in reverse order (to avoid later rows not matching):#
      for(j in rev(commarows)) {#
        # Get multiple names of reconciliation:#
        multiplenames <- strsplit(rownames(ListBlock$Matrix)[j], "%%%%")[[1]]#
        # Get multiple-taxon numbers:#
        multitaxonnumbers <- strsplit(multiplenames[1], ";")[[1]]#
        # Get multiple-taxon names:#
        multitaxonnames <- strsplit(multiplenames[2], ",")[[1]]#
        # Check data integrity with respect to multiple-taxon values:#
        if(length(multitaxonnumbers) != length(multitaxonnames)) stop(paste("Problem with multiple-taxon reconciliation(s) in ", ListBlock$FileName, " (check commas and semi-colons are correct; i.e., of same length).", sep = ""))#
        # Add new rows at base of matrix:#
        ListBlock$Matrix <- rbind(ListBlock$Matrix, matrix(rep(ListBlock$Matrix[j, ], length(multitaxonnumbers)), nrow = length(multitaxonnumbers), byrow = TRUE, dimnames = list(paste(multitaxonnumbers, multitaxonnames, sep = "%%%%"), c())))#
        # Remove now redundant row from matrix:#
        ListBlock$Matrix <- ListBlock$Matrix[-j, , drop = FALSE]#
      }#
    }#
    # Return updated list block:#
    return(ListBlock)#
  }#
  # Separate out multi-taxon reconcilations:#
  MRPList <- lapply(MRPList, SeparateMultiTaxonReconciliations)#
  # If excluding specimen-level OTUs:#
  if(!IncludeSpecimenLevelOTUs) {#
    # Print current processing status:#
    cat("Done\nRemoving specimen-level OTUs...")#
    # Convert specimen-level OTUs to taxa to DELETE:#
    MRPList <- lapply(MRPList, function(x) {RowNamesToDelete <- which(unlist(lapply(strsplit(rownames(x$Matrix), split = ""), function(y) sum(y == "_") > 2))); if(length(RowNamesToDelete) > 0) rownames(x$Matrix)[RowNamesToDelete] <- "0%%%%DELETE"; x})#
  }#
  # Print current processing status:#
  cat("Done\nRemoving taxa with initial reconciliations of \"DELETE\"...")#
  # Remove any taxa reconciled as DELETE:#
  MRPList <- lapply(MRPList, function(x) {DeleteRows <- which(unlist(lapply(strsplit(rownames(x$Matrix), split = "%%%%"), function(y) y[2])) == "DELETE"); if(length(DeleteRows) > 0) x$Matrix <- x$Matrix[-DeleteRows, , drop = FALSE]; x})#
  # Prune matrices following deletion:#
  MRPList[ActiveMRP(MRPList)] <- lapply(MRPList[ActiveMRP(MRPList)], function(x) {y <- PisaniMRPPrune(Claddis::MakeMorphMatrix(x$Matrix, weights = x$Weights, ignore.duplicate.taxa = TRUE)); x$Matrix <- y$Matrix_1$Matrix; x$Weights <- y$Matrix_1$Weights; x})#
#
  # Print current processing status:#
  cat("Done\nSearching for and collapsing pre-reconciliation duplicated taxa...")#
  # Collapse any duplicate taxon names:#
  MRPList[ActiveMRP(MRPList)] <- lapply(MRPList[ActiveMRP(MRPList)], function(x) {y <- Claddis::MakeMorphMatrix(x$Matrix, weights = x$Weights, ignore.duplicate.taxa = TRUE); if(any(duplicated(rownames(y$Matrix_1$Matrix)))) {DuplicateNames <- setdiff(unlist(lapply(strsplit(rownames(y$Matrix_1$Matrix)[duplicated(rownames(y$Matrix_1$Matrix))], split = "%%%%"), '[', 2)), "DELETE"); if(length(DuplicateNames) > 0) cat(paste("\nDuplicate resolved OTU name(s) found in ", x$FileName, ": ", paste(DuplicateNames, collapse = ", "), ". Check this is correct.", sep = "")); y <- CollapseDuplicateTaxonMRP(y)}; x$Matrix <- y$Matrix_1$Matrix; x$Weights <- y$Weights; x})#
  # Print current processing status:#
  cat("Done\nBuilding initial taxonomy matrix...")#
  # Create taxonomy matrix to store all taxon resolution data:#
  TaxonomyMatrix <- do.call(rbind, strsplit(unique(unname(unlist(lapply(MRPList[ActiveMRP(MRPList)], function(x) rownames(x$Matrix))))), split ="%%%%"))#
  # Add column names:#
  colnames(TaxonomyMatrix) <- c("TaxonNo", "TaxonName")#
  # Print current processing status:#
  cat("Done\nChecking for missing taxon numbers...")#
  # If any "-1" taxa found stop and tell user:#
  if(any(TaxonomyMatrix[, "TaxonNo"] == "-1")) stop(paste("The following taxa have the reconciliation number \"-1\": ", paste(TaxonomyMatrix[TaxonomyMatrix[, "TaxonNo"] == "-1", "TaxonName"], collapse = ", "), sep = ""))#
  # Print current processing status:#
  cat("Done\nChecking for combined taxon numbers (&)...")#
  # If any "&" taxa found stop and tell user:#
  if(length(grep("&", TaxonomyMatrix[, "TaxonNo"]))) stop(paste("The following taxa have multiple reconciliation numbers: ", paste(TaxonomyMatrix[grep("&", TaxonomyMatrix[, "TaxonNo"]), "TaxonName"], collapse = ", "), sep = ""))
ReadMetatreeXML("~/Documents/Homepage/www.graemetlloyd.com/xml/Cau_etal_inpressa.xml")
x <- ReadMetatreeXML("~/Documents/Homepage/www.graemetlloyd.com/xml/Cau_etal_inpressa.xml")
names(x)
names(x$SourceTree)
names(x$SourceTree$Taxa)
names(x$SourceTree$Taxa$TagContents)
x$SourceTree$Taxa$TagSupplement
x$SourceTree$Taxa$TagContents
x$SourceTree$Taxa$TagContents[, "ListValue"]
CauXMLNames <- x$SourceTree$Taxa$TagContents[, "ListValue"]
rownames(ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/mrp/Cau_etal_inpressamrp.nex")$Matrix_1$Matrix)
setdiff(CauXMLNames, rownames(ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/mrp/Cau_etal_inpressamrp.nex")$Matrix_1$Matrix))
setdiff(rownames(ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/mrp/Cau_etal_inpressamrp.nex")$Matrix_1$Matrix), CauXMLNames)
ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/nexus/Brocklehurst_etal_2013a.nex")
ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/mrp/Brocklehurst_etal_2013amrp.nex")
rownames(ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/mrp/Brocklehurst_etal_2013amrp.nex")$Matrix_1$Matrix)
x <- rownames(ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/mrp/Brocklehurst_etal_2013amrp.nex")$Matrix_1$Matrix)
x[duplicated(X)]
x[duplicated(x)]
ReadMetatreeXML("~/Documents/Homepage/www.graemetlloyd.com/xml/Brocklehurst_etal_2013a.xml")
x <- ReadMetatreeXML("~/Documents/Homepage/www.graemetlloyd.com/xml/Brocklehurst_etal_2013a.xml")
x$SourceTree$Taxa
x$SourceTree$Taxa$TagContents
x$SourceTree$Taxa$TagContents[, "ListValue"]
x <- x$SourceTree$Taxa$TagContents[, "ListValue"]
x[duplicated(x)]
MRPDirectory <- "/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp"#
  XMLDirectory <- "/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/xml"#
  TargetClade <- "Ichthyopterygia"#
  InclusiveDataList <- sort(c(GetFilesForClade("matricht.html"), "Bickelmann_etal_2009a", "Caldwell_1996a", "Chen_etal_2014ba", "Chen_etal_2014bb", "deBraga_et_Rieppel_1997a", "Gauthier_etal_1988b", "Laurin_et_Reisz_1995a", "Muller_2004a", "Reisz_etal_2011a", "Rieppel_et_Reisz_1999a", "Rieppel_et_deBraga_1996a", "Young_2003a"))#
  ExclusiveDataList <- c("Averianov_inpressa", "Bravo_et_Gaete_2015a", "Brocklehurst_etal_2013a", "Brocklehurst_etal_2015aa", "Brocklehurst_etal_2015ab", "Brocklehurst_etal_2015ac", "Brocklehurst_etal_2015ad", "Brocklehurst_etal_2015ae", "Brocklehurst_etal_2015af", "Bronzati_etal_2012a", "Bronzati_etal_2015ab", "Brusatte_etal_2009ba", "Campbell_etal_2016ab", "Carr_et_Williamson_2004a", "Carr_etal_2017ab", "Frederickson_et_Tumarkin-Deratzian_2014aa", "Frederickson_et_Tumarkin-Deratzian_2014ab", "Frederickson_et_Tumarkin-Deratzian_2014ac", "Frederickson_et_Tumarkin-Deratzian_2014ad", "Garcia_etal_2006a", "Gatesy_etal_2004ab", "Grellet-Tinner_2006a", "Grellet-Tinner_et_Chiappe_2004a", "Grellet-Tinner_et_Makovicky_2006a", "Knoll_2008a", "Kurochkin_1996a", "Lopez-Martinez_et_Vicens_2012a", "Lu_etal_2014aa", "Norden_etal_inpressa", "Pisani_etal_2002a", "Ruiz-Omenaca_etal_1997a", "Ruta_etal_2003ba", "Ruta_etal_2003bb", "Ruta_etal_2007a", "Selles_et_Galobart_2016a", "Sereno_1993a", "Sidor_2001a", "Skutschas_etal_in
pressa", "Tanaka_etal_2011a", "Toljagic_et_Butler_2013a", "Tsuihiji_etal_2011aa", "Varricchio_et_Jackson_2004a", "Vila_etal_2017a", "Wilson_2005aa", "Wilson_2005ab", "Zelenitsky_et_Therrien_2008a")#
  HigherTaxaToCollapse = c()#
  MissingSpecies = "exclude"#
  Interval = NULL#
  VeilLine = TRUE#
  SpeciesToExclude = c()#
  IncludeSpecimenLevelOTUs = TRUE#
  BackboneConstraint = NULL#
  MonophylyConstraint = NULL#
  InclusiveDataList = c()#
  ExclusiveDataList = c()#
  # New Options (requires code to actually use them)#
  ##
  # HigherTaxaToCollapse Vector can be empty.#
  # VeilLine TRUE/FALSE (will be in output)#
  # SpeciesToExclude Vector of any species to be excluded from the final metatree. E.g., Eshanosaurus, Ricardoestesia.#
  # BackboneConstraint Newick string of backbone constraint (allows taxa not in topology). NULL as default.#
  # MonophylyConstraint Newick string of monophyly constraint (excludes taxa not in topology). NULL as default.#
  # CHECK PARENT IS A DATA SET AND NOT A REFERENCE, E.G., IF ENTER A REFERENCE AS PARENT THEN PARENT TURNS OUT TO HAVE TWO DATA SETS#
  # CHECK FOR SPECIES THAT BELONG TO A GENUS DIFFERENT TO THE ONE IN THEIR NAME!#
  # NEED TO CATCH ISSUE WHERE GENUS NUMBER IS USED FOR A SPECIES (HARD TO CHECK SO FAR DUE TO INDETERMINATES CONTINGENCY)#
  # NEED SOME TEST THAT HELPS CHECK ROOT IS SENSIBLE#
  # NEED SOME TEST THAT HELPS DETERMINE IF MULTIPLE OCCURRENCES OF SAME TAXON AFTER RECONCILIATION IS CORRECT OR AN ERROR#
  # ADD MORE COMPLEX WEIGHTS BY USING ADDITIONAL CHARACTER STATES! (EACH DATASET TOTAL WEIGHT DETERMINED BY YEAR AND DEPENDENCE THEN SUBDIVIDED ACROSS CHARACTER?) - BUT THIS SEEMS TO SLOW THINGS DRAMATICALLY MAYBE DO BY DUPLICATING CHARACTERS INSTEAD#
  # MAKE STR OPTIONAL (SAVES A LITTLE TIME)#
  # CHECK THERE ARE MULTIPLE TAXA PRE-RECONCILIATION#
  # CHECK INDETS DO NOT GIVE MULTIPLE MATCHES#
  # HOW TO DELETE DATA SETS THAT STILL CONTRIBUTE TO DEPENDENCE?#
  # Subfunction that gives just MRPs where matrix is still intact (has rows and columns):#
  ActiveMRP <- function(MRPList) unname(which(unlist(lapply(MRPList, function(x) prod(dim(x$Matrix)))) > 0))#
#
  # Check MRPDirectory is formatted correctly adn stop and warn user if not:#
  if(!all(is.character(MRPDirectory)) || length(MRPDirectory) != 1) stop("MRPDirectory must be a single character string indicating the path to the folder containing the MRP files.")#
  # Check XMLDirectory is formatted correctly adn stop and warn user if not:#
  if(!all(is.character(XMLDirectory)) || length(XMLDirectory) != 1) stop("XMLDirectory must be a single character string indicating the path to the folder containing the XML files.")#
  # Check TargetClade is formatted correctly adn stop and warn user if not:#
  if(!all(is.character(TargetClade)) || length(TargetClade) != 1) stop("TargetClade must be a single character string indicating the desired clade the metatree will represent.")#
  # Check MissingSpecies respresents a valid option:#
  if(length(setdiff(MissingSpecies, c("all", "exclude", "genus"))) > 0) stop("MissingSpecies must be one of \"all\", \"exclude\", or \"genus\".")#
  # Check VeilLine is a logical and stop and warn user if not:#
  if(!is.logical(VeilLine)) stop("VeilLine must be a logical (TRUE or FALSE).")#
  # Check IncludeSpecimenLevelOTUs is a logical and stop and warn user if not:#
  if(!is.logical(IncludeSpecimenLevelOTUs)) stop("IncludeSpecimenLevelOTUs must be a logical (TRUE or FALSE).")#
  # If not a NULL read backbone constraint (checks it is a valid Newick format):#
  if(!is.null(BackboneConstraint)) BackboneConstraintTree <- ape::read.tree(text = BackboneConstraint)#
  # If not a NULL read monophyly constraint (checks it is a valid Newick format):#
  if(!is.null(MonophylyConstraint)) MonophylyConstraintTree <- ape::read.tree(text = MonophylyConstraint)#
  # List of types of resolution that require finding a senior synonym:#
  synonyms <- c("corrected to", "misspelling of", "objective synonym of", "obsolete variant of", "recombined as", "replaced by", "subjective synonym of")#
  # List of types of resolution that require changing reconciliation to DELETE:#
  deletes <- c("nomen dubium", "nomen vanum", "nomen nudum", "nomen oblitum", "invalid subgroup of")#
  # Print current processing status:#
  cat("Reading MRP data...")#
  # Set working directory as MRP directory:#
  setwd(MRPDirectory)#
  # List MRP files (or just use inclusivedatalist if set):#
  MRPFileList <- strsplit(ifelse(length(InclusiveDataList) > 0, paste(setdiff(sort(unique(InclusiveDataList)), sort(unique(ExclusiveDataList))), "mrp.nex", sep = "", collapse = "%%"), paste(setdiff(gsub("mrp\\.nex", "", list.files()), sort(unique(ExclusiveDataList))), "mrp.nex", sep = "", collapse = "%%")), "%%")[[1]]#
  # Read in all MRP files and store in a list (include duplicate headers to store parent sibling info later):#
  MRPList <- lapply(lapply(as.list(MRPFileList), Claddis::ReadMorphNexus), function(x) {y <- list(x$Matrix_1$Matrix, x$Matrix_1$Weights, "", "", ""); names(y) <- c("Matrix", "Weights", "FileName", "Parent", "Sibling"); y})#
  # Set names of MRP files:#
  names(MRPList) <- gsub("mrp.nex", "", MRPFileList)#
  # Print current processing status:#
  cat("Done\nReading XML data...")#
  # Set working directory as XML (i.e., metadata) directory:#
  setwd(XMLDirectory)#
  # List MRP files (or just use inslusivedatalist if set):#
  XMLFileList <- strsplit(ifelse(length(InclusiveDataList) > 0, paste(setdiff(sort(unique(InclusiveDataList)), sort(unique(ExclusiveDataList))), ".xml", sep = "", collapse = "%%"), paste(setdiff(gsub("\\.xml", "", list.files()), sort(unique(ExclusiveDataList))), ".xml", sep = "", collapse = "%%")), "%%")[[1]]#
  XMLFileList <- setdiff(XMLFileList, "Hartman_etal_2019a.xml")#
  # Check there are no MRPs not listed as XMLs and vice versa (should return empty vector):#
  MRPXMLunion <- c(setdiff(gsub("\\.xml", "", XMLFileList), gsub("mrp\\.nex", "", MRPFileList)), setdiff(gsub("mrp\\.nex", "", MRPFileList), gsub("\\.xml", "", XMLFileList)))#
  # Stop if MRP datasets not listed as XMLs and vice versa:#
  if(length(MRPXMLunion) > 0) stop(paste("Datasets do not match (MRP and XML)!:", MRPXMLunion, collapse = " "))#
  # Read in all XML files and store in a list:#
  XMLList <- lapply(as.list(XMLFileList), function(x) ReadMetatreeXML(x))#
  # Add names to XML list:#
  names(XMLList) <- gsub(".xml", "", XMLFileList)#
  # Collapse to just pertinent information:#
  XMLList <- lapply(XMLList, function(x) {y <- list(); y[["TaxonMatrix"]] <- x$SourceTree$Taxa$TagContents; y[["FileName"]] <- unname(unlist(x$SourceTree$Filename)); y[["Parent"]] <- unname(unlist(x$SourceTree$Parent)); y[["Sibling"]] <- unname(unlist(x$SourceTree$Sibling)); y})#
  # Find any files that contain duplicated taxon names:#
  FilesWithDuplicatedTaxonNames <- names(XMLList)[which(unlist(lapply(XMLList, function(x) any(duplicated(x$TaxonMatrix[, "ListValue"])))))]#
  # If duplicate names were found stop and warn user:#
  if(length(FilesWithDuplicatedTaxonNames) > 0) stop(paste("The following files contain duplicate taxon names: ", paste(FilesWithDuplicatedTaxonNames, collapse = ", "), ". Ensure all taxon names are unique and try again.", sep = ""))#
  # Find any taxon names that do not match between MRP and XML:#
  TaxonMismatches <- mapply(function(x, y) {MRPNames <- rownames(x$Matrix); XMLNames <- y$TaxonMatrix[, "ListValue"]; c(setdiff(MRPNames, XMLNames), setdiff(XMLNames, MRPNames))}, x = MRPList[names(MRPList)], y = XMLList[names(MRPList)])#
  # Find any files with mismatching taxon names between MRP and XML:#
  FilesWithTaxonMismatches <- names(TaxonMismatches)[which(unlist(lapply(TaxonMismatches, function(x) length(x))) > 0)]#
  # If such files are found then stop and warn user:#
  if(length(FilesWithTaxonMismatches) > 0) stop(paste("The following files contain mismatching taxon names between the MRP and XML versions: ", paste(FilesWithTaxonMismatches, collapse = ", "), ". Ensure all taxon names match and try again.", sep = ""))#
  # Compile any name issues:#
  NameIssues <- lapply(XMLList, function(x) {TaxonMatrix <- x$TaxonMatrix; SpacesFound <- c(grep(" ", TaxonMatrix[, "recon_name"]), grep(" ", TaxonMatrix[, "recon_no"]), grep(" ", TaxonMatrix[, "ListValue"])); EmptyValuesFound <- c(which(TaxonMatrix[, "recon_name"] == ""), which(TaxonMatrix[, "recon_no"] == ""), which(TaxonMatrix[, "ListValue"] == "")); RogueNumberCharacters <- setdiff(unique(unlist(strsplit(TaxonMatrix[, "recon_no"], ""))), c(0:9, ";", "-")); RogueNameCharacters <- setdiff(unique(c(unlist(strsplit(TaxonMatrix[, "recon_name"], "")), unlist(strsplit(TaxonMatrix[, "ListValue"], "")))), c(LETTERS, letters, 0:9, "_", ",")); y <- list(SpacesFound, EmptyValuesFound, RogueNumberCharacters, RogueNameCharacters); names(y) <- c("SpacesFound", "EmptyValuesFound", "RogueNumberCharacters", "RogueNameCharacters"); y})#
  # Find any files with spaces in taxon names:#
  FilesWithSpaces <- names(NameIssues)[unlist(lapply(NameIssues, function(x) length(x$SpacesFound))) > 0]#
  # Find any values with empty values for taxon names:#
  FilesWithEmptyValues <- names(NameIssues)[unlist(lapply(NameIssues, function(x) length(x$EmptyValuesFound))) > 0]#
  # Files with rogue values in the recon number field:#
  FilesWithRogueTaxonNumbers <- names(NameIssues)[unlist(lapply(NameIssues, function(x) length(x$RogueNumberCharacters))) > 0]#
  # Files with rogue values in the name fields:#
  FilesWithRogueTaxonNames <- names(NameIssues)[unlist(lapply(NameIssues, function(x) length(x$RogueNameCharacters))) > 0]#
  # If issues with spaces in names stop and warn user:#
  if(length(FilesWithSpaces) > 0) stop(paste("The following files contain spaces in the taxonomic reconciliation (names or numbers): ", paste(FilesWithSpaces, collapse = ", "), ". Remove spaces and try again.", sep = ""))#
  # If issues with empty names stop and warn user:#
  if(length(FilesWithEmptyValues) > 0) stop(paste("The following files contain empty values in the taxonomic reconciliation (names or numbers): ", paste(FilesWithEmptyValues, collapse = ", "), ". Ensure all values are filled and try again.", sep = ""))#
  # If issues with rogue characters in number field stop and warn user:#
  if(length(FilesWithRogueTaxonNumbers) > 0) stop(paste("The following files contain rogue values in the taxonomic reconciliation (numbers): ", paste(FilesWithRogueTaxonNumbers, collapse = ", "), ". Ensure all taxon numbers only include semicolon(s) (the separating character) or dashes (for negative values) and try again.", sep = ""))#
  # If issues with rogue characters in name field stop and warn user:#
  if(length(FilesWithRogueTaxonNames) > 0) stop(paste("The following files contain rogue values in the taxonomic reconciliation (names): ", paste(FilesWithRogueTaxonNames, collapse = ", "), ". Ensure all taxon names are formed from alphanumerics, commas (the separating character) or underscores and try again.", sep = ""))#
  # Reconcile OTU names with XML version:#
  MRPList <- mapply(function(x, y) {rownames(x$Matrix)[unlist(lapply(as.list(rownames(x$Matrix)), function(z) which(y$TaxonMatrix[, "ListValue"] == z)))] <- paste(y$TaxonMatrix[, "recon_no"], y$TaxonMatrix[, "recon_name"], sep = "%%%%"); x$FileName <- y$FileName; if(!is.null(y$Parent)) x$Parent <- y$Parent; if(!is.null(y$Sibling)) x$Sibling <- y$Sibling; x}, x = MRPList[names(MRPList)], y = XMLList[names(MRPList)], SIMPLIFY = FALSE)#
#
  # Print current processing status:#
  cat("Done\nChecking for unsampled parents and siblings...")#
  # Extract parent and sibling names:#
  ParentAndSiblingNames <- sort(unlist(lapply(as.list(unique(unname(unlist(lapply(MRPList, '[', c("Parent", "Sibling")))))), function(x) x[nchar(x) > 0])))#
  # Warn user about any unsampled parents and/or siblings:#
  if(length(setdiff(ParentAndSiblingNames, names(MRPList))) > 0) print(paste("The following parents and siblings are not in the sample (check they are correct or add them into the sample): ", paste(setdiff(ParentAndSiblingNames, names(MRPList)), collapse = ", "), sep = ""))#
  # Print current processing status:#
  cat("Done\nFinding initial multiple-taxon reconciliations...")#
  # Subfunction to make multi-taxon reconciliations unique OTUs:#
  SeparateMultiTaxonReconciliations <- function(ListBlock) {#
    # Find comma rows (multiple taxa in initial reconciliation):#
    commarows <- grep(",", rownames(ListBlock$Matrix))#
    # If there is at least one multiple-taxon reconciliation:#
    if(length(commarows) > 0) {#
      # For each multiple-taxon reconciliation in reverse order (to avoid later rows not matching):#
      for(j in rev(commarows)) {#
        # Get multiple names of reconciliation:#
        multiplenames <- strsplit(rownames(ListBlock$Matrix)[j], "%%%%")[[1]]#
        # Get multiple-taxon numbers:#
        multitaxonnumbers <- strsplit(multiplenames[1], ";")[[1]]#
        # Get multiple-taxon names:#
        multitaxonnames <- strsplit(multiplenames[2], ",")[[1]]#
        # Check data integrity with respect to multiple-taxon values:#
        if(length(multitaxonnumbers) != length(multitaxonnames)) stop(paste("Problem with multiple-taxon reconciliation(s) in ", ListBlock$FileName, " (check commas and semi-colons are correct; i.e., of same length).", sep = ""))#
        # Add new rows at base of matrix:#
        ListBlock$Matrix <- rbind(ListBlock$Matrix, matrix(rep(ListBlock$Matrix[j, ], length(multitaxonnumbers)), nrow = length(multitaxonnumbers), byrow = TRUE, dimnames = list(paste(multitaxonnumbers, multitaxonnames, sep = "%%%%"), c())))#
        # Remove now redundant row from matrix:#
        ListBlock$Matrix <- ListBlock$Matrix[-j, , drop = FALSE]#
      }#
    }#
    # Return updated list block:#
    return(ListBlock)#
  }#
  # Separate out multi-taxon reconcilations:#
  MRPList <- lapply(MRPList, SeparateMultiTaxonReconciliations)#
  # If excluding specimen-level OTUs:#
  if(!IncludeSpecimenLevelOTUs) {#
    # Print current processing status:#
    cat("Done\nRemoving specimen-level OTUs...")#
    # Convert specimen-level OTUs to taxa to DELETE:#
    MRPList <- lapply(MRPList, function(x) {RowNamesToDelete <- which(unlist(lapply(strsplit(rownames(x$Matrix), split = ""), function(y) sum(y == "_") > 2))); if(length(RowNamesToDelete) > 0) rownames(x$Matrix)[RowNamesToDelete] <- "0%%%%DELETE"; x})#
  }#
  # Print current processing status:#
  cat("Done\nRemoving taxa with initial reconciliations of \"DELETE\"...")#
  # Remove any taxa reconciled as DELETE:#
  MRPList <- lapply(MRPList, function(x) {DeleteRows <- which(unlist(lapply(strsplit(rownames(x$Matrix), split = "%%%%"), function(y) y[2])) == "DELETE"); if(length(DeleteRows) > 0) x$Matrix <- x$Matrix[-DeleteRows, , drop = FALSE]; x})#
  # Prune matrices following deletion:#
  MRPList[ActiveMRP(MRPList)] <- lapply(MRPList[ActiveMRP(MRPList)], function(x) {y <- PisaniMRPPrune(Claddis::MakeMorphMatrix(x$Matrix, weights = x$Weights, ignore.duplicate.taxa = TRUE)); x$Matrix <- y$Matrix_1$Matrix; x$Weights <- y$Matrix_1$Weights; x})#
#
  # Print current processing status:#
  cat("Done\nSearching for and collapsing pre-reconciliation duplicated taxa...")#
  # Collapse any duplicate taxon names:#
  MRPList[ActiveMRP(MRPList)] <- lapply(MRPList[ActiveMRP(MRPList)], function(x) {y <- Claddis::MakeMorphMatrix(x$Matrix, weights = x$Weights, ignore.duplicate.taxa = TRUE); if(any(duplicated(rownames(y$Matrix_1$Matrix)))) {DuplicateNames <- setdiff(unlist(lapply(strsplit(rownames(y$Matrix_1$Matrix)[duplicated(rownames(y$Matrix_1$Matrix))], split = "%%%%"), '[', 2)), "DELETE"); if(length(DuplicateNames) > 0) cat(paste("\nDuplicate resolved OTU name(s) found in ", x$FileName, ": ", paste(DuplicateNames, collapse = ", "), ". Check this is correct.", sep = "")); y <- CollapseDuplicateTaxonMRP(y)}; x$Matrix <- y$Matrix_1$Matrix; x$Weights <- y$Weights; x})#
  # Print current processing status:#
  cat("Done\nBuilding initial taxonomy matrix...")#
  # Create taxonomy matrix to store all taxon resolution data:#
  TaxonomyMatrix <- do.call(rbind, strsplit(unique(unname(unlist(lapply(MRPList[ActiveMRP(MRPList)], function(x) rownames(x$Matrix))))), split ="%%%%"))#
  # Add column names:#
  colnames(TaxonomyMatrix) <- c("TaxonNo", "TaxonName")#
  # Print current processing status:#
  cat("Done\nChecking for missing taxon numbers...")#
  # If any "-1" taxa found stop and tell user:#
  if(any(TaxonomyMatrix[, "TaxonNo"] == "-1")) stop(paste("The following taxa have the reconciliation number \"-1\": ", paste(TaxonomyMatrix[TaxonomyMatrix[, "TaxonNo"] == "-1", "TaxonName"], collapse = ", "), sep = ""))#
  # Print current processing status:#
  cat("Done\nChecking for combined taxon numbers (&)...")#
  # If any "&" taxa found stop and tell user:#
  if(length(grep("&", TaxonomyMatrix[, "TaxonNo"]))) stop(paste("The following taxa have multiple reconciliation numbers: ", paste(TaxonomyMatrix[grep("&", TaxonomyMatrix[, "TaxonNo"]), "TaxonName"], collapse = ", "), sep = ""))
TaxonMismatches
TaxonMismatches[["Cau_etal_inpressa"]]
# Load Claddis library:#
library(Claddis)#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get file list:#
file.list <- list.files()#
#
# Get just the group matrix pages:#
file.list <- file.list[grep("matr[a-z]{4}.html", file.list)]#
#
# Vector for storing output:#
results <- vector(mode = "character")#
#
# Main loop:#
for(i in 1:length(file.list)) {#
  # Read in ith file:#
  X <- scan(file.list[i], what = "", sep = "\n", quiet = TRUE)#
  # Find first p tag opening:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # FInd last p tag closing:#
  ends <- grep("</p>", X)#
  # Reduce X to just the portion with references:#
  X <- X[begins[1]:ends[length(ends)]]#
  # Find where p tags open:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # Find where p tags close:#
  ends <- grep("</p>", X)#
  # Check p tags are closed and warn if not:#
  if(length(begins) != length(ends)) print(paste("Error in", file.list[i]))#
  # For each set of p tags:#
  for(j in 1:length(ends)) {#
    # Get full reference block:#
    Y <- X[begins[j]:ends[j]]#
    # Only proceed if this has not already been dealt with:#
    if(length(grep("<a href", Y)) == 0) {#
      # Remove bookmarks:#
      Y <- gsub("</p>", "", gsub("<p class=\"hangingindent\">", "", Y))#
      # Strip out leading whitespace:#
      while(length(grep("\t", Y)) > 0) Y <- gsub("\t", " ", Y)#
      # Strip out leading whitespace:#
      while(length(grep("  ", Y)) > 0) Y <- gsub("  ", " ", Y)#
      # Strip out last leading whitespace"#
      for(k in 1:length(Y)) Y[k] <- paste(strsplit(Y[k], "")[[1]][2:length(strsplit(Y[k], "")[[1]])], collapse = "")#
      # Isolate author and year:#
      authorandyear <- strsplit(gsub(" and ", "%%", gsub("\\., ", ".%%", Y[1])), "%%")[[1]]#
      # Isolate title:#
      title <- Y[2]#
      ##
      locale <- gsub("</b>", "", gsub("<b>", "", gsub("</em>", "", gsub("<em>", "", strsplit(gsub("\\.", "", gsub(", ", "%%", Y[3])), "%%")[[1]]))))#
      ##
      authorline <- paste("\t\t<Author>\n", paste("\t\t\t<List>", authorandyear[1:(length(authorandyear) - 1)], "</List>", sep = "", collapse = "\n"), "\n\t\t</Author>\n", sep = "")#
      ##
      yearline <- paste("\t\t<Year>", gsub("\\.", "", authorandyear[length(authorandyear)]), "</Year>\n", sep = "")#
      ##
      year <- gsub("</Year>\n", "", gsub("\t\t<Year>", "", yearline))#
      ##
      titleline <- strsplit(title, "")[[1]]#
      ##
      if(titleline[length(titleline)] == ".") titleline <- titleline[-length(titleline)]#
      ##
      titleline <- paste(titleline, collapse = "")#
      ##
      titleline <- paste("\t\t<Title>", titleline, "</Title>\n", sep = "")#
      # Case if a book chapter:#
      if(length(grep("In ", locale[1])) == 1) {#
        # Restore locale to original line:#
        locale <- Y[3]#
        ##
        locale <- gsub("<em>In</em> ", "", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\) ", "%%", locale)#
        # Isolate editors#
        editors <- strsplit(locale, "%%")[[1]][1]#
        # Add "and" separator:#
        editors <- gsub(" and ", "%%", editors)#
        ##
        if(length(grep(",", editors)) > 0) {#
          # Case if single editor in correct "Surname, Initials" format:#
          if(length(grep("%%", editors)) == 0) editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
          # Case if authors are in incorrect "Intitals Surname" format:#
          if(strsplit(editors, "")[[1]][2] == ".") {#
            # Add separator between names:#
            editors <- gsub(", ", "%%", editors)#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
            ##
          } else {#
            # Add separator between names:#
            editors <- gsub("\\., ", ".%%", editors)#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", strsplit(editors, "%%")[[1]], "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
          ##
        } else {#
          # Case if single editor in incorrect "Intitals Surname" format:#
          if(length(grep("%%",editors)) == 0) {#
            ##
            editors <- strsplit(editors, "\\. ")[[1]]#
            ##
            editors <- paste(paste(editors[length(editors)], ",", sep = ""), paste(editors[1:(length(editors) - 1)], ".", sep = "", collapse = " "), collapse = " ")#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
            # Case of two authors in incorrect "Intitals Surname" format:#
          } else {#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
        }#
        # Remove editors from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Find end of book title separator:#
        locale <- gsub("\\. ", "%%", locale)#
        # Remove trailing period:#
        locale <- gsub("\\.", "", locale)#
        # Isolate booktitle:#
        booktitleline <- paste("\t\t<Booktitle>", strsplit(locale, "%%")[[1]][1], "</Booktitle>\n", sep = "")#
        # Remove booktitle from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Remove false gaps:#
        while(length(locale) > 1) locale <- paste(locale, collapse = ". ")#
        # Separate remaining portions:#
        locale <- strsplit(locale, ", ")[[1]]#
        ##
        publisherline <- paste("\t\t<Publisher>", locale[1], "</Publisher>\n", sep = "")#
        ##
        cityline <- paste("\t\t<City>", locale[2], "</City>\n", sep = "")#
        ##
        pagesline <- paste("\t\t<Pages>", gsub("<br>", "", gsub("p", "", locale[3])), "</Pages>\n", sep = "")#
        ##
        fulllines <- paste(authorline, yearline, titleline, "\t\t<Journal/>\n", "\t\t<Volume/>\n", pagesline, booktitleline, publisherline, cityline, editorsline, sep = "")#
        # Case if a journal:#
      } else {#
        ##
        if(year == "in press") {#
          # Case if journal title with commas:#
          if(length(locale) > 2) {#
            # Collapse journal title:#
            locale[1] <- paste(locale[1], locale[2], sep = ", ")#
            # Remove redudnant second part#
            locale <- locale[-2]#
          }#
          # Delete empty volume value#
          if(locale[2] == "") locale <- locale[-2]#
        }#
        # Find journal titles with commas:#
        while(length(locale) > 3) {#
          # Collapse journal title:#
          locale[1] <- paste(locale[1], locale[2], sep = ", ")#
          # Remove redudnant second part:#
          locale <- locale[-2]#
        }#
        ##
        journalline <- paste("\t\t<Journal>", locale[1], "</Journal>\n", sep = "")#
        ##
        if(length(locale) > 1) {#
          ##
          volumeline <- paste("\t\t<Volume>", locale[2], "</Volume>\n", sep = "")#
          ##
        } else {#
          ##
          volumeline <- "\t\t<Volume/>\n"#
        }#
        ##
        if(length(locale) > 2) {#
          ##
          pagesline <- paste("\t\t<Pages>", locale[3], "</Pages>\n", sep = "")#
          ##
        } else {#
          ##
          pagesline <- "\t\t<Pages/>\n"#
        }#
        ##
        fulllines <- paste(authorline, yearline, titleline, journalline, volumeline, pagesline, "\t\t<Booktitle/>\n", "\t\t<Publisher/>\n", "\t\t<City/>\n","\t\t<Editor/>\n", sep = "")#
      }#
    }#
    ##
    results <- c(results, fulllines)#
  }#
}#
#
# Collapse to just unique references (not sure how duplicates ended up in here...):#
results <- sort(unique(results))#
#
# Create empty vector to store hypothetical file names:#
filenames <- vector(mode = "character")#
#
# For each reference:#
for(i in 1:length(results)) {#
  # Isolate authors:#
  authors <- strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]][which(nchar(strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]]) > 0)]#
  # Isolate surnames:#
  surnames <- unlist(lapply(strsplit(authors, split = ","), '[', 1))#
  # Get publication year:#
  year <- gsub(" ", "", strsplit(gsub("\n|\t", "", results[i]), split = "<Year>|</Year>")[[1]][2])#
  # If a single author:#
  if(length(surnames) == 1) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames, year, sep = "_"))))#
  # If two authors:#
  if(length(surnames) == 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(paste(surnames, collapse = "_et_"), year, sep = "_"))))#
  # If more than two authors:#
  if(length(surnames) > 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames[1], "etal", year, sep = "_"))))#
}#
#
# Isolate references that have multiple file names (i.e., two or more refrences could be contracted to the same name):#
duplicates <- unique(filenames[duplicated(filenames)])#
#
# Set working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Get list of folders:#
folder.list <- list.files()[-grep("\\.", list.files())]#
#
# Get full paths for each folder:#
for(i in 1:length(folder.list)) folder.list[i] <- paste(getwd(), "/", folder.list[i], sep = "")#
#
# Vector for storing nexus file list:#
file.list <- vector(mode = "character")#
#
# Find all file paths for nexus files:#
for(i in 1:length(folder.list)) {#
  # Set working directory for current folder:#
  setwd(folder.list[i])#
  # Look for NEXUS files:#
  if(length(grep(".nex", list.files())) > 0) {#
    # Add any found to file list:#
    file.list <- c(file.list, paste(folder.list[i], "/", list.files()[grep(".nex", list.files())], sep = ""))#
  }#
}#
#
# Get just the NEXUS file names:#
nexus.files <- unlist(lapply(strsplit(file.list, "/"), '[', 9))#
#
# Reset working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Create vector to store multiple hits:#
multi_hitters <- vector(mode = "character")#
#
# Set scratch counter:#
scratch_counter <- 1#
#
# Create nexus, tnt and xml files:#
for(i in 1:length(file.list)) {#
  # Start feedback:#
  cat("Attempting to read: ", file.list[i], "...")#
  # Get stripped verion of name (i.e., missing a, b, aa etc. ending):#
  stripped_name <- gsub(strsplit(nexus.files[i], "[:0-9:]{4}|inpress")[[1]][2], "", nexus.files[i])#
  # Get hits for stripped name in filenames:#
  hits <- grep(stripped_name, filenames)#
  # Check there is a match:#
  if(length(hits) == 0) stop("No reference with matching name.")#
  # Create reference info:#
  reference_info <- paste(results[hits], collapse = "\n\nOR\n\n")#
  # If multiple hits add to list so these can be manually checked later:#
  if(length(hits) > 1) multi_hitters <- c(multi_hitters, nexus.files[i])#
  # Read in matrix:#
  mymatrix <- ReadMorphNexus(file.list[i])}
# Load Claddis library:#
library(Claddis)#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get file list:#
file.list <- list.files()#
#
# Get just the group matrix pages:#
file.list <- file.list[grep("matr[a-z]{4}.html", file.list)]#
#
# Vector for storing output:#
results <- vector(mode = "character")#
#
# Main loop:#
for(i in 1:length(file.list)) {#
  # Read in ith file:#
  X <- scan(file.list[i], what = "", sep = "\n", quiet = TRUE)#
  # Find first p tag opening:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # FInd last p tag closing:#
  ends <- grep("</p>", X)#
  # Reduce X to just the portion with references:#
  X <- X[begins[1]:ends[length(ends)]]#
  # Find where p tags open:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # Find where p tags close:#
  ends <- grep("</p>", X)#
  # Check p tags are closed and warn if not:#
  if(length(begins) != length(ends)) print(paste("Error in", file.list[i]))#
  # For each set of p tags:#
  for(j in 1:length(ends)) {#
    # Get full reference block:#
    Y <- X[begins[j]:ends[j]]#
    # Only proceed if this has not already been dealt with:#
    if(length(grep("<a href", Y)) == 0) {#
      # Remove bookmarks:#
      Y <- gsub("</p>", "", gsub("<p class=\"hangingindent\">", "", Y))#
      # Strip out leading whitespace:#
      while(length(grep("\t", Y)) > 0) Y <- gsub("\t", " ", Y)#
      # Strip out leading whitespace:#
      while(length(grep("  ", Y)) > 0) Y <- gsub("  ", " ", Y)#
      # Strip out last leading whitespace"#
      for(k in 1:length(Y)) Y[k] <- paste(strsplit(Y[k], "")[[1]][2:length(strsplit(Y[k], "")[[1]])], collapse = "")#
      # Isolate author and year:#
      authorandyear <- strsplit(gsub(" and ", "%%", gsub("\\., ", ".%%", Y[1])), "%%")[[1]]#
      # Isolate title:#
      title <- Y[2]#
      ##
      locale <- gsub("</b>", "", gsub("<b>", "", gsub("</em>", "", gsub("<em>", "", strsplit(gsub("\\.", "", gsub(", ", "%%", Y[3])), "%%")[[1]]))))#
      ##
      authorline <- paste("\t\t<Author>\n", paste("\t\t\t<List>", authorandyear[1:(length(authorandyear) - 1)], "</List>", sep = "", collapse = "\n"), "\n\t\t</Author>\n", sep = "")#
      ##
      yearline <- paste("\t\t<Year>", gsub("\\.", "", authorandyear[length(authorandyear)]), "</Year>\n", sep = "")#
      ##
      year <- gsub("</Year>\n", "", gsub("\t\t<Year>", "", yearline))#
      ##
      titleline <- strsplit(title, "")[[1]]#
      ##
      if(titleline[length(titleline)] == ".") titleline <- titleline[-length(titleline)]#
      ##
      titleline <- paste(titleline, collapse = "")#
      ##
      titleline <- paste("\t\t<Title>", titleline, "</Title>\n", sep = "")#
      # Case if a book chapter:#
      if(length(grep("In ", locale[1])) == 1) {#
        # Restore locale to original line:#
        locale <- Y[3]#
        ##
        locale <- gsub("<em>In</em> ", "", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\.\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(eds\\) ", "%%", locale)#
        # Insert first (editor(s)) separator:#
        locale <- gsub(" \\(ed\\) ", "%%", locale)#
        # Isolate editors#
        editors <- strsplit(locale, "%%")[[1]][1]#
        # Add "and" separator:#
        editors <- gsub(" and ", "%%", editors)#
        ##
        if(length(grep(",", editors)) > 0) {#
          # Case if single editor in correct "Surname, Initials" format:#
          if(length(grep("%%", editors)) == 0) editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
          # Case if authors are in incorrect "Intitals Surname" format:#
          if(strsplit(editors, "")[[1]][2] == ".") {#
            # Add separator between names:#
            editors <- gsub(", ", "%%", editors)#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
            ##
          } else {#
            # Add separator between names:#
            editors <- gsub("\\., ", ".%%", editors)#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", strsplit(editors, "%%")[[1]], "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
          ##
        } else {#
          # Case if single editor in incorrect "Intitals Surname" format:#
          if(length(grep("%%",editors)) == 0) {#
            ##
            editors <- strsplit(editors, "\\. ")[[1]]#
            ##
            editors <- paste(paste(editors[length(editors)], ",", sep = ""), paste(editors[1:(length(editors) - 1)], ".", sep = "", collapse = " "), collapse = " ")#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = ""), "\t\t</Editor>\n", sep = "")#
            # Case of two authors in incorrect "Intitals Surname" format:#
          } else {#
            ##
            editors <- strsplit(editors, "%%")[[1]]#
            ##
            for(k in 1:length(editors)) {#
              ##
              temp <- strsplit(editors[k], "\\. ")[[1]]#
              ##
              editors[k] <- paste(temp[length(temp)], paste(temp[1:(length(temp) - 1)], ".", sep = "", collapse = " "), sep = ", ")#
            }#
            ##
            editorsline <- paste("\t\t<Editor>\n", paste("\t\t\t<List>", editors, "</List>\n", sep = "", collapse = ""), "\t\t</Editor>\n", sep = "")#
          }#
        }#
        # Remove editors from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Find end of book title separator:#
        locale <- gsub("\\. ", "%%", locale)#
        # Remove trailing period:#
        locale <- gsub("\\.", "", locale)#
        # Isolate booktitle:#
        booktitleline <- paste("\t\t<Booktitle>", strsplit(locale, "%%")[[1]][1], "</Booktitle>\n", sep = "")#
        # Remove booktitle from rest of book information:#
        locale <- paste(strsplit(locale, "%%")[[1]][2:length(strsplit(locale, "%%")[[1]])], sep = "%%")#
        # Remove false gaps:#
        while(length(locale) > 1) locale <- paste(locale, collapse = ". ")#
        # Separate remaining portions:#
        locale <- strsplit(locale, ", ")[[1]]#
        ##
        publisherline <- paste("\t\t<Publisher>", locale[1], "</Publisher>\n", sep = "")#
        ##
        cityline <- paste("\t\t<City>", locale[2], "</City>\n", sep = "")#
        ##
        pagesline <- paste("\t\t<Pages>", gsub("<br>", "", gsub("p", "", locale[3])), "</Pages>\n", sep = "")#
        ##
        fulllines <- paste(authorline, yearline, titleline, "\t\t<Journal/>\n", "\t\t<Volume/>\n", pagesline, booktitleline, publisherline, cityline, editorsline, sep = "")#
        # Case if a journal:#
      } else {#
        ##
        if(year == "in press") {#
          # Case if journal title with commas:#
          if(length(locale) > 2) {#
            # Collapse journal title:#
            locale[1] <- paste(locale[1], locale[2], sep = ", ")#
            # Remove redudnant second part#
            locale <- locale[-2]#
          }#
          # Delete empty volume value#
          if(locale[2] == "") locale <- locale[-2]#
        }#
        # Find journal titles with commas:#
        while(length(locale) > 3) {#
          # Collapse journal title:#
          locale[1] <- paste(locale[1], locale[2], sep = ", ")#
          # Remove redudnant second part:#
          locale <- locale[-2]#
        }#
        ##
        journalline <- paste("\t\t<Journal>", locale[1], "</Journal>\n", sep = "")#
        ##
        if(length(locale) > 1) {#
          ##
          volumeline <- paste("\t\t<Volume>", locale[2], "</Volume>\n", sep = "")#
          ##
        } else {#
          ##
          volumeline <- "\t\t<Volume/>\n"#
        }#
        ##
        if(length(locale) > 2) {#
          ##
          pagesline <- paste("\t\t<Pages>", locale[3], "</Pages>\n", sep = "")#
          ##
        } else {#
          ##
          pagesline <- "\t\t<Pages/>\n"#
        }#
        ##
        fulllines <- paste(authorline, yearline, titleline, journalline, volumeline, pagesline, "\t\t<Booktitle/>\n", "\t\t<Publisher/>\n", "\t\t<City/>\n","\t\t<Editor/>\n", sep = "")#
      }#
    }#
    ##
    results <- c(results, fulllines)#
  }#
}#
#
# Collapse to just unique references (not sure how duplicates ended up in here...):#
results <- sort(unique(results))#
#
# Create empty vector to store hypothetical file names:#
filenames <- vector(mode = "character")#
#
# For each reference:#
for(i in 1:length(results)) {#
  # Isolate authors:#
  authors <- strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]][which(nchar(strsplit(strsplit(gsub("\n|\t", "", results[i]), split = "<Author>|</Author>")[[1]][2], split = "<List>|</List>")[[1]]) > 0)]#
  # Isolate surnames:#
  surnames <- unlist(lapply(strsplit(authors, split = ","), '[', 1))#
  # Get publication year:#
  year <- gsub(" ", "", strsplit(gsub("\n|\t", "", results[i]), split = "<Year>|</Year>")[[1]][2])#
  # If a single author:#
  if(length(surnames) == 1) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames, year, sep = "_"))))#
  # If two authors:#
  if(length(surnames) == 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(paste(surnames, collapse = "_et_"), year, sep = "_"))))#
  # If more than two authors:#
  if(length(surnames) > 2) filenames <- c(filenames, gsub("'", "", gsub(" ", "_", paste(surnames[1], "etal", year, sep = "_"))))#
}#
#
# Isolate references that have multiple file names (i.e., two or more refrences could be contracted to the same name):#
duplicates <- unique(filenames[duplicated(filenames)])#
#
# Set working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Get list of folders:#
folder.list <- list.files()[-grep("\\.", list.files())]#
#
# Get full paths for each folder:#
for(i in 1:length(folder.list)) folder.list[i] <- paste(getwd(), "/", folder.list[i], sep = "")#
#
# Vector for storing nexus file list:#
file.list <- vector(mode = "character")#
#
# Find all file paths for nexus files:#
for(i in 1:length(folder.list)) {#
  # Set working directory for current folder:#
  setwd(folder.list[i])#
  # Look for NEXUS files:#
  if(length(grep(".nex", list.files())) > 0) {#
    # Add any found to file list:#
    file.list <- c(file.list, paste(folder.list[i], "/", list.files()[grep(".nex", list.files())], sep = ""))#
  }#
}#
#
# Get just the NEXUS file names:#
nexus.files <- unlist(lapply(strsplit(file.list, "/"), '[', 9))#
#
# Reset working directory:#
setwd("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/ToAdd")#
#
# Create vector to store multiple hits:#
multi_hitters <- vector(mode = "character")#
#
# Set scratch counter:#
scratch_counter <- 1#
#
# Create nexus, tnt and xml files:#
for(i in 1:length(file.list)) {#
  # Start feedback:#
  cat("Attempting to read: ", file.list[i], "...")#
  # Get stripped verion of name (i.e., missing a, b, aa etc. ending):#
  stripped_name <- gsub(strsplit(nexus.files[i], "[:0-9:]{4}|inpress")[[1]][2], "", nexus.files[i])#
  # Get hits for stripped name in filenames:#
  hits <- grep(stripped_name, filenames)#
  # Check there is a match:#
  if(length(hits) == 0) stop("No reference with matching name.")#
  # Create reference info:#
  reference_info <- paste(results[hits], collapse = "\n\nOR\n\n")#
  # If multiple hits add to list so these can be manually checked later:#
  if(length(hits) > 1) multi_hitters <- c(multi_hitters, nexus.files[i])#
  # Read in matrix:#
  mymatrix <- ReadMorphNexus(file.list[i])#
  # Update header text:#
  mymatrix$Topper$Header <- "File downloaded from graemetlloyd.com"#
  # Make file name:#
  file.name <- gsub(".nex", "", strsplit(file.list[i], "/")[[1]][length(strsplit(file.list[i], "/")[[1]])])#
  # Write out NEXUS data:#
  WriteMorphNexus(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus", "/", file.name, ".nex", sep = ""))#
  # Write out TNT data:#
  WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/tnt", "/", file.name, ".tnt", sep = ""))#
  # Write out TNT for analysis:#
  WriteMorphTNT(CladisticMatrix = mymatrix, filename = paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""), add.analysis.block = TRUE)#
  TNTFA <- readLines(paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""))#
  # If scratch.tre is found:#
  if(length(grep("scratch.tre", TNTFA, fixed = TRUE)) > 0) {#
    # Replace scratch.tre with numbered version:#
    TNTFA <- gsub("scratch.tre", paste("scratch", scratch_counter, ".tre", sep = ""), TNTFA, fixed = TRUE)#
    # Overwrite TNT for analysis with numbered scratch.tre:#
    write(TNTFA, paste("/Users/eargtl", "/", file.name, ".tnt", sep = ""))#
    # Increment scratch counter:#
    scratch_counter <- scratch_counter + 1#
  }#
  # Make XML file:#
  myxml <- paste(paste("<?xml version=\"1.0\" standalone=\"yes\"?>\n<SourceTree>\n\t<Source>\n", reference_info, "\t</Source>"), paste("\t<Taxa number=\"", length(mymatrix$Matrix_1$Matrix[, 1]), "\">", sep = ""), paste(paste("\t\t<List recon_name=\"DELETE\" recon_no=\"-1\">", rownames(mymatrix$Matrix_1$Matrix), "</List>", sep = ""), collapse = "\n"), "\t</Taxa>\n\t<Characters>\n\t\t<Molecular/>", paste("\t\t<Morphological number=\"", sum(unlist(lapply(lapply(mymatrix[2:length(mymatrix)], '[[', "Matrix"), ncol))), "\">", sep = ""), "\t\t\t<Type>Osteology</Type>\n\t\t</Morphological>\n\t\t<Behavioural/>\n\t\t<Other/>\n\t</Characters>\n\t<Analysis>\n\t\t<Type>Maximum Parsimony</Type>\n\t</Analysis>\n\t<Notes>Based on reanalysis of the original matrix.</Notes>", paste("\t<Filename>", gsub("\\.nex", "", strsplit(file.list[i], "/")[[1]][length(strsplit(file.list[i], "/")[[1]])]), "</Filename>", sep = ""), "\t<Parent/>\n\t<Sibling/>\n</SourceTree>", sep = "\n")#
  # Write out XML file:#
  write(myxml, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/xml", "/", file.name, ".xml", sep = ""))#
  # Feedback:#
  cat("Done\n")#
}#
#
# List multiple hitters for checking:#
sort(multi_hitters)
# Open libraries:#
library(Claddis)#
library(metatree)#
#
# Set working directory:#
setwd("/Users/eargtl")#
#
# Get list of mrp files:#
mrp.list <- list.files()[grep("mrp.nex", list.files())]#
#
# Get list of mrp files:#
trees.list <- list.files()[grep("mpts_plus_strict.nex", list.files())]#
#
# Make tree files:#
for(i in 1:length(trees.list)) {#
  # Read in TNT trees and split into mpts and strict consensus:#
  mytrees <- Trees2MPTsAndStrict(trees.list[i])#
  # If the tree limit of 100000 was hit (i.e., not all MPTs are guranteed to have been sampled) or no MRP could be created due to sheer number of trees:#
  if(length(mytrees$mpts) == 100000 || sum(mrp.list == gsub("tntmpts_plus_strict.nex", "mrp.nex", trees.list[i])) == 0) {#
    # Create MRP filename:#
    mrp.filename <- gsub("tntmpts_plus_strict.nex", "mrp.nex", trees.list[i])#
    # If MRP file was generated:#
    if(length(which(mrp.list == mrp.filename)) > 0) {#
      # Remove from MRP list:#
      mrp.list <- mrp.list[-which(mrp.list == mrp.filename)]#
      # Delete raw file as likely too big anyway:#
      file.remove(mrp.filename)#
    }#
    # Create nexus file name:#
    nexus.filename <- gsub("mrp\\.nex", ".nex", mrp.filename)#
    # Read in original matrix:#
    mymatrix <- ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus", "/", nexus.filename, sep = ""))#
    # Write out regular TNT file:#
    WriteMorphTNT(mymatrix, gsub("\\.nex", ".tnt", nexus.filename))#
    # Read in TNT lines:#
    TNT.lines <- readLines(gsub("\\.nex", ".tnt", nexus.filename))#
    # Get TNT data block:#
    tnt.block <- TNT.lines[1:(grep("proc/;", TNT.lines) - 1)]#
    # Create analysis block:#
    anal.block <- paste(c("rseed*;\nhold 999;\nxmult=rss fuse 50 drift 50 ratchet 50;\nmult 50 =tbr drift;\ntsave scratch.tre;\nsave;\ntsave /;", rep("rseed*;\nhold 999;\nxmult=rss fuse 50 drift 50 ratchet 50;\nmult 50 =tbr drift;\ntsave scratch.tre +;\nsave;\ntsave /;", 4), "hold 5000;\nshortread scratch.tre;\nbbreak=tbr;"), collapse = "\n")#
    # Cretae empty vector to store final block:#
    full.block <- vector(mode = "character")#
    # Fill out all blocks for analysis:#
    for(j in 1:20) full.block <- c(full.block, paste(paste(tnt.block, collapse = "\n"), anal.block, "mrp;", paste("export ", gsub("\\.nex", "", nexus.filename), "mrp_", j, ".nex;", sep = ""), sep = "\n"))#
    # Write out TNT file:#
    write(paste(paste(full.block, collapse = "\n"), "\nproc/;\n", sep = ""), gsub("\\.nex", ".tnt", nexus.filename))#
  }#
  # Make file name:#
  file.name <- gsub("tntmpts_plus_strict.nex", "", trees.list[i])#
  # Write out MPTs:#
  write(mytrees$mpts, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mpts", "/", file.name, ".tre", sep = ""))#
  # Write out first MPT:#
  write(mytrees$mpts[1], paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/firstmpt", "/", file.name, ".tre", sep = ""))#
  # Write out strict consensus:#
  write(mytrees$strict, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/sc", "/", file.name, ".tre", sep = ""))#
  # Delete trees file now no longer needed:#
  file.remove(trees.list[i])#
  # Spit out loop position:#
  cat(i, " ")#
}#
#
# Make mrp files:#
for(i in 1:length(mrp.list)) {#
  # Add assumptions block to MRP:#
  x <- paste(c(readLines(mrp.list[i]), "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")#
  # Write out MRP file with assumptions added (can then be read in with ReadMorphNexus):#
  write(x = x, file = mrp.list[i])#
  # Read in MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Remove root taxon:#
  mymrp$Matrix_1$Matrix <- mymrp$Matrix_1$Matrix[-which(rownames(mymrp$Matrix_1$Matrix) == "ROOT"), ]#
  # Collapse to just unique characters:#
  mymrp <- CompactifyMatrix(mymrp)#
  # Overwrite weights (set all to one):#
  mymrp$Matrix_1$Weights <- rep(1, length(mymrp$Matrix_1$Weights))#
  # Make file name:#
  file.name <- gsub(".nex", "", mrp.list[i])#
  # Isolate MPR taxon names:#
  mrp.names <- rownames(mymrp$Matrix_1$Matrix)#
  # Isolate full names:#
  nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)#
  # Check to see if MRP names are contracted:#
  if(length(setdiff(mrp.names, nexus.names)) > 0) {#
    # List all contracted names:#
    contracted.names <- setdiff(mrp.names, nexus.names)#
    # For each contracted name:#
    for(j in 1:length(contracted.names)) {#
      # Get matching full name(s):#
      full.name <- nexus.names[grep(contracted.names[j], nexus.names)]#
      # Check that there are not multiple matches:#
      if(length(full.name) > 1) stop("Multiple names match contracted form. Check manually.")#
      # Overwrite contracted name with full name:#
      rownames(mymrp$Matrix_1$Matrix)[which(rownames(mymrp$Matrix_1$Matrix) == contracted.names[j])] <- full.name#
    }#
  }#
  # Write out MRP in #NEXUS format:#
  WriteMorphNexus(mymrp, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, ".nex", sep = ""))#
  # Delete file once finished:#
  file.remove(mrp.list[i])#
  # Spit out loop position:#
  cat(i, " ")#
}
# Load gdata library:#
library(gdata)#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# Get list of xml file names):#
xml.list <- list.files()#
#
# Create empty list to store XML data:#
XML.data <- vector(mode = "character")#
#
# Import XML data and store in list:#
for(i in xml.list) XML.data[i] <- ifelse(length(grep("<Title>", trim(readLines(i)))) > 0, strsplit(trim(readLines)(i)[grep("<Title>", readLines(i))], "<Title>|</Title>")[[1]][2], strsplit(trim(readLines(i))[grep("<Booktitle>", trim(readLines(i)))], "<Booktitle>|</Booktitle>")[[1]][2])#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/mpts")#
#
# Get list of tree file names (to use later in case of .zip endings):#
trees.list <- list.files()#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get file list:#
html.file.list <- list.files()[grep("matr[a-z]{4}.html", list.files())]#
#
# Create empty list to store html:#
html.data <- list()#
#
# For each set of matrices:#
for(i in 1:length(html.file.list)) {#
  # Read in the file:#
  X <- readLines(con = html.file.list[i])#
  # Get beginning sof references:#
  begins <- grep("<p class=\"hangingindent\">", X)#
  # Get endings of references:#
  ends <- grep("</p>", X)#
  # Check p tags are closed:#
  if(length(begins) != length(ends)) stop("Opening and closing paragraph marks do not match up.")#
  # For each reference add html to list (file name, title, actual text block):#
  for(j in 1:length(begins)) html.data[[(length(html.data) + 1)]] <- c(html.file.list[i], trim(X[begins[j] + 1]), X[begins[j]:ends[j]])#
}#
#
# For each html with links:#
for(i in which(unlist(lapply(html.data, length)) > 5)) {#
  # Strip down to just reference:#
  html.data[[i]] <- html.data[[i]][1:5]#
  # Remove remaining beginning of link:#
  html.data[[i]][5] <- gsub("<br><font size=\"-1\">", "</p>", html.data[[i]][5])#
}#
#
# Empty list to store matching titles from XML in HTML:#
matchedhtmldata <- list()#
#
# For each XML file:#
for(i in 1:length(XML.data)) {#
  # Store filename for ith XML file:#
  FileName <- names(XML.data)[i]#
  # Reset i as title sentence:#
  i <- XML.data[i]#
  # Get found matches (grep) for XML title in html data:#
  foundmatches <- grep(i, unlist(lapply(html.data, '[', 2)), fixed = TRUE)#
  # If length of found matcehs is zero (no matches) stop and warn:#
  if(length(foundmatches) == 0) stop("No matches!")#
  # If multiple matches find one closest in character length (most likely outcome!) and update foundmatches accordingly:#
  if(length(foundmatches) > 1) foundmatches <- foundmatches[which(abs(nchar(i) - nchar(unlist(lapply(html.data, '[', 2))[foundmatches])) == min(abs(nchar(i) - nchar(unlist(lapply(html.data, '[', 2))[foundmatches]))))]#
  # If a single match (ideal outcome):#
  if(length(foundmatches) == 1) matchedhtmldata[[(length(matchedhtmldata) + 1)]] <- foundmatches#
  # Final check that found matches do not exceed one:#
  if(length(foundmatches) > 1) {#
    # Get first names of authors on each match:#
    FirstNames <- unlist(lapply(lapply(lapply(lapply(lapply(lapply(lapply(html.data[foundmatches], '[', 3), strsplit, split = "<p class=\"hangingindent\">"), unlist), '[', 2), strsplit, split = ", "), unlist), '[', 1))#
    # Get first name of author from ith XML file:#
    FirstName <- strsplit(strsplit(FileName, split = "[:0-9:]{4}|inpress")[[1]][1], split = "_")[[1]][1]#
    # Update found matches with matching first author surname(s):#
    foundmatches <- foundmatches[which(FirstNames == FirstName)]#
    # If no matches stop and warn user:#
    if(length(foundmatches) == 0) stop("No matches found for first author name.")#
    # If a single match (ideal outcome):#
    if(length(foundmatches) == 1) matchedhtmldata[[(length(matchedhtmldata) + 1)]] <- foundmatches#
    # If multiple matches stop and warn user:#
    if(length(foundmatches) > 1) stop("Multiple matches found for title and first author.")#
  }#
}#
#
# Now only single matches convert matchedhtml to vector:#
matchedhtmldata <- unlist(matchedhtmldata)#
#
# For each HTML file that has an XML file:#
for(i in sort(unique(unlist(matchedhtmldata)))) {#
  # Get filenames for current html matches:#
  filenames <- sort(gsub(".xml", "", names(XML.data[which(matchedhtmldata == i)])))#
  # Create empty vectors to store mpt filenames and treestrings for treevector links:#
  mptsfilenames <- treestrings <- vector(mode = "character")#
  # Set working directory to strict consensus folder:#
  setwd("~/Documents/Homepage/www.graemetlloyd.com/sc")#
  # Get tree Newick strings for treevector links:#
  for(j in paste(filenames, ".tre", sep = "")) treestrings <- c(treestrings, paste("http://supfam.cs.bris.ac.uk/TreeVector/cgi-bin/maketree.cgi?topology=", gsub("\\;", "%3B", gsub("\\)", "%29", gsub("\\(", "%28", readLines(j)))), "&treetype=-clad", sep = ""))#
  # Set working directory to MPTs folder:#
  setwd("~/Documents/Homepage/www.graemetlloyd.com/mpts")#
  # Get MPTs filenames:#
  for(j in filenames) mptsfilenames <- c(mptsfilenames, list.files()[which(lapply(strsplit(list.files(), split = j), '[[', 1) == "")])#
  # Put it all together by updating html.data with links:#
  html.data[[i]] <- gsub("</p>", paste("<br><font size=\"-1\">\n                                        ", paste(paste("<a href=\"nexus/", filenames, ".nex\" target=\"_blank\">NEXUS</a> | <a href=\"tnt/", filenames, ".tnt\" target=\"_blank\">TNT</a> | <a href=\"mpts/", mptsfilenames, "\" target=\"_blank\">MPT(s)</a> <a href=\"firstmpt/", filenames, ".tre\" target=\"_blank\">(1)</a> | <a href=\"sc/", filenames, ".tre\" target=\"_blank\">SC</a> <a href=\"", treestrings, "\" target=\"_blank\">(TV)</a> | <a href=\"mrp/", filenames, "mrp.nex\" target=\"_blank\">MRP</a> | <a href=\"xml/", filenames, ".xml\" target=\"_blank\">XML</a>", sep = ""), collapse = "<br>\n                                    "), "</font></p>", sep = ""), html.data[[i]])#
}#
#
# Set working directory:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# For each html file:#
for(i in html.file.list) {#
  # Read in html for ith file:#
  fullhtml <- readLines(i)#
  # Store opening lines:#
  openinglines <- paste(fullhtml[1:max(grep("<a href=\"matr.html\">", fullhtml))], collapse = "\n")#
  # Store closing lines:#
  closinglines <- paste(fullhtml[grep("<!-- InstanceEndEditable -->", fullhtml, fixed = TRUE):length(fullhtml)], collapse = "\n")#
  # Find out which references are on the ith html page:#
  currentrefs <- which(unlist(lapply(html.data, '[', 1)) == i)#
  # Build initial references block (of html code):#
  refsblock <- html.data[currentrefs]#
  # Get publication years for each reference:#
  pubyears <- trim(gsub(".", "", unlist(lapply(lapply(strsplit(unlist(lapply(refsblock, '[', 3)), ","), rev), '[', 1)), fixed = TRUE))#
  # Little check that references are in order:#
  if(any(diff(as.numeric(setdiff(pubyears, "in press"))) > 0)) stop("References out of order!")#
  # Replace lower case in with upper case In for in press stuff:#
  pubyears <- gsub("in press", "In press", pubyears)#
  # Create new refs block (for stroing them collapsed by pub year):#
  newrefsblock <- vector(mode = "character")#
  # For each unique publication year create html block:#
  for(j in unique(pubyears)) newrefsblock <- c(newrefsblock, paste(paste("                                    <h4>", j, "</h4>\n\n", sep = ""), paste(unlist(lapply(lapply(refsblock[which(pubyears == j)], '[', 3:5), paste, collapse = "\n")), collapse = "\n\n"), sep = ""))#
  # Write html to file:#
  write(paste(c(openinglines, newrefsblock, closinglines), collapse = "\n\n"), file = i)#
}
# Load metatree library:#
library(metatree)#
#
# Setw roking directory to XML folder:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# Get list of all XML files:#
XMLFiles <- list.files()#
#
# Read all XML files into a list:#
XMLs <- lapply(as.list(XMLFiles), function(x) metatree::ReadMetatreeXML(x))#
#
# Remove all taxonomic reconciliations:#
XMLs <- lapply(XMLs, function(x) {x$SourceTree$Taxa$TagContents[, "recon_name"] <- "DELETE"; x$SourceTree$Taxa$TagContents[, "recon_no"] <- "-1"; x$SourceTree$Taxa$TagContents; x})#
#
# Remove all parent-sibling tags:#
XMLs <- lapply(XMLs, function(x) {x$SourceTree$Parent <- list(NULL); x$SourceTree$Parent <- list(NULL); x})#
#
# Set working directory to release folder:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xmlrelease")#
#
# Write files to release folder:#
mapply(function(x, y) metatree::WriteMetatreeXML(x, y), x = XMLs, y = as.list(XMLFiles))
# CODE TO FILL OUT SAFE FIRST GUESSES FOR TAXON RECONCILIATION WHERE MISSING FROM XML FILES#
# I.E., USES EXACT SPECIES NAMES AND PALEOBIOLOGY DATABASE OR USES RECONCILIATION FROM PARENT DATASET FOR EXACT SAME OTU OTHERWISE OTUS ARE NOT ALTERED AT ALL#
#
# Load libraries:#
library(metatree)#
library(Claddis)#
#
# Set working directory to HTML files:#
setwd("~/Documents/Homepage/www.graemetlloyd.com")#
#
# Get list of matrix HTMLs:#
MatrixHTMLfiles <- list.files()[grep("matr[:a-z:]{4}.html", list.files())]#
#
# Create empty HTML list:#
MatrixHTML <- list()#
#
# Read in each raw HTML code into list:#
for(i in 1:length(MatrixHTMLfiles)) MatrixHTML[[i]] <- readLines(MatrixHTMLfiles[i])#
#
# Add file names to HTML list:#
names(MatrixHTML) <- gsub(",html", "", MatrixHTMLfiles)#
#
# Set working directory as XML folder:#
setwd("~/Documents/Homepage/www.graemetlloyd.com/xml")#
#
# List all XML files:#
xmlfiles <- list.files()#
#
# Create empty vector to store parent datasets:#
parentdataset <- vector(mode = "character")#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    parentdataset <- c(parentdataset, ifelse(length(grep("<Parent>", currentxml)) > 0, strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3], ""))#
}#
#
# Add file names to parent data set vector:#
names(parentdataset) <- gsub(".xml", "", xmlfiles)#
#
# Get vector of "dead" parents, i.e., those not currently present in the data pool:#
deadparents <- sort(setdiff(unique(parentdataset), names(parentdataset)))[which(nchar(sort(setdiff(unique(parentdataset), names(parentdataset)))) > 0)]#
#
# Remove dead parents from parent list:#
for(i in deadparents) parentdataset[which(parentdataset == i)] <- ""#
#
# Create empty parent depth vector (number of links to original data set):#
parentdepth <- vector(mode = "numeric")#
#
# For each data set:#
for(i in names(parentdataset)) {#
    # Set starting depth at zero (no parent at all):#
    currentdepth <- 0#
    # If there is at least an initial parent:#
    if(parentdataset[i] != "") {#
        # Increase depth by one:#
        currentdepth <- currentdepth + 1#
        # While there are further parents:#
        while(parentdataset[i] != "") {#
            # Increase depth by one#
            currentdepth <- currentdepth + 1#
            # Update new parent:#
            i <- parentdataset[i]#
        }#
    }#
    # Add parent depth to vector:#
    parentdepth <- c(parentdepth, currentdepth)#
}#
#
# Reorder xml file lists by parent depth (ensures parents are filled out before children so names can always be carried forwards):#
xmlfiles <- xmlfiles[order(parentdepth, decreasing = FALSE)]#
#
# Count of total OTUs (start with zero):#
TotalOTUs <- 0#
#
# For each XML file:#
for(i in xmlfiles) {#
    # Read in ith XML file:#
    currentxml <- readLines(i)#
    # If there is a parent data set (from which to draw recinciliation data from):#
    if(length(grep("<Parent>", currentxml)) > 0) {#
        # Get parent data set file name:#
        parentdataset <- strsplit(currentxml[grep("<Parent>", currentxml)], "<|>")[[1]][3]#
        # Check parent has been processed (has an XML file on which to draw):#
        if(!is.na(match(paste(parentdataset, ".xml", sep = ""), xmlfiles))) {#
            # Isolate taxon names block:#
            taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
            # Reformat as matrix:#
            taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
            # Add number of OTUs to count:#
            TotalOTUs <- TotalOTUs + nrow(taxonnameblock)#
            # If there are unreconciled taxa:#
            if(any(taxonnameblock[, "ReconNo"] == "-1")) {#
                # Get just the unreconciled names (we don't care about ones already done):#
                unreconcilednames <- taxonnameblock[which(taxonnameblock[, "ReconNo"] == "-1"), "OTUName"]#
                # Read in ith XML file:#
                parentxml <- readLines(paste(parentdataset, ".xml", sep = ""))#
                # Isolate taxon names block:#
                parenttaxonnameblock <- parentxml[(grep("<Taxa", parentxml) + 1):(grep("</Taxa", parentxml) - 1)]#
                # Reformat as matrix:#
                parenttaxonnameblock <- matrix(unlist(lapply(strsplit(parenttaxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
                # If at least one name can be reconciled using parent data set data:#
                if(any(!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"])))) {#
                    # Update unreconciled names as just those also present in parent data:#
                    unreconcilednames <- unreconcilednames[!is.na(match(unreconcilednames, parenttaxonnameblock[, "OTUName"]))]#
#
                    # Update taxon names block with parent data:#
                    taxonnameblock[match(unreconcilednames, taxonnameblock[, "OTUName"]), ] <- parenttaxonnameblock[match(unreconcilednames, parenttaxonnameblock[, "OTUName"]), ]#
                }#
            }#
            # Add taxonblock back into currentxml:#
            currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)] <- paste("\t\t<List recon_name=\"", taxonnameblock[, "ReconName"], "\" recon_no=\"", taxonnameblock[, "ReconNo"], "\">", taxonnameblock[, "OTUName"], "</List>", sep = "")#
#
        }#
#
    }#
    # Write out XML:#
    write(x = paste(currentxml, collapse = "\n"), file = i)#
}#
#
# Set empty priorites matrix:#
Priorities <- matrix(nrow = 0, ncol = 4, dimnames = list(c(), c("Name", "NUnreconciled", "ParentDepth", "HTMLFile")))#
#
# For each XML file:#
for(i in xmlfiles) {#
  # Read in ith XML file:#
  currentxml <- readLines(i)#
  # Work out which html file the data set belongs to:#
  currenthtml <- names(which(unlist(lapply(lapply(MatrixHTML, grep, pattern = paste("xml/", i, sep = "")), length)) == 1))#
  # Isolate taxon names block:#
  taxonnameblock <- currentxml[(grep("<Taxa", currentxml) + 1):(grep("</Taxa", currentxml) - 1)]#
  # Reformat as matrix:#
  taxonnameblock <- matrix(unlist(lapply(strsplit(taxonnameblock, "\"|>|<"), '[', c(3, 5, 7))), ncol = 3, byrow = TRUE, dimnames = list(c(), c("ReconName", "ReconNo", "OTUName")))#
  # Add to priorities matrix:#
  Priorities <- rbind(Priorities, c(i, sum(taxonnameblock[, "ReconNo"] == "-1"), parentdepth[order(parentdepth, decreasing = FALSE)][which(xmlfiles == i)], currenthtml))#
}#
#
# Remove any XMLs where all taxa are already reconciled:#
Priorities <- Priorities[-which(Priorities[, "NUnreconciled"] == "0"), , drop = FALSE]#
#
# Number of files that still need taxa reconciled:#
paste(nrow(Priorities), " files still contain unreconciled taxa (", round(((length(xmlfiles) - nrow(Priorities)) / length(xmlfiles) * 100), 2), "% complete)", sep = "")#
#
# Number of individual OTU names that still need reconciling:#
paste(sum(as.numeric(Priorities[, "NUnreconciled"])), " OTUs are still unreconciled (", round(((TotalOTUs - sum(as.numeric(Priorities[, "NUnreconciled"]))) / TotalOTUs * 100), 2), "% complete)", sep = "")#
#
# Order by number of unreconciled OTUs:#
Priorities <- Priorities[order(as.numeric(Priorities[, "NUnreconciled"])), ]#
#
# Display priorities for archosaurs (excluding Cenozoic birds):#
Priorities[sort(c(which(Priorities[, "HTMLFile"] == "matrarch.html"), which(Priorities[, "HTMLFile"] == "matrdino.html"))), ]
# Get functions in:#
library(Claddis)#
library(ade4)#
library(foreach)#
library(doParallel)#
#
# Register parallel back end as number of cores available:#
registerDoParallel(cores = 4)#
#
# Set working directory:#
setwd("/Users/eargtl")#
#
# Get list of mrp files:#
mrp.list <- list.files()[grep("mrp_", list.files())]#
#
# For each MRP file:#
for(i in 1:length(mrp.list)) {#
  # Read in raw MRP file:#
  x <- readLines(mrp.list[i])#
  # If there is no assumptions block:#
  if(length(grep("begin assumptions", x, ignore.case = TRUE)) == 0) {#
    # Add assumptions block to MRP:#
    x <- paste(c(x, "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")#
    # Write out MRP file with assumptions added (can then be read in with ReadMorphNexus):#
    write(x = x, file = mrp.list[i])#
  }#
}#
#
# Make mrp files:#
x <- foreach(i = 1:length(mrp.list), .combine = "rbind") %dopar% {#
#
  # Read in ith MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Compactify the matrix:#
  mymrp <- CompactifyMatrix(mymrp)#
  # To avoid non-numeric weight (e.g., 1+e05) set all weights above ten to ten:#
  if(sum(mymrp$Matrix_1$Weights > 10)) mymrp$Matrix_1$Weights[which(mymrp$Matrix_1$Weights > 10)] <- 10#
  # If any rogue NAs are found prune these from the data:#
  if(length(unique(as.vector(mymrp$Matrix_1$Matrix))) > 2) mymrp <- MatrixPruner(mymrp, characters2prune = which((apply(apply(mymrp$Matrix_1$Matrix, 2, '==', "0") + apply(mymrp$Matrix_1$Matrix, 2, '==', "1"), 2, sum)) < nrow(mymrp$Matrix_1$Matrix)))#
#
  # Overwrite original data with compactified version:#
  WriteMorphNexus(mymrp, mrp.list[i])#
}#
#
# Get unique data set names:#
data.sets <- unique(matrix(unlist(strsplit(mrp.list, "mrp_")), ncol = 2, byrow = TRUE)[, 1])#
#
# For each data set:#
for(i in 1:length(data.sets)) {#
  # Get numbers for files to read in:#
  files.to.load <- grep(data.sets[i], mrp.list)#
  # For each file in data set:#
  for(j in files.to.load) {#
    # Read in current matrix:#
    current.matrix <- ReadMorphNexus(mrp.list[j])#
    # Sort by row name to ensure taxa line up later:#
    current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[sort(rownames(current.matrix$Matrix_1$Matrix)), ]#
    # If first file of data set:#
    if(files.to.load[1] == j) {#
      # Set matrix using current matrix:#
      MATRIX <- current.matrix$Matrix_1$Matrix#
      # Set weights using current matrix:#
      WEIGHTS <- current.matrix$Matrix_1$Weights#
    # If not first file of data set:#
    } else {#
      # Add current matrix to data set:#
      MATRIX <- cbind(MATRIX, current.matrix$Matrix_1$Matrix)#
      # Add current weights to data set:#
      WEIGHTS <- c(WEIGHTS, current.matrix$Matrix_1$Weights)#
    }#
  }#
  # Overwrite current matrix with full data set:#
  current.matrix$Matrix_1$Matrix <- MATRIX#
  # Overwrite current matrix weights with full data set:#
  current.matrix$Matrix_1$Weights <- WEIGHTS#
  # Set ordering for full data set:#
  current.matrix$Matrix_1$Ordering <- rep("unord", ncol(current.matrix$Matrix_1$Matrix))#
  # Set maximum values for full data set:#
  current.matrix$Matrix_1$MinVals <- rep(1, ncol(current.matrix$Matrix_1$Matrix))#
  # Set minimum values for full data set:#
  current.matrix$Matrix_1$MaxVals <- rep(0, ncol(current.matrix$Matrix_1$Matrix))#
  # Collapse data set:#
  current.matrix <- CompactifyMatrix(current.matrix)#
  # To avoid non-numeric weight (e.g., 1+e05) set all weights above ten to ten:#
  if(sum(current.matrix$Matrix_1$Weights > 10)) current.matrix$Matrix_1$Weights[which(current.matrix$Matrix_1$Weights > 10)] <- 10#
  # Make file name:#
  file.name <- data.sets[i]#
  # Case if MRP is done (minimum weight is greater than 1):#
  if(min(current.matrix$Matrix_1$Weights) > 1) {#
    # Remove "ROOT" taxon if present:#
    if(sum(rownames(current.matrix$Matrix_1$Matrix) == "ROOT") > 0) current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[-which(rownames(current.matrix$Matrix_1$Matrix) == "ROOT"), ]#
    # Collapse matrix again:#
    current.matrix <- CompactifyMatrix(current.matrix)#
    # Overwrite all weights with 1:#
    current.matrix$Matrix_1$Weights <- rep(1, length(current.matrix$Matrix_1$Weights))#
    # Update matrix in nesting order (outgroup first):#
    current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[names(sort(apply(apply(current.matrix$Matrix_1$Matrix, 1, as.numeric), 2, sum))), ]#
    # Isolate MRP taxon names:#
    mrp.names <- rownames(current.matrix$Matrix_1$Matrix)#
    # Isolate full names:#
    nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)#
    # Check to see if MRP names are contracted:#
    if(length(setdiff(mrp.names, nexus.names)) > 0) {#
      # List all contracted names:#
      contracted.names <- setdiff(mrp.names, nexus.names)#
      # For each contracted name:#
      for(j in 1:length(contracted.names)) {#
        # Get matching full name(s):#
        full.name <- nexus.names[grep(contracted.names[j], nexus.names)]#
        # Check that there are not multiple matches:#
        if(length(full.name) > 1) stop("Multiple names match contracted form. Check manually.")#
        # Overwrite contracted name with full name:#
        rownames(current.matrix$Matrix_1$Matrix)[which(rownames(current.matrix$Matrix_1$Matrix) == contracted.names[j])] <- full.name#
      }#
    }#
    # Write out MRP in #NEXUS format:#
    WriteMorphNexus(current.matrix, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, "mrp.nex", sep = ""))#
    # Remove dead files:#
    file.remove(c(mrp.list[files.to.load], paste(file.name, ".tnt", sep = "")))#
  # Case if MRP needs to continue (minimum weight is 1):#
  } else {#
    # Remove dead files:#
    file.remove(mrp.list[files.to.load])#
    # Write out MRP in #NEXUS format:#
    WriteMorphNexus(current.matrix, paste("/Users/eargtl/", file.name, "mrp_0.nex", sep = ""))#
  }#
  # Output loop position:#
  cat(i, " ")#
}
library(CLaddis)
library(Claddis)
x <- ReadMorphNexus("~/Documents/Homepage/www.graemetlloyd.com/nexus/Brocklehurst_etal_2013a.nex")
STR(x)
y <- SafeTaxonomicReduction(x)
y$removed.matrix
names(y)
y$reduced.matrix
WriteMorphTNT(y$reduced.matrix, "~/Brocklehurst_etal_2013aR.tnt")
# Get functions in:#
library(Claddis)#
library(ade4)#
library(foreach)#
library(doParallel)#
#
# Register parallel back end as number of cores available:#
registerDoParallel(cores = 4)#
#
# Set working directory:#
setwd("/Users/eargtl")#
#
# Get list of mrp files:#
mrp.list <- list.files()[grep("mrp_", list.files())]#
#
# For each MRP file:#
for(i in 1:length(mrp.list)) {#
  # Read in raw MRP file:#
  x <- readLines(mrp.list[i])#
  # If there is no assumptions block:#
  if(length(grep("begin assumptions", x, ignore.case = TRUE)) == 0) {#
    # Add assumptions block to MRP:#
    x <- paste(c(x, "BEGIN ASSUMPTIONS;", "OPTIONS  DEFTYPE=unord PolyTcount=MINSTEPS ;", "END;"), collapse = "\n")#
    # Write out MRP file with assumptions added (can then be read in with ReadMorphNexus):#
    write(x = x, file = mrp.list[i])#
  }#
}#
#
# Make mrp files:#
x <- foreach(i = 1:length(mrp.list), .combine = "rbind") %dopar% {#
#
  # Read in ith MRP file:#
  mymrp <- ReadMorphNexus(mrp.list[i])#
  # Compactify the matrix:#
  mymrp <- CompactifyMatrix(mymrp)#
  # To avoid non-numeric weight (e.g., 1+e05) set all weights above ten to ten:#
  if(sum(mymrp$Matrix_1$Weights > 10)) mymrp$Matrix_1$Weights[which(mymrp$Matrix_1$Weights > 10)] <- 10#
  # If any rogue NAs are found prune these from the data:#
  if(length(unique(as.vector(mymrp$Matrix_1$Matrix))) > 2) mymrp <- MatrixPruner(mymrp, characters2prune = which((apply(apply(mymrp$Matrix_1$Matrix, 2, '==', "0") + apply(mymrp$Matrix_1$Matrix, 2, '==', "1"), 2, sum)) < nrow(mymrp$Matrix_1$Matrix)))#
#
  # Overwrite original data with compactified version:#
  WriteMorphNexus(mymrp, mrp.list[i])#
}#
#
# Get unique data set names:#
data.sets <- unique(matrix(unlist(strsplit(mrp.list, "mrp_")), ncol = 2, byrow = TRUE)[, 1])#
#
# For each data set:#
for(i in 1:length(data.sets)) {#
  # Get numbers for files to read in:#
  files.to.load <- grep(data.sets[i], mrp.list)#
  # For each file in data set:#
  for(j in files.to.load) {#
    # Read in current matrix:#
    current.matrix <- ReadMorphNexus(mrp.list[j])#
    # Sort by row name to ensure taxa line up later:#
    current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[sort(rownames(current.matrix$Matrix_1$Matrix)), ]#
    # If first file of data set:#
    if(files.to.load[1] == j) {#
      # Set matrix using current matrix:#
      MATRIX <- current.matrix$Matrix_1$Matrix#
      # Set weights using current matrix:#
      WEIGHTS <- current.matrix$Matrix_1$Weights#
    # If not first file of data set:#
    } else {#
      # Add current matrix to data set:#
      MATRIX <- cbind(MATRIX, current.matrix$Matrix_1$Matrix)#
      # Add current weights to data set:#
      WEIGHTS <- c(WEIGHTS, current.matrix$Matrix_1$Weights)#
    }#
  }#
  # Overwrite current matrix with full data set:#
  current.matrix$Matrix_1$Matrix <- MATRIX#
  # Overwrite current matrix weights with full data set:#
  current.matrix$Matrix_1$Weights <- WEIGHTS#
  # Set ordering for full data set:#
  current.matrix$Matrix_1$Ordering <- rep("unord", ncol(current.matrix$Matrix_1$Matrix))#
  # Set maximum values for full data set:#
  current.matrix$Matrix_1$MinVals <- rep(1, ncol(current.matrix$Matrix_1$Matrix))#
  # Set minimum values for full data set:#
  current.matrix$Matrix_1$MaxVals <- rep(0, ncol(current.matrix$Matrix_1$Matrix))#
  # Collapse data set:#
  current.matrix <- CompactifyMatrix(current.matrix)#
  # To avoid non-numeric weight (e.g., 1+e05) set all weights above ten to ten:#
  if(sum(current.matrix$Matrix_1$Weights > 10)) current.matrix$Matrix_1$Weights[which(current.matrix$Matrix_1$Weights > 10)] <- 10#
  # Make file name:#
  file.name <- data.sets[i]#
  # Case if MRP is done (minimum weight is greater than 1):#
  if(min(current.matrix$Matrix_1$Weights) > 1) {#
    # Remove "ROOT" taxon if present:#
    if(sum(rownames(current.matrix$Matrix_1$Matrix) == "ROOT") > 0) current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[-which(rownames(current.matrix$Matrix_1$Matrix) == "ROOT"), ]#
    # Collapse matrix again:#
    current.matrix <- CompactifyMatrix(current.matrix)#
    # Overwrite all weights with 1:#
    current.matrix$Matrix_1$Weights <- rep(1, length(current.matrix$Matrix_1$Weights))#
    # Update matrix in nesting order (outgroup first):#
    current.matrix$Matrix_1$Matrix <- current.matrix$Matrix_1$Matrix[names(sort(apply(apply(current.matrix$Matrix_1$Matrix, 1, as.numeric), 2, sum))), ]#
    # Isolate MRP taxon names:#
    mrp.names <- rownames(current.matrix$Matrix_1$Matrix)#
    # Isolate full names:#
    nexus.names <- rownames(ReadMorphNexus(paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/nexus/", gsub("mrp", "", file.name), ".nex", sep = ""))$Matrix_1$Matrix)#
    # Check to see if MRP names are contracted:#
    if(length(setdiff(mrp.names, nexus.names)) > 0) {#
      # List all contracted names:#
      contracted.names <- setdiff(mrp.names, nexus.names)#
      # For each contracted name:#
      for(j in 1:length(contracted.names)) {#
        # Get matching full name(s):#
        full.name <- nexus.names[grep(contracted.names[j], nexus.names)]#
        # Check that there are not multiple matches:#
        if(length(full.name) > 1) stop("Multiple names match contracted form. Check manually.")#
        # Overwrite contracted name with full name:#
        rownames(current.matrix$Matrix_1$Matrix)[which(rownames(current.matrix$Matrix_1$Matrix) == contracted.names[j])] <- full.name#
      }#
    }#
    # Write out MRP in #NEXUS format:#
    WriteMorphNexus(current.matrix, paste("/Users/eargtl/Documents/Homepage/www.graemetlloyd.com/mrp", "/", file.name, "mrp.nex", sep = ""))#
    # Remove dead files:#
    file.remove(c(mrp.list[files.to.load], paste(file.name, ".tnt", sep = "")))#
  # Case if MRP needs to continue (minimum weight is 1):#
  } else {#
    # Remove dead files:#
    file.remove(mrp.list[files.to.load])#
    # Write out MRP in #NEXUS format:#
    WriteMorphNexus(current.matrix, paste("/Users/eargtl/", file.name, "mrp_0.nex", sep = ""))#
  }#
  # Output loop position:#
  cat(i, " ")#
}
# NB: Assumes you have already sent an STR version of the data set through the full protocol to completion (min weight greater than zero)#
#
# Load libraries:#
library(Claddis)#
library(metatree)#
#
# Read in target MRP:#
MRP <- ReadMorphNexus("/Users/eargtl/Brocklehurst_etal_2013amrp.nex")#
#
# Read in source NEXUS:#
NEXUS <- ReadMorphNexus("/Users/eargtl/Brocklehurst_etal_2013a.nex")#
#
# Perform STR on NEXUS to get reinsertion list:#
STR <- SafeTaxonomicReduction(NEXUS)#
#
# Order taxa so can reinsert in order of fewest to most senior taxa:#
TaxaInOrder <- rle(sort(STR$str.list[, 1]))$values[order(rle(sort(STR$str.list[, 1]))$lengths)]#
#
# For each taxon to be reinserted:#
for(i in TaxaInOrder) {#
    # Print current loop position:#
    cat(paste("Attempting to reinsert taxon ", which(TaxaInOrder == i), " of ", length(TaxaInOrder), "\n", sep = ""))#
    # Check taxon has not already been reinserted:#
    if(sum(rownames(MRP$Matrix_1$Matrix) == i) == 1) {#
        # Report if found:#
        cat("Taxon already inserted\n")#
    # If not found continue to reinsertion step:#
    } else {#
        # Get senior taxa for current junior:#
        Senior_taxa <- STR$str.list[STR$str.list[, "Junior"] == i, "Senior"]#
        # If there are more than two senior taxa (want to "chunk" the analysis to get around memory issues:#
        if(length(Senior_taxa) > 2) {#
            # Add scores for first senior taxon to MRP matrix as a new row:#
            MRP$Matrix_1$Matrix <- rbind(MRP$Matrix_1$Matrix, MRP$Matrix_1$Matrix[Senior_taxa[1], ])#
            # Give that row the name of the current junior:#
            rownames(MRP$Matrix_1$Matrix)[nrow(MRP$Matrix_1$Matrix)] <- i#
            # Now for each subsequent senior taxon:#
            for(j in 2:length(Senior_taxa)) {#
                # Get new MRP row for current junior:#
                NewMRPRow <- c(MRP$Matrix_1$Matrix[i, ], MRP$Matrix_1$Matrix[Senior_taxa[j], ])#
#
                # Duplicate matrix:#
                DuplicatedMatrix <- matrix(rep(MRP$Matrix_1$Matrix, times = 2), nrow = nrow(MRP$Matrix_1$Matrix), dimnames = list(rownames(MRP$Matrix_1$Matrix), c()))#
                # Update junior with enw row (includes scorings for jth taxon):#
                DuplicatedMatrix[i, ] <- NewMRPRow#
                # Turn into a proper cladistic matrix:#
                DuplicatedMatrix <- MakeMorphMatrix(CTmatrix = DuplicatedMatrix)#
                # Collapse MRP to remove duplicated taxa:#
                DuplicatedMatrix <- MRPCollapse(DuplicatedMatrix)#
                # Overwrite MRP with new matrix:#
                MRP <- DuplicatedMatrix#
            }#
            # Write out new MRP:#
            WriteMorphNexus(MRP, "/Users/eargtl/Arcila_etal_2015abmrp.nex")#
        # If there are only one or two senior taxa (no easy way to chunk the task):#
        } else {#
            # Duplicate matrix for every senior taxon:#
            DuplicatedMatrix <- matrix(rep(MRP$Matrix_1$Matrix, times = length(Senior_taxa)), nrow = nrow(MRP$Matrix_1$Matrix), dimnames = list(rownames(MRP$Matrix_1$Matrix), c()))#
            # Get new MRP row for current junior:#
            NewMRPRow <- as.vector(t(MRP$Matrix_1$Matrix[Senior_taxa, ]))#
            # Add new row to duplicated matrix:#
            DuplicatedMatrix <- rbind(DuplicatedMatrix, NewMRPRow)#
            # Update last row name to current junior:#
            rownames(DuplicatedMatrix)[nrow(DuplicatedMatrix)] <- i#
            # Turn into a proper cladistic matrix:#
            DuplicatedMatrix <- MakeMorphMatrix(CTmatrix = DuplicatedMatrix)#
            # Collapse MRP to remove duplicated taxa:#
            DuplicatedMatrix <- MRPCollapse(DuplicatedMatrix)#
            # Overwrite MRP with new matrix:#
            MRP <- DuplicatedMatrix#
            # Write out new MRP:#
            WriteMorphNexus(MRP, "/Users/eargtl/Arcila_etal_2015abmrp.nex")#
        }#
    }#
}
# NB: Assumes you have already sent an STR version of the data set through the full protocol to completion (min weight greater than zero)#
#
# Load libraries:#
library(Claddis)#
library(metatree)#
#
# Read in target MRP:#
MRP <- ReadMorphNexus("/Users/eargtl/Brocklehurst_etal_2013amrp.nex")#
#
# Read in source NEXUS:#
NEXUS <- ReadMorphNexus("/Users/eargtl/Brocklehurst_etal_2013a.nex")#
#
# Perform STR on NEXUS to get reinsertion list:#
STR <- SafeTaxonomicReduction(NEXUS)#
#
# Order taxa so can reinsert in order of fewest to most senior taxa:#
TaxaInOrder <- rle(sort(STR$str.list[, 1]))$values[order(rle(sort(STR$str.list[, 1]))$lengths)]
TaxaInOrder
names(STR$str.list)
STR$str.list
?MakeMorphMatrix
# NB: Assumes you have already sent an STR version of the data set through the full protocol to completion (min weight greater than zero)#
#
# Load libraries:#
library(Claddis)#
library(metatree)#
#
# Read in target MRP:#
MRP <- ReadMorphNexus("/Users/eargtl/Brocklehurst_etal_2013amrp.nex")#
#
# Read in source NEXUS:#
NEXUS <- ReadMorphNexus("/Users/eargtl/Brocklehurst_etal_2013a.nex")#
#
# Perform STR on NEXUS to get reinsertion list:#
STR <- SafeTaxonomicReduction(NEXUS)#
#
# Order taxa so can reinsert in order of fewest to most senior taxa:#
TaxaInOrder <- rle(sort(STR$str.list[, 1]))$values[order(rle(sort(STR$str.list[, 1]))$lengths)]#
#
# For each taxon to be reinserted:#
for(i in TaxaInOrder) {#
    # Print current loop position:#
    cat(paste("Attempting to reinsert taxon ", which(TaxaInOrder == i), " of ", length(TaxaInOrder), "\n", sep = ""))#
    # Check taxon has not already been reinserted:#
    if(sum(rownames(MRP$Matrix_1$Matrix) == i) == 1) {#
        # Report if found:#
        cat("Taxon already inserted\n")#
    # If not found continue to reinsertion step:#
    } else {#
        # Get senior taxa for current junior:#
        Senior_taxa <- STR$str.list[STR$str.list[, "Junior"] == i, "Senior"]#
        # If there are more than two senior taxa (want to "chunk" the analysis to get around memory issues:#
        if(length(Senior_taxa) > 2) {#
            # Add scores for first senior taxon to MRP matrix as a new row:#
            MRP$Matrix_1$Matrix <- rbind(MRP$Matrix_1$Matrix, MRP$Matrix_1$Matrix[Senior_taxa[1], ])#
            # Give that row the name of the current junior:#
            rownames(MRP$Matrix_1$Matrix)[nrow(MRP$Matrix_1$Matrix)] <- i#
            # Now for each subsequent senior taxon:#
            for(j in 2:length(Senior_taxa)) {#
                # Get new MRP row for current junior:#
                NewMRPRow <- c(MRP$Matrix_1$Matrix[i, ], MRP$Matrix_1$Matrix[Senior_taxa[j], ])#
#
                # Duplicate matrix:#
                DuplicatedMatrix <- matrix(rep(MRP$Matrix_1$Matrix, times = 2), nrow = nrow(MRP$Matrix_1$Matrix), dimnames = list(rownames(MRP$Matrix_1$Matrix), c()))#
                # Update junior with enw row (includes scorings for jth taxon):#
                DuplicatedMatrix[i, ] <- NewMRPRow#
                # Turn into a proper cladistic matrix:#
                DuplicatedMatrix <- MakeMorphMatrix(CharacterTaxonMatrix = DuplicatedMatrix)#
                # Collapse MRP to remove duplicated taxa:#
                DuplicatedMatrix <- MRPCollapse(DuplicatedMatrix)#
                # Overwrite MRP with new matrix:#
                MRP <- DuplicatedMatrix#
            }#
            # Write out new MRP:#
            WriteMorphNexus(MRP, "/Users/eargtl/Arcila_etal_2015abmrp.nex")#
        # If there are only one or two senior taxa (no easy way to chunk the task):#
        } else {#
            # Duplicate matrix for every senior taxon:#
            DuplicatedMatrix <- matrix(rep(MRP$Matrix_1$Matrix, times = length(Senior_taxa)), nrow = nrow(MRP$Matrix_1$Matrix), dimnames = list(rownames(MRP$Matrix_1$Matrix), c()))#
            # Get new MRP row for current junior:#
            NewMRPRow <- as.vector(t(MRP$Matrix_1$Matrix[Senior_taxa, ]))#
            # Add new row to duplicated matrix:#
            DuplicatedMatrix <- rbind(DuplicatedMatrix, NewMRPRow)#
            # Update last row name to current junior:#
            rownames(DuplicatedMatrix)[nrow(DuplicatedMatrix)] <- i#
            # Turn into a proper cladistic matrix:#
            DuplicatedMatrix <- MakeMorphMatrix(CharacterTaxonMatrix = DuplicatedMatrix)#
            # Collapse MRP to remove duplicated taxa:#
            DuplicatedMatrix <- MRPCollapse(DuplicatedMatrix)#
            # Overwrite MRP with new matrix:#
            MRP <- DuplicatedMatrix#
            # Write out new MRP:#
            WriteMorphNexus(MRP, "/Users/eargtl/Arcila_etal_2015abmrp.nex")#
        }#
    }#
}
# NB: Assumes you have already sent an STR version of the data set through the full protocol to completion (min weight greater than zero)#
#
# Load libraries:#
library(Claddis)#
library(metatree)#
#
# Read in target MRP:#
MRP <- ReadMorphNexus("/Users/eargtl/Brocklehurst_etal_2013amrp.nex")#
#
# Read in source NEXUS:#
NEXUS <- ReadMorphNexus("/Users/eargtl/Brocklehurst_etal_2013a.nex")#
#
# Perform STR on NEXUS to get reinsertion list:#
STR <- SafeTaxonomicReduction(NEXUS)#
#
# Order taxa so can reinsert in order of fewest to most senior taxa:#
TaxaInOrder <- rle(sort(STR$str.list[, 1]))$values[order(rle(sort(STR$str.list[, 1]))$lengths)]#
#
# For each taxon to be reinserted:#
for(i in TaxaInOrder) {#
    # Print current loop position:#
    cat(paste("Attempting to reinsert taxon ", which(TaxaInOrder == i), " of ", length(TaxaInOrder), "\n", sep = ""))#
    # Check taxon has not already been reinserted:#
    if(sum(rownames(MRP$Matrix_1$Matrix) == i) == 1) {#
        # Report if found:#
        cat("Taxon already inserted\n")#
    # If not found continue to reinsertion step:#
    } else {#
        # Get senior taxa for current junior:#
        Senior_taxa <- STR$str.list[STR$str.list[, "Junior"] == i, "Senior"]#
        # If there are more than two senior taxa (want to "chunk" the analysis to get around memory issues:#
        if(length(Senior_taxa) > 2) {#
            # Add scores for first senior taxon to MRP matrix as a new row:#
            MRP$Matrix_1$Matrix <- rbind(MRP$Matrix_1$Matrix, MRP$Matrix_1$Matrix[Senior_taxa[1], ])#
            # Give that row the name of the current junior:#
            rownames(MRP$Matrix_1$Matrix)[nrow(MRP$Matrix_1$Matrix)] <- i#
            # Now for each subsequent senior taxon:#
            for(j in 2:length(Senior_taxa)) {#
                # Get new MRP row for current junior:#
                NewMRPRow <- c(MRP$Matrix_1$Matrix[i, ], MRP$Matrix_1$Matrix[Senior_taxa[j], ])#
#
                # Duplicate matrix:#
                DuplicatedMatrix <- matrix(rep(MRP$Matrix_1$Matrix, times = 2), nrow = nrow(MRP$Matrix_1$Matrix), dimnames = list(rownames(MRP$Matrix_1$Matrix), c()))#
                # Update junior with enw row (includes scorings for jth taxon):#
                DuplicatedMatrix[i, ] <- NewMRPRow#
                # Turn into a proper cladistic matrix:#
                DuplicatedMatrix <- MakeMorphMatrix(CharacterTaxonMatrix = DuplicatedMatrix)#
                # Collapse MRP to remove duplicated taxa:#
                DuplicatedMatrix <- MRPCollapse(DuplicatedMatrix)#
                # Overwrite MRP with new matrix:#
                MRP <- DuplicatedMatrix#
            }#
            # Write out new MRP:#
            WriteMorphNexus(MRP, "/Users/eargtl/Brocklehurst_etal_2013amrp.nex")#
        # If there are only one or two senior taxa (no easy way to chunk the task):#
        } else {#
            # Duplicate matrix for every senior taxon:#
            DuplicatedMatrix <- matrix(rep(MRP$Matrix_1$Matrix, times = length(Senior_taxa)), nrow = nrow(MRP$Matrix_1$Matrix), dimnames = list(rownames(MRP$Matrix_1$Matrix), c()))#
            # Get new MRP row for current junior:#
            NewMRPRow <- as.vector(t(MRP$Matrix_1$Matrix[Senior_taxa, ]))#
            # Add new row to duplicated matrix:#
            DuplicatedMatrix <- rbind(DuplicatedMatrix, NewMRPRow)#
            # Update last row name to current junior:#
            rownames(DuplicatedMatrix)[nrow(DuplicatedMatrix)] <- i#
            # Turn into a proper cladistic matrix:#
            DuplicatedMatrix <- MakeMorphMatrix(CharacterTaxonMatrix = DuplicatedMatrix)#
            # Collapse MRP to remove duplicated taxa:#
            DuplicatedMatrix <- MRPCollapse(DuplicatedMatrix)#
            # Overwrite MRP with new matrix:#
            MRP <- DuplicatedMatrix#
            # Write out new MRP:#
            WriteMorphNexus(MRP, "/Users/eargtl/Brocklehurst_etal_2013amrp.nex")#
        }#
    }#
}
library(phytools)
?rerootingMethod
library(metatree)
?metatree
library(Claddis)
MatrixSim1 <- ReadMorphNexus("Library/Containers/com.apple.mail/Data/Library/Mail Downloads/14895B1D-69C6-4B61-9264-87C4EA8343A5/Matrix138t.nex")
MatrixSim1
tree.nexus1<-read.nexus("Library/Containers/com.apple.mail/Data/Library/Mail Downloads/72D676F4-93AD-4521-9BD2-47EC5F919722/MCT_NEXUS_138t.nex")
tree.nexus1
is.rooted(tree.nexus1)#
tree.nexus1$root.time<-322.3#
tree.nexus1$root.time
pcoa_input1<-MorphMatrix2PCoA(MatrixSim1, Distance = "MORD",#
                              TransformDistances = "arcsine_sqrt",#
                              DistPolymorphismBehaviour = "min.difference",#
                              DistUncertaintyBehaviour = "min.difference",#
                              DistInapplicableBehaviour = "missing",#
                              correction = "cailliez",#
                              Tree = tree.nexus1,#
                              EstimateAllNodes = FALSE, EstimateTipValues = FALSE,#
                              Threshold = 0.01)
rownames(MatrixSim1$Matrix_1$Matrix)
GetNodeAges(tree.nexus1)#
tree.nexus1$node.label<-seq(139, 275, 1)#
tree.nexus1$node.label
FileToWorkOn <- "Hartman_etal_2019a"#
#
setwd("~")
system("ls")
FileToWorkOn <- "Hartman_etal_2019a"#
#
setwd("~")#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", "tnt mxram 1024, log tnt_log.$j.txt, run mymatrix.tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", "done", ")")#
#
write(TNTCommands, "TNTCommands")
"tnt mxram 1024, log tnt_log.$j.txt, run mymatrix.tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &"
paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = "")
FileToWorkOn <- "Hartman_etal_2019a"#
#
setwd("~")#
#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
write(TNTCommands, "TNTCommands")
paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex")
paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = "")
paste(FileToWorkOn, ".tnt", sep = "")
library(Claddis)#
#
setwd("~")#
#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
write(TNTCommands, "TNTCommands")#
#
system("TNTCommands 2 2")
FileToWorkOn <- "Hartman_etal_2019a"#
#
library(Claddis)#
#
setwd("~")#
#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
write(TNTCommands, "TNTCommands")#
#
system("/Users/eargtl/TNTCommands 2 2")
system("bash /Users/eargtl/TNTCommands 2 2")
paste("bash /Users/eargtl/TNTCommands, NCores, NCores)
system("quit;")
14,210,141,297
system("quit;")
# Set file to work on:#
FileToWorkOn <- "Hartman_etal_2019a"#
#
# Set number of cores to use:#
NCores <- 4#
#
# Load Claddis library:#
library(Claddis)#
#
# Set working directory to user directory:#
setwd("~")#
#
# Read in NEXUS file:#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
# Write to user directory as TNT file:#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
# Build a TNT commands bash file (text):#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
# Write bash commands to user directory:#
write(TNTCommands, "TNTCommands")#
#
paste("bash /Users/eargtl/TNTCommands", NCores, NCores)
# Set file to work on:#
FileToWorkOn <- "Hartman_etal_2019a"#
#
# Set number of cores to use:#
NCores <- 4#
#
# Load Claddis library:#
library(Claddis)#
#
# Set working directory to user directory:#
setwd("~")#
#
# Read in NEXUS file:#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
# Write to user directory as TNT file:#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
# Build a TNT commands bash file (text):#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
# Write bash commands to user directory:#
write(TNTCommands, "TNTCommands")#
#
##
system(paste("bash /Users/eargtl/TNTCommands", NCores, NCores))
y
list.files()
grep("tnt_log", list.files())
list.files()[grep("tnt_log", list.files())]
TreesFiles <- list.files()[grep("trees_tnt", list.files())]
TreesFiles
# Set file to work on:#
FileToWorkOn <- "Hartman_etal_2019a"#
#
# Set number of cores to use:#
NCores <- 2#
#
# Load Claddis library:#
library(Claddis)#
#
# Set working directory to user directory:#
setwd("~")#
#
# Read in NEXUS file:#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
# Write to user directory as TNT file:#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
# Build a TNT commands bash file (text):#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
# Write bash commands to user directory:#
write(TNTCommands, "TNTCommands")#
#
# Execute TNTCommands in terminal:#
system(paste("bash /Users/eargtl/TNTCommands", NCores, NCores))#
LogFiles <- list.files()[grep("tnt_log", list.files())]#
#
TreesFiles <- list.files()[grep("trees_tnt", list.files())]
y
# Set file to work on:#
FileToWorkOn <- "Hartman_etal_2019a"#
#
# Set number of cores to use:#
NCores <- 2#
#
# Load Claddis library:#
library(Claddis)#
#
# Set working directory to user directory:#
setwd("~")#
#
# Read in NEXUS file:#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
# Write to user directory as TNT file:#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
# Build a TNT commands bash file (text):#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
# Write bash commands to user directory:#
write(TNTCommands, "TNTCommands")#
#
# Execute TNTCommands in terminal:#
system(paste("bash /Users/eargtl/TNTCommands", NCores, NCores))#
LogFiles <- list.files()[grep("tnt_log", list.files())]#
#
TreesFiles <- list.files()[grep("trees_tnt", list.files())]
y
y#
y
y#
y#
y
y#
y#
y#
y
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
y#
I agree
?system
# Set file to work on:#
FileToWorkOn <- "Hartman_etal_2019a"#
#
# Set number of cores to use:#
NCores <- 2#
#
# Load Claddis library:#
library(Claddis)#
#
# Set working directory to user directory:#
setwd("~")#
#
# Read in NEXUS file:#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
# Write to user directory as TNT file:#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
# Build a TNT commands bash file (text):#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
# Write bash commands to user directory:#
write(TNTCommands, "TNTCommands")#
#
# Execute TNTCommands in terminal:#
system2(paste("bash /Users/eargtl/TNTCommands", NCores, NCores))#
LogFiles <- list.files()[grep("tnt_log", list.files())]#
#
TreesFiles <- list.files()[grep("trees_tnt", list.files())]
?system2
paste("bash /Users/eargtl/TNTCommands", NCores, NCores)
system2(paste("bash /Users/eargtl/TNTCommands", NCores, NCores))
list.files()
# Set file to work on:#
FileToWorkOn <- "Hartman_etal_2019a"#
#
# Set number of cores to use:#
NCores <- 2#
#
# Load Claddis library:#
library(Claddis)#
#
# Set working directory to user directory:#
setwd("~")#
#
# Read in NEXUS file:#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
# Write to user directory as TNT file:#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
# Build a TNT commands bash file (text):#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
# Write bash commands to user directory:#
write(TNTCommands, "TNTCommands")
system2(paste("bash /Users/eargtl/TNTCommands", NCores, NCores))
system(paste("bash /Users/eargtl/TNTCommands", NCores, NCores))
LogFiles <- list.files()[grep("tnt_log", list.files())]#
#
TreesFiles <- list.files()[grep("trees_tnt", list.files())]
LogFiles
TreesFiles
TreeFiles <- list.files()[grep("trees_tnt", list.files())]
TreeFiles
# Set file to work on:#
FileToWorkOn <- "Hartman_etal_2019a"#
#
# Set number of cores to use:#
NCores <- 2#
#
# Load Claddis library:#
library(Claddis)#
#
# Set working directory to user directory:#
setwd("~")#
#
# Read in NEXUS file:#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
# Write to user directory as TNT file:#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
# Build a TNT commands bash file (text):#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
# Write bash commands to user directory:#
write(TNTCommands, "TNTCommands")#
#
# Execute TNTCommands in terminal:#
system(paste("bash /Users/eargtl/TNTCommands", NCores, NCores))#
LogFiles <- list.files()[grep("tnt_log", list.files())]#
#
TreeFiles <- list.files()[grep("trees_tnt", list.files())]
COMMAND: TSAVE#
COMMAND: QUIT                                                                  #
Stuff
?system
# Set file to work on:#
FileToWorkOn <- "Hartman_etal_2019a"#
#
# Set number of cores to use:#
NCores <- 2#
#
# Load Claddis library:#
library(Claddis)#
#
# Set working directory to user directory:#
setwd("~")#
#
# Read in NEXUS file:#
NEXUS <- Claddis::ReadMorphNexus(paste("~/Documents/Homepage/www.graemetlloyd.com/nexus/", FileToWorkOn, ".nex", sep = ""))#
#
# Write to user directory as TNT file:#
Claddis::WriteMorphTNT(NEXUS, paste(FileToWorkOn, ".tnt", sep = ""))#
#
# Build a TNT commands bash file (text):#
TNTCommands <- c("N=$1", "(", "for j in $(seq 0 $2); do", "((i=i%N)); ((i++==0)) && wait", paste("tnt mxram 1024, log tnt_log.$j.txt, run ", FileToWorkOn, ".tnt, echo= , timeout 24:00:00, rseed0, rseed*,hold 1000,xmult= level 10, taxname=, tsave *trees_tnt.$j.tnt, save, tsave / , scores, log / , quit &", sep = ""), "done", ")")#
#
# Write bash commands to user directory:#
write(TNTCommands, "TNTCommands")#
#
# Execute TNTCommands in terminal:#
x <- system(paste("bash /Users/eargtl/TNTCommands", NCores - 1, NCores))#
LogFiles <- list.files()[grep("tnt_log", list.files())]#
#
TreeFiles <- list.files()[grep("trees_tnt", list.files())]
# Load metatree library:#
library(metatree)#
#
# Build inclusive data list:#
InclusiveDataList <- sort(unique(c(GetFilesForClade(c("matrsarc.html", "matramph.html")), "Gauthier_etal_1988b", "Lu_etal_2016c", "deBraga_et_Rieppel_1997a", "Giles_etal_2015a", "Davis_etal_2012a")))#
#
# Copy just inclusive XML data sets from main directory to project one:#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Homepage/www.graemetlloyd.com/xml/", x, ".xml", sep = ""), to =  paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML/", x, ".xml", sep = "")))#
#
# Copy just inclusive MRP data sets from main directory to project one:#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Homepage/www.graemetlloyd.com/mrp/", x, "mrp.nex", sep = ""), to =  paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/MRP/", x, "mrp.nex", sep = "")))#
#
# Standard excsluive data lsit (supertrees and the like):#
ExclusiveDataList <- c("Averianov_inpressa", "Bravo_et_Gaete_2015a", "Brocklehurst_etal_2013a", "Brocklehurst_etal_2015aa", "Brocklehurst_etal_2015ab", "Brocklehurst_etal_2015ac", "Brocklehurst_etal_2015ad", "Brocklehurst_etal_2015ae", "Brocklehurst_etal_2015af", "Bronzati_etal_2012a", "Bronzati_etal_2015ab", "Brusatte_etal_2009ba", "Campbell_etal_2016ab", "Carr_et_Williamson_2004a", "Carr_etal_2017ab", "Frederickson_et_Tumarkin-Deratzian_2014aa", "Frederickson_et_Tumarkin-Deratzian_2014ab", "Frederickson_et_Tumarkin-Deratzian_2014ac", "Frederickson_et_Tumarkin-Deratzian_2014ad", "Garcia_etal_2006a", "Gatesy_etal_2004ab", "Grellet-Tinner_2006a", "Grellet-Tinner_et_Chiappe_2004a", "Grellet-Tinner_et_Makovicky_2006a", "Hartman_etal_2019a", "Knoll_2008a", "Kurochkin_1996a", "Lopez-Martinez_et_Vicens_2012a", "Lu_etal_2014aa", "Norden_etal_inpressa", "Pisani_etal_2002a", "Ruiz-Omenaca_etal_1997a", "Ruta_etal_2003ba", "Ruta_etal_2003bb", "Ruta_etal_2007a", "Selles_et_Galobart_2016a", "Sereno_1993a", "Sidor_2001a", "Skutschas_etal_inpressa", "Tanaka_etal_2011a", "Toljagic_et_Butler_2013a", "Tsuihiji_etal_2011aa", "Varricchio_et_Jackson_2004a", "Vila_etal_2017a", "Wilson_2005aa", "Wilson_2005ab", "Zelenitsky_et_Therrien_2008a")#
#
# Build amphibia metatree:#
Amphibia <- Metatree(MRPDirectory = "~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/MRP", XMLDirectory = "~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML", TargetClade = "Tetrapoda", InclusiveDataList = InclusiveDataList, ExclusiveDataList = ExclusiveDataList, MissingSpecies = "exclude", RelativeWeights = c(0, 100, 10, 1), WeightCombination = "sum", ReportContradictionsToScreen = FALSE)#
#
# Build taxonomy tree (for basic checks ahead of building constraint trees):#
pdf("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/PDFTrees/TaxonomyTree.pdf", width = 30, height = 50)#
plot(Amphibia$TaxonomyTree, cex = 0.3)#
nodelabels(Amphibia$TaxonomyTree$node.label, cex = 0.5)#
dev.off()#
#
# Build vector of anuran (frog) taxa:#
AnuriTaxa <- Amphibia$TaxonomyTree$tip.label[strap::FindDescendants(n = Ntip(Amphibia$TaxonomyTree) + which(Amphibia$TaxonomyTree$node.label == "Anuri"), tree = Amphibia$TaxonomyTree)]#
#
# Build vector of cadudatan (salamander) taxa:#
CaudataTaxa <- Amphibia$TaxonomyTree$tip.label[strap::FindDescendants(n = Ntip(Amphibia$TaxonomyTree) + which(Amphibia$TaxonomyTree$node.label == "Caudata"), tree = Amphibia$TaxonomyTree)]#
#
# Build vector of batrachian (frog + salamander) taxa:#
BatrachiaTaxa <- Amphibia$TaxonomyTree$tip.label[strap::FindDescendants(n = Ntip(Amphibia$TaxonomyTree) + which(Amphibia$TaxonomyTree$node.label == "Batrachia"), tree = Amphibia$TaxonomyTree)]#
#
# Build vector of gymnophoniann (salamander) taxa:#
GymnophionaTaxa <- Amphibia$TaxonomyTree$tip.label[strap::FindDescendants(n = Ntip(Amphibia$TaxonomyTree) + which(Amphibia$TaxonomyTree$node.label == "Gymnophiona"), tree = Amphibia$TaxonomyTree)]#
#
# Update gymnophoniann vector to ensure Eocaecilia is included (above will potnetially fail as Eocaecilia alone won't form a clade):#
GymnophionaTaxa <- unique(c(GymnophionaTaxa, "Eocaecilia_micropodia"))#
#
# Build vector of lepospondylian taxa:#
LepospondyliTaxa <- Amphibia$TaxonomyTree$tip.label[strap::FindDescendants(n = Ntip(Amphibia$TaxonomyTree) + grep("Lepospondyli", Amphibia$TaxonomyTree$node.label), tree = Amphibia$TaxonomyTree)]#
#
# Build vector of temnospondylian taxa:#
TemnospondyliTaxa <- Amphibia$TaxonomyTree$tip.label[strap::FindDescendants(n = Ntip(Amphibia$TaxonomyTree) + grep("Eutemnospondyli", Amphibia$TaxonomyTree$node.label), tree = Amphibia$TaxonomyTree)]#
#
# Build vector of lissamphibian (crown amphibians) taxa:#
LissamphibiaTaxa <- Amphibia$TaxonomyTree$tip.label[strap::FindDescendants(n = Ntip(Amphibia$TaxonomyTree) + grep("Lissamphibia", Amphibia$TaxonomyTree$node.label), tree = Amphibia$TaxonomyTree)]#
#
# Update temnospondylian vector by removing lissamphibia:#
TemnospondyliTaxa <- setdiff(TemnospondyliTaxa, LissamphibiaTaxa)#
#
# Build vector of amniote taxa:#
AmniotaTaxa <- Amphibia$TaxonomyTree$tip.label[strap::FindDescendants(n = Ntip(Amphibia$TaxonomyTree) + grep("Amniota", Amphibia$TaxonomyTree$node.label), tree = Amphibia$TaxonomyTree)]#
#
# HYPOTHESIS CONSTRAINT TREES (From Laurin et al. http://dx.doi.org/10.1101/352609):#
#
# 1. Monophyletic lissamphibia amongst temnospondyls:#
MonophyleticLissamphibiaInsideTemnospondyli <- ape::collapse.singles(ape::read.tree(text = paste("(", paste(paste("(", paste(LepospondyliTaxa, collapse = ","), ")", sep = ""), paste("(", paste(paste(TemnospondyliTaxa, collapse = ","), paste("(", paste(LissamphibiaTaxa, collapse = ","), ")", sep = ""), sep = ","), ")", sep = ""), sep = ","), ");", sep = "")))#
#
# 2. Monophyletic lissamphibia amongst lepospondyls:#
MonophyleticLissamphibiaInsideLepospondyli <- ape::collapse.singles(ape::read.tree(text = paste("(", paste(paste("(", paste(TemnospondyliTaxa, collapse = ","), ")", sep = ""), paste("(", paste(paste(LepospondyliTaxa, collapse = ","), paste("(", paste(LissamphibiaTaxa, collapse = ","), ")", sep = ""), sep = ","), ")", sep = ""), sep = ","), ");", sep = "")))#
#
# 3. Diphyletic lissamphibia, batrachia amongst temnospondyls and gymniophona amongst lepospondyls:#
DiphyleticLissamphibiaBatrachiaInsideTemnospondyliGymnophionaInsideLepospondyli <- ape::collapse.singles(ape::read.tree(text = paste("(", paste(paste("(", paste(paste(LepospondyliTaxa, collapse = ","), paste("(", paste(GymnophionaTaxa, collapse = ","), ")", sep = ""), sep = ","), ")", sep = ""), paste("(", paste(paste(TemnospondyliTaxa, collapse = ","), paste("(", paste(BatrachiaTaxa, collapse = ","), ")", sep = ""), collapse = ","), ")", sep = ""), sep = ","), ");", sep = "")))#
#
# 4. Diphyletic lissamphibia inside temnospondyli, batrachia and gymniophona as sepaarte clades:#
DiphyleticLissamphibiaInsideTemnospondyli <- ape::collapse.singles(ape::read.tree(text = paste("(", paste(paste("(", paste(LepospondyliTaxa, collapse = ","), ")", sep = ""), paste("(", paste(paste(TemnospondyliTaxa, collapse = ","), paste("(", paste(BatrachiaTaxa, collapse = ","), ")", sep = ""), paste("(", paste(GymnophionaTaxa, collapse = ","), ")", sep = ""), sep = ","), ")", sep = ""), sep = ","), ");", sep = "")))#
#
# 5. Triphyletic Lissamphibia (Anuri and Caudata inside Temnospondyli,\nGymniophona inside Lepospondyli and Crown Lissamphibia monophyletic):#
TriphyleticLissamphibiaAnuriAndCaudataInsideTemnospondyliGymnophionaInsideLepospondyliCrownLissamphibiaMonophyletic <-  ape::collapse.singles(ape::read.tree(text = paste("(", paste(paste("(", paste(paste("(", paste(paste(LepospondyliTaxa, collapse = ","), paste("(", paste(GymnophionaTaxa, collapse = ","), ")", sep = ""), sep = ","), ")", sep = ""), paste("(", paste(paste(TemnospondyliTaxa, collapse = ","), paste("(", paste(AnuriTaxa, collapse = ","), ")", sep = ""), paste("(", paste(CaudataTaxa, collapse = ","), ")", sep = ""), sep = ","), ")", sep = ""), sep = ","), ")", sep = ""), paste("(", paste(AmniotaTaxa, collapse = ","), ")", sep = ""), sep = ","), ");", sep = "")))#
#
# 6. Triphyletic Lissamphibia (Anuri and Caudata inside Temnospondyli,\nGymniophona inside Lepospondyli and Crown Lissamphibia paraphyletic to Amniota):#
TriphyleticLissamphibiaAnuriAndCaudataInsideTemnospondyliGymnophionaInsideLepospondyliCrownLissamphibiaParaphyleticWithRespectToAmniota <- ape::collapse.singles(ape::read.tree(text = paste("(", paste(paste("(", paste(paste("(", paste(paste(LepospondyliTaxa, collapse = ","), paste("(", paste(GymnophionaTaxa, collapse = ","), ")", sep = ""), sep = ","), ")", sep = ""), paste("(", paste(AmniotaTaxa, collapse = ","), ")", sep = ""), sep = ","), ")", sep = ""), paste("(", paste(paste(TemnospondyliTaxa, collapse = ","), paste("(", paste(AnuriTaxa, collapse = ","), ")", sep = ""), paste("(", paste(CaudataTaxa, collapse = ","), ")", sep = ""), sep = ","), ")", sep = ""), sep = ","), ");", sep = "")))#
#
# Build PDF of constraint trees for visualisation:#
pdf("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/PDFTrees/ConstraintTrees.pdf", width = 10, height = 20)#
plot(MonophyleticLissamphibiaInsideTemnospondyli, cex = 0.3, main = "1. Monophyletic Lissamphibia inside Temnospondyli")#
plot(MonophyleticLissamphibiaInsideLepospondyli, cex = 0.3, main = "2. Monophyletic Lissamphibia inside Lepospondyli")#
plot(DiphyleticLissamphibiaBatrachiaInsideTemnospondyliGymnophionaInsideLepospondyli, cex = 0.3, main = "3. Diphyletic Lissamphibia (Batrachia inside Temnospondyli and Gymnophiona inside Lepospondyli)")#
plot(DiphyleticLissamphibiaInsideTemnospondyli, cex = 0.3, main = "4. Diphyletic Lissamphibia inside Temnospondyli (Batrachia and Gymnophiona as separate clades)")#
plot(TriphyleticLissamphibiaAnuriAndCaudataInsideTemnospondyliGymnophionaInsideLepospondyliCrownLissamphibiaMonophyletic, cex = 0.3, main = "5. Triphyletic Lissamphibia (Anuri and Caudata inside Temnospondyli,\nGymniophona inside Lepospondyli and Crown Lissamphibia monophyletic)")#
plot(TriphyleticLissamphibiaAnuriAndCaudataInsideTemnospondyliGymnophionaInsideLepospondyliCrownLissamphibiaParaphyleticWithRespectToAmniota, cex = 0.3, main = "6. Triphyletic Lissamphibia (Anuri and Caudata inside Temnospondyli,\nGymniophona inside Lepospondyli and Crown Lissamphibia paraphyletic to Amniota)")#
dev.off()#
#
# Build hypothees into MRP trees:#
HypothesisOneMRP <- Tree2MRP(MonophyleticLissamphibiaInsideTemnospondyli)#
HypothesisTwoMRP <- Tree2MRP(MonophyleticLissamphibiaInsideLepospondyli)#
HypothesisThreeMRP <- Tree2MRP(DiphyleticLissamphibiaBatrachiaInsideTemnospondyliGymnophionaInsideLepospondyli)#
HypothesisFourMRP <- Tree2MRP(DiphyleticLissamphibiaInsideTemnospondyli)#
HypothesisFiveMRP <- Tree2MRP(TriphyleticLissamphibiaAnuriAndCaudataInsideTemnospondyliGymnophionaInsideLepospondyliCrownLissamphibiaMonophyletic)#
HypothesisSixMRP <- Tree2MRP(TriphyleticLissamphibiaAnuriAndCaudataInsideTemnospondyliGymnophionaInsideLepospondyliCrownLissamphibiaParaphyleticWithRespectToAmniota)#
#
# Update outgroup from allzero to a real tetrapod taxon (Westlothiana_lizziae):#
rownames(HypothesisOneMRP$Matrix_1$Matrix) <- gsub("allzero", "Westlothiana_lizziae", rownames(HypothesisOneMRP$Matrix_1$Matrix))#
rownames(HypothesisTwoMRP$Matrix_1$Matrix) <- gsub("allzero", "Westlothiana_lizziae", rownames(HypothesisTwoMRP$Matrix_1$Matrix))#
rownames(HypothesisThreeMRP$Matrix_1$Matrix) <- gsub("allzero", "Westlothiana_lizziae", rownames(HypothesisThreeMRP$Matrix_1$Matrix))#
rownames(HypothesisFourMRP$Matrix_1$Matrix) <- gsub("allzero", "Westlothiana_lizziae", rownames(HypothesisFourMRP$Matrix_1$Matrix))#
rownames(HypothesisFiveMRP$Matrix_1$Matrix) <- gsub("allzero", "Westlothiana_lizziae", rownames(HypothesisFiveMRP$Matrix_1$Matrix))#
rownames(HypothesisSixMRP$Matrix_1$Matrix) <- gsub("allzero", "Westlothiana_lizziae", rownames(HypothesisSixMRP$Matrix_1$Matrix))#
#
# Get vector of indeterminate OTUs:#
IndeterminateOTUs <- rownames(Amphibia$FullMRPMatrix$Matrix_1$Matrix)[unlist(lapply(as.list(rownames(Amphibia$FullMRPMatrix$Matrix_1$Matrix)), function(x) length(strsplit(x, "_")[[1]]))) > 2]#
#
# Get vector of OTUs with subgenera in them:#
SubgeneraOTUs <- setdiff(unique(c(rownames(HypothesisOneMRP$Matrix_1$Matrix), rownames(HypothesisTwoMRP$Matrix_1$Matrix), rownames(HypothesisThreeMRP$Matrix_1$Matrix), rownames(HypothesisFourMRP$Matrix_1$Matrix), rownames(HypothesisFiveMRP$Matrix_1$Matrix), rownames(HypothesisSixMRP$Matrix_1$Matrix))), IndeterminateOTUs)[unlist(lapply(as.list(setdiff(rownames(HypothesisOneMRP$Matrix_1$Matrix), IndeterminateOTUs)), function(x) nchar(gsub("[:a-z:]|_", "", x)))) > 1]#
#
# Get correctly formatted version of subgenus names:#
FormattedSubgeneraOTUNames <- unlist(lapply(as.list(SubgeneraOTUs), function(x) {SplitName <- strsplit(x, "")[[1]]; SubgenusPosition <- grep("[:A-Z:]", SplitName)[2]; SplitName[SubgenusPosition] <- paste("_(", SplitName[SubgenusPosition], sep = ""); SpeciesPosition <- which(SplitName == "_"); SplitName[SpeciesPosition] <- ")_"; paste(SplitName, collapse = "")}))#
#
# Get vector of all other (regular) OTUs:#
RegularOTUs <- sort(setdiff(unique(c(rownames(HypothesisOneMRP$Matrix_1$Matrix), rownames(HypothesisTwoMRP$Matrix_1$Matrix), rownames(HypothesisThreeMRP$Matrix_1$Matrix), rownames(HypothesisFourMRP$Matrix_1$Matrix), rownames(HypothesisFiveMRP$Matrix_1$Matrix), rownames(HypothesisSixMRP$Matrix_1$Matrix))), c(IndeterminateOTUs, SubgeneraOTUs)))#
#
# Build matrix of al taxonomic reconciliation from XML:#
AllXML <- do.call(rbind, lapply(as.list(InclusiveDataList), function(x) metatree::ReadMetatreeXML(paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML/", x, ".xml", sep = ""))$SourceTree$Taxa$TagContents))#
#
# Get vector of recon numbers for OTU names:#
IndeterminateOTUReconNumbers <- AllXML[match(IndeterminateOTUs, AllXML[, "recon_name"]), "recon_no"]#
#
# Perform a database query to get the recon numbers for the non-indeterminate taxa:#
DatabaseQuery <- PaleobiologyDBTaxaQuerier("1", c(RegularOTUs, FormattedSubgeneraOTUNames))#
#
# Build recon table to draw from in building constraint tree XMLs:#
ReconTable <- cbind(c(RegularOTUs, SubgeneraOTUs, IndeterminateOTUs), c(unlist(lapply(apply(DatabaseQuery[, 1:2], 1, as.list), function(x) {x <- gsub("txn:|var:", "", unlist(x)); unname(x[!is.na(x)][1])})), IndeterminateOTUReconNumbers), c(RegularOTUs, FormattedSubgeneraOTUNames, IndeterminateOTUs))#
#
# Add column names to table:#
colnames(ReconTable) <- c("OTUName", "ReconNumber", "ReconName")#
#
# Setw working directory to XMLs:#
setwd("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML")#
#
# Read in first XML to use as template:#
EmptyXML <- metatree::ReadMetatreeXML(list.files()[1])#
#
# Make an empty XML by deleting most of the existing information:#
EmptyXML$SourceTree$Source$Author <- list(TagContents = matrix("NA", nrow = 1, ncol = 1, dimnames = list(c(), c("ListValue"))), TagSupplement = list(NULL))#
EmptyXML$SourceTree$Source$Year <- list(NULL)#
EmptyXML$SourceTree$Source$Title <- list(NULL)#
EmptyXML$SourceTree$Source$Journal <- list(NULL)#
EmptyXML$SourceTree$Source$Volume <- list(NULL)#
EmptyXML$SourceTree$Source$Pages <- list(NULL)#
EmptyXML$SourceTree$Source$Booktitle <- list(NULL)#
EmptyXML$SourceTree$Source$Publisher <- list(NULL)#
EmptyXML$SourceTree$Source$City <- list(NULL)#
EmptyXML$SourceTree$Source$Editor <- list(NULL)#
EmptyXML$SourceTree$Characters$Molecular <- list(NULL)#
EmptyXML$SourceTree$Characters$Morphological <- list(NULL)#
EmptyXML$SourceTree$Characters$Behavioural <- list(NULL)#
EmptyXML$SourceTree$Characters$Other <- list(TagContents = matrix("MRP", nrow = 1, ncol = 1, dimnames = list(c(), c("TypeValue"))), TagSupplement = matrix(c("Measure", "1"), nrow = 1, ncol = 2, dimnames = list(c(), c("Measure", "Value"))))#
EmptyXML$SourceTree$Notes <- list(TagContents = "Constraint tree.", TagSupplement = list(NULL))#
EmptyXML$SourceTree$Parent <- list(NULL)#
EmptyXML$SourceTree$Sibling <- list(NULL)#
#
# Copy empty XML to create each hypothesis XML:#
HypothesisOneXML <- HypothesisTwoXML <- HypothesisThreeXML <- HypothesisFourXML <- HypothesisFiveXML <- HypothesisSixXML <- EmptyXML#
#
# Build XMLs for every hypothesis:#
HypothesisOneXML$SourceTree$Taxa <- list(TagContents = matrix(as.vector(ReconTable[match(rownames(HypothesisOneMRP$Matrix_1$Matrix), ReconTable[, "OTUName"]), c("ReconName", "ReconNumber", "OTUName")]), ncol = 3, byrow = FALSE, dimnames = list(c(), c("recon_name", "recon_no", "ListValue"))), TagSupplement = matrix(c("number", as.character(nrow(HypothesisOneMRP$Matrix_1$Matrix))), nrow = 1, ncol = 2, dimnames = list(c(), c("Measure", "Value"))))#
HypothesisTwoXML$SourceTree$Taxa <- list(TagContents = matrix(as.vector(ReconTable[match(rownames(HypothesisTwoMRP$Matrix_1$Matrix), ReconTable[, "OTUName"]), c("ReconName", "ReconNumber", "OTUName")]), ncol = 3, byrow = FALSE, dimnames = list(c(), c("recon_name", "recon_no", "ListValue"))), TagSupplement = matrix(c("number", as.character(nrow(HypothesisTwoMRP$Matrix_1$Matrix))), nrow = 1, ncol = 2, dimnames = list(c(), c("Measure", "Value"))))#
HypothesisThreeXML$SourceTree$Taxa <- list(TagContents = matrix(as.vector(ReconTable[match(rownames(HypothesisThreeMRP$Matrix_1$Matrix), ReconTable[, "OTUName"]), c("ReconName", "ReconNumber", "OTUName")]), ncol = 3, byrow = FALSE, dimnames = list(c(), c("recon_name", "recon_no", "ListValue"))), TagSupplement = matrix(c("number", as.character(nrow(HypothesisThreeMRP$Matrix_1$Matrix))), nrow = 1, ncol = 2, dimnames = list(c(), c("Measure", "Value"))))#
HypothesisFourXML$SourceTree$Taxa <- list(TagContents = matrix(as.vector(ReconTable[match(rownames(HypothesisFourMRP$Matrix_1$Matrix), ReconTable[, "OTUName"]), c("ReconName", "ReconNumber", "OTUName")]), ncol = 3, byrow = FALSE, dimnames = list(c(), c("recon_name", "recon_no", "ListValue"))), TagSupplement = matrix(c("number", as.character(nrow(HypothesisFourMRP$Matrix_1$Matrix))), nrow = 1, ncol = 2, dimnames = list(c(), c("Measure", "Value"))))#
HypothesisFiveXML$SourceTree$Taxa <- list(TagContents = matrix(as.vector(ReconTable[match(rownames(HypothesisFiveMRP$Matrix_1$Matrix), ReconTable[, "OTUName"]), c("ReconName", "ReconNumber", "OTUName")]), ncol = 3, byrow = FALSE, dimnames = list(c(), c("recon_name", "recon_no", "ListValue"))), TagSupplement = matrix(c("number", as.character(nrow(HypothesisFiveMRP$Matrix_1$Matrix))), nrow = 1, ncol = 2, dimnames = list(c(), c("Measure", "Value"))))#
HypothesisSixXML$SourceTree$Taxa <- list(TagContents = matrix(as.vector(ReconTable[match(rownames(HypothesisSixMRP$Matrix_1$Matrix), ReconTable[, "OTUName"]), c("ReconName", "ReconNumber", "OTUName")]), ncol = 3, byrow = FALSE, dimnames = list(c(), c("recon_name", "recon_no", "ListValue"))), TagSupplement = matrix(c("number", as.character(nrow(HypothesisSixMRP$Matrix_1$Matrix))), nrow = 1, ncol = 2, dimnames = list(c(), c("Measure", "Value"))))#
#
# Update character number in XMLs:#
HypothesisOneXML$SourceTree$Characters$Other$TagSupplement[, "Value"] <- as.character(ncol(HypothesisOneMRP$Matrix_1$Matrix))#
HypothesisTwoXML$SourceTree$Characters$Other$TagSupplement[, "Value"] <- as.character(ncol(HypothesisTwoMRP$Matrix_1$Matrix))#
HypothesisThreeXML$SourceTree$Characters$Other$TagSupplement[, "Value"] <- as.character(ncol(HypothesisThreeMRP$Matrix_1$Matrix))#
HypothesisFourXML$SourceTree$Characters$Other$TagSupplement[, "Value"] <- as.character(ncol(HypothesisFourMRP$Matrix_1$Matrix))#
HypothesisFiveXML$SourceTree$Characters$Other$TagSupplement[, "Value"] <- as.character(ncol(HypothesisFiveMRP$Matrix_1$Matrix))#
HypothesisSixXML$SourceTree$Characters$Other$TagSupplement[, "Value"] <- as.character(ncol(HypothesisSixMRP$Matrix_1$Matrix))#
#
# Update file names in XMLs:#
HypothesisOneXML$SourceTree$Filename <- HypothesisTwoXML$SourceTree$Filename <- HypothesisThreeXML$SourceTree$Filename <- HypothesisFourXML$SourceTree$Filename <- HypothesisFiveXML$SourceTree$Filename <- HypothesisSixXML$SourceTree$Filename <- list(TagContents = paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), TagSupplements = list(NULL))#
#
# Create copies of XML files to each hypothesis folder:#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML/", x, ".xml", sep = ""), to =  paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisOne/XML/", x, ".xml", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML/", x, ".xml", sep = ""), to =  paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisTwo/XML/", x, ".xml", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML/", x, ".xml", sep = ""), to =  paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisThree/XML/", x, ".xml", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML/", x, ".xml", sep = ""), to =  paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisFour/XML/", x, ".xml", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML/", x, ".xml", sep = ""), to =  paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisFive/XML/", x, ".xml", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/XML/", x, ".xml", sep = ""), to =  paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisSix/XML/", x, ".xml", sep = "")))#
#
# Create copies of MRP files to each hypothesis folder:#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/MRP/", x, "mrp.nex", sep = ""), to = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisOne/MRP/", x, "mrp.nex", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/MRP/", x, "mrp.nex", sep = ""), to = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisTwo/MRP/", x, "mrp.nex", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/MRP/", x, "mrp.nex", sep = ""), to = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisThree/MRP/", x, "mrp.nex", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/MRP/", x, "mrp.nex", sep = ""), to = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisFour/MRP/", x, "mrp.nex", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/MRP/", x, "mrp.nex", sep = ""), to = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisFive/MRP/", x, "mrp.nex", sep = "")))#
x <- lapply(as.list(InclusiveDataList), function(x) file.copy(from = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/MRP/", x, "mrp.nex", sep = ""), to = paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisSix/MRP/", x, "mrp.nex", sep = "")))#
#
# Write out constraint MRPs:#
Claddis::WriteMorphNexus(HypothesisOneMRP, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisOne/MRP/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), "mrp.nex", sep = ""))#
Claddis::WriteMorphNexus(HypothesisTwoMRP, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisTwo/MRP/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), "mrp.nex", sep = ""))#
Claddis::WriteMorphNexus(HypothesisThreeMRP, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisThree/MRP/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), "mrp.nex", sep = ""))#
Claddis::WriteMorphNexus(HypothesisFourMRP, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisFour/MRP/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), "mrp.nex", sep = ""))#
Claddis::WriteMorphNexus(HypothesisFiveMRP, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisFive/MRP/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), "mrp.nex", sep = ""))#
Claddis::WriteMorphNexus(HypothesisSixMRP, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisSix/MRP/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), "mrp.nex", sep = ""))#
#
# Write out XML files for each hypthesis:#
metatree::WriteMetatreeXML(HypothesisOneXML, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisOne/XML/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), ".xml", sep = ""))#
metatree::WriteMetatreeXML(HypothesisTwoXML, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisTwo/XML/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), ".xml", sep = ""))#
metatree::WriteMetatreeXML(HypothesisThreeXML, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisThree/XML/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), ".xml", sep = ""))#
metatree::WriteMetatreeXML(HypothesisFourXML, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisFour/XML/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), ".xml", sep = ""))#
metatree::WriteMetatreeXML(HypothesisFiveXML, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisFive/XML/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), ".xml", sep = ""))#
metatree::WriteMetatreeXML(HypothesisSixXML, paste("~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisSix/XML/", paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""), ".xml", sep = ""))#
#
# Build#
#
HypothesisOne <- metatree::Metatree(MRPDirectory = "~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisOne/MRP", XMLDirectory = "~/Documents/Publications/in prep/Temnospondyl Supertree 2 - Dan/Metatree/HypothesisOne/XML", TargetClade = "Tetrapoda", InclusiveDataList = c(), ExclusiveDataList = ExclusiveDataList, MissingSpecies = "exclude", RelativeWeights = c(0, 100, 10, 1), WeightCombination = "sum", ReportContradictionsToScreen = FALSE, BackboneConstraint = paste("Constraint_", strsplit(as.character(Sys.Date()), split = "-")[[1]][1], "a", sep = ""))
ncol(HypothesisOne$FullMRPMatrix$Matrix_1$Matrix)
